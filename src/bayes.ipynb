{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text tag1  tag2\n",
      "0     Wir wollen heute auf den Ticker der @ank_ffm h...  NEG  HATE\n",
      "1     @Bedb_Puetz Niemand hat die Türken gebraucht s...  NEG  HATE\n",
      "2     @Mesut__A @ntvde @ntv Soll ich dir alle Polize...  NEG  HATE\n",
      "3           @N24 Dank der Islamisierung unserer Heimat!  NEG  HATE\n",
      "4     Die meisten Schiffbrüchigen sind doch keine Fl...  NEG  HATE\n",
      "...                                                 ...  ...   ...\n",
      "1784  @MarleneSchfer1 @blubberette Der Kommentar war...  NEG  HATE\n",
      "1785  @DrAndreaKahl1 @DrDavidBerger @focusonline @St...  NEG  HATE\n",
      "1786  @RegSprecher So sieht Meinungsfreiheit in Deut...  NEG  HATE\n",
      "1787  @mjungclaus GRÜNE = Umweltschänder https://t.c...  NEG  HATE\n",
      "1788  @Nobby1949Z @Holger56228648 Weg mit den allen ...  NEG  HATE\n",
      "\n",
      "[1789 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Datensatz laden\n",
    "\n",
    "# Probeklassifikation mit: \"..\\Daten\\Volksverhetzung_Probeannotation_GermEval2018_Training_komplett\\\\vvh_allg.json\"\n",
    "#data = data.drop(axis=1,labels=\"id\")\n",
    "\n",
    "\n",
    "# \"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Train_HATE_VVH.json\",\n",
    "# \"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Test_HATE_VVH.json\",\n",
    "\n",
    "# Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Test_HATE_Gruppe.json\n",
    "\n",
    "#Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test_HATE.txt\n",
    "\n",
    "\n",
    "train_data = pd.read_json(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train_HATE.json\")\n",
    "train_data = train_data.drop(axis=1, labels=\"id\")\n",
    "\n",
    "test_data = pd.read_json(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test_HATE.json\")\n",
    "test_data = test_data.drop(axis=1, labels=\"id\")\n",
    "\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: \n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "# from gensim.utils import tokenize # Alternativ\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_lg\", )\n",
    "\n",
    "def preprocess(text, langmod):\n",
    "    \"\"\" Vorverarbeitung eines Strings\n",
    "    Output: Liste von Tokens\n",
    "    Optionen:\n",
    "        - Lemmatisierung\n",
    "        - Zeichensetzung entfernen\n",
    "        - Mindesttokenlänge\n",
    "        - Stopwortentfernung\n",
    "        - alles klein geschrieben\n",
    "        - Ziffern entfernen\n",
    "        - Links entfernen\n",
    "        - alles außer Buchstabenketten entfernen\n",
    "        - \n",
    "    \"\"\"\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    # Tokenisieren\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Zeichensetzung entfernen\n",
    "    tokens = [token for token in doc if token.pos_ not in {\"PROPN\", \"NUM\", \"SYM\", \"PUNCT\"}]\n",
    "    \n",
    "    # Mindesttokenlänge\n",
    "    minlen = 2\n",
    "    tokens = [token for token in tokens if len(token) > minlen]\n",
    "\n",
    "    # Nur Alphabetszeichenketten\n",
    "    tokens = [token for token in tokens if token.text.isalpha()]\n",
    "\n",
    "    # Stopwortentfernung\n",
    "    #stopwords = STOP_WORDS.union({''})\n",
    "    tokens = [token for token in tokens if token.lower() not in STOP_WORDS]\n",
    "\n",
    "    # Lemmatisierung\n",
    "    tokens = [token.lemma_ for token in tokens]\n",
    "\n",
    "\n",
    "    # POS-Tags: nur Verben\n",
    "\n",
    "\n",
    "    # POS-Tags: nur Substantive\n",
    "\n",
    "\n",
    "    # POS-Tags: nur Verben und Substantive\n",
    "\n",
    "    # Ausgabe als Array\n",
    "    # np_array = doc.toarray(['Attributes here'])\n",
    "\n",
    "    return tokens\n",
    "\n",
    "    \n",
    "\n",
    "def preprocess_corpus(corpus, langmod):\n",
    "    corpus_prepped = [preprocess(text, langmod) for text in corpus]\n",
    "    return corpus_prepped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing & Feature Extraction\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Features:\n",
    "# 1. Zeichen-N-Gramme, char_wb: Unigramme innerhalb von Wortgrenzen (Leerzeichen)\n",
    "vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(1, 4), lowercase=False)\n",
    "X = vectorizer.fit_transform(train_data[\"text\"])\n",
    "X_train = X.toarray()\n",
    "X_test = vectorizer.transform(test_data[\"text\"])\n",
    "X_test = X_test.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. doc2vec (in der gensim-Implementierung)\n",
    "\n",
    "# s. https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html\n",
    "\n",
    "\n",
    "import gensim\n",
    "#from gensim import corpora, models, similarities, downloader\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "\n",
    "\n",
    "def read_corpus(datei, tokens_only=False):\n",
    "    with open(datei, encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            id, tweet, anno = line.strip().split(\"\\t\")\n",
    "            # TODO: ändern, so dass nicht alles lowercase ist\n",
    "            tokens = gensim.utils.simple_preprocess(tweet)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                # Tag z.B. Zeilennummer ([i]), oder: Korpus-ID\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [id])\n",
    "\n",
    "train_corpus = list(read_corpus(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Train_HATE_Handlung.txt\"))\n",
    "test_corpus = list(read_corpus(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Test_HATE_Handlung.txt\"))\n",
    "\n",
    "\n",
    "# Distributed Memory\n",
    "model_dm = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=2, epochs=30)\n",
    "model_dm.build_vocab(train_corpus)\n",
    "model_dm.train(train_corpus, total_examples=model_dm.corpus_count, epochs=model_dm.epochs)\n",
    "\n",
    "# Distributed Bag of Words\n",
    "model_dbow = gensim.models.doc2vec.Doc2Vec(dm=0, vector_size=300, min_count=2, epochs=30)\n",
    "model_dbow.build_vocab(train_corpus)\n",
    "model_dbow.train(train_corpus, total_examples=model_dbow.corpus_count, epochs=model_dbow.epochs)\n",
    "\n",
    "# zur Kombination von DBOW und DM\n",
    "concat_model = ConcatenatedDoc2Vec([model_dbow, model_dm])\n",
    "\n",
    "# Modell speichern\n",
    "\n",
    "\n",
    "# Einzelnen Vektor berechnen (für einen bereits vorverarbeiteten, tokenisierten String): \n",
    "#vector = concat_model.infer_vector(gensim.utils.simple_preprocess( \"Die gr\\u00f6\\u00dfte Terrororganisation der Welt ist die USA und ihrer Ostk\\u00fcste!\"))\n",
    "#print(vector)\n",
    "\n",
    "# Alle Vektoren berechnen\n",
    "\n",
    "# von https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4;\n",
    "# regressors? steps?\n",
    "def feature_vectors(model, docs, tags):\n",
    "    targets, tags_final = zip(*[(model.infer_vector(doc.words, epochs=20), tags[line]) for line, doc in enumerate(docs)])\n",
    "    return targets, tags_final\n",
    "\n",
    "# Alternativ: mit der gensim-eigenen API für sklearn\n",
    "# s. https://radimrehurek.com/gensim_3.8.3/sklearn_api/d2vmodel.html\n",
    "# from gensim.sklearn_api import D2VTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user', 'user', 'scheiss', 'deutsche', 'politiker', 'mehr', 'gibt', 'es', 'wohl', 'nicht', 'dazu', 'zu', 'sagen']\n"
     ]
    }
   ],
   "source": [
    "print(train_corpus[4].words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotation in Zahlen konvertieren\n",
    "\n",
    "labels_vvh = [\"KEINE\", \"VVH\"]\n",
    "labels_gruppe = [\"KeineGruppe\", \"Gruppe\"]\n",
    "labels_handlung = [\"KeineHandlung\", \"Handlung\"]\n",
    "\n",
    "def labeling(label):\n",
    "    if label == \"NEG\": return 1\n",
    "    else: return 0\n",
    "\n",
    "def labeling_vvh(label):\n",
    "    if label == \"VVH\": return 1\n",
    "    elif label == \"KEINE\": return 0\n",
    "\n",
    "def labeling_gruppe(label):\n",
    "    if label == \"Gruppe\": return 1\n",
    "    elif label == \"KeineGruppe\": return 0\n",
    "\n",
    "def labeling_handlung(label):\n",
    "    if label == \"Handlung\": return 1\n",
    "    elif label == \"KeineHandlung\": return 0\n",
    "\n",
    "tags_train = list(map(labeling, list(train_data[\"tag1\"])))\n",
    "tags_test = list(map(labeling, list(test_data[\"tag1\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "\n",
    "# durch die Dateien vorgegeben, ca 70/30\n",
    "X_train, y_train  = feature_vectors(concat_model, train_corpus, tags_train)\n",
    "X_test, y_test  = feature_vectors(concat_model, test_corpus, tags_test)\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3,random_state=109, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X_train, X_test, tags_train, tags_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression # war wohl ganz gut (Projekt FU)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 1, does not match size of target_names, 2. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-174-4e7a00ddaf5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# np.mean(predicted==y_test) # Accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels_handlung\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cbi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cbi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   1985\u001b[0m             )\n\u001b[0;32m   1986\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1987\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   1988\u001b[0m                 \u001b[1;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1989\u001b[0m                 \u001b[1;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of classes, 1, does not match size of target_names, 2. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "# np.mean(predicted==y_test) # Accuracy\n",
    "\n",
    "print(str(metrics.classification_report(y_test, predicted, target_names=labels_handlung)))\n",
    "print(str(metrics.confusion_matrix(y_test, predicted)))\n",
    "\n",
    "#Bayes, Probeklassifikation\n",
    "\"\"\"              precision    recall  f1-score   support\n",
    "\n",
    "    VVH-Allg       0.40      0.19      0.26        21\n",
    "       Keine       0.94      0.98      0.96       286\n",
    "\n",
    "    accuracy                           0.93       307\n",
    "   macro avg       0.67      0.58      0.61       307\n",
    "weighted avg       0.91      0.93      0.91       307\n",
    "\n",
    "[[  4  17]\n",
    " [  6 280]]\n",
    "\n",
    " precision    recall  f1-score   support\n",
    "\n",
    "    VVH-Allg       0.00      0.00      0.00        23\n",
    "       Keine       0.93      1.00      0.96       284\n",
    "\n",
    "    accuracy                           0.93       307\n",
    "   macro avg       0.46      0.50      0.48       307\n",
    "weighted avg       0.86      0.93      0.89       307\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Cross validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "cvmodel = MultinomialNB()\n",
    "scores = cross_val_score(cvmodel, X=X.toarray(), y=Y, cv=5, scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bfe21998ac49805528ba5c524b8c99ad95f34df1263c0d2d07976ea5fcbab9a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
