{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id                                               data  \\\n",
      "0     1112521  @user @user @user Weitaus schlimmer. Heute ist...   \n",
      "1     1114995   Das Deutsche Kaiserreich soll wieder auferstehen   \n",
      "2     1110545                   Die BRD ist eine einzige Schande   \n",
      "3     1114326  @user @user Die Grünen....besser kann man das ...   \n",
      "4     4112169  @user @user Scheiss deutsche Politiker! Mehr g...   \n",
      "...       ...                                                ...   \n",
      "3045  1223336  Unbequeme Wahrheit:   Sexuelle Belästigung ist...   \n",
      "3046  1221963  Vor was habt ihr Angst Liebe Bürger !!!???? St...   \n",
      "3047  2220834  @user Schön den Dummkopf #Oppermann von der Vo...   \n",
      "3048  1220580  @user   Hoffentlich sind die Nationalisten dan...   \n",
      "3049  2222357  @user Klar, was sollen Sie denn auch anderes s...   \n",
      "\n",
      "                         label  \n",
      "0                [KeineGruppe]  \n",
      "1                [KeineGruppe]  \n",
      "2                [KeineGruppe]  \n",
      "3     [Politische Einstellung]  \n",
      "4                [KeineGruppe]  \n",
      "...                        ...  \n",
      "3045             [KeineGruppe]  \n",
      "3046             [KeineGruppe]  \n",
      "3047  [Politische Einstellung]  \n",
      "3048         [Anderes Merkmal]  \n",
      "3049             [KeineGruppe]  \n",
      "\n",
      "[3050 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Datensatz laden\n",
    "\n",
    "daten = pd.read_json(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_HATE_GruppeDetail.json\")\n",
    "daten = daten.drop(axis=1, labels=[\"tag1\", \"tag2\"])\n",
    "\n",
    "print(daten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Für die detaillierteren Annotationen bei Gruppe und Handlung:\n",
    "# Labels aufspalten, so dass nur noch binäre Probleme vorliegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotationen konvertieren\n",
    "\n",
    "ja = [\"NEG\", \"VVH\", \"Gruppe\", \"Handlung\", \"Nationalität\", 'ethnische Herkunft / \"Rasse\"', \"Religion / Weltanschauung\",\n",
    "        \"Politische Einstellung\", \"Geschlecht\", \"Anderes Merkmal\", \"Aufstachelung zu Hass\", \"Aufforderung zu Gewalt- oder Willkürmaßnahmen\", \"Angriff der Menschenwürde\"]\n",
    "\n",
    "nein = [\"KEINE\", \"KeineGruppe\", \"KeineHandlung\"]\n",
    "\n",
    "labels_vvh = [\"KEINE\", \"VVH\"]\n",
    "labels_gruppe = [\"Gruppe\", \"KeineGruppe\"]\n",
    "labels_handlung = [\"KeineHandlung\", \"Handlung\"]\n",
    "\n",
    "def transform_label(label):\n",
    "    if label in ja: return 1\n",
    "    else: return 0\n",
    "\n",
    "targets = list(map(transform_label, list(daten[\"label\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'key of type tuple not found and not a MultiIndex'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-c1a6e39c15a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#X_train, X_test, y_train, y_test = train_test_split(daten[\"data\"], targets, test_size=0.33, random_state=36, stratify=targets)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterative_train_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdaten\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.33\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\cbi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\skmultilearn\\model_selection\\iterative_stratification.py\u001b[0m in \u001b[0;36miterative_train_test_split\u001b[1;34m(X, y, test_size)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0mtrain_indexes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_indexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstratifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_indexes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_indexes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_indexes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_indexes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cbi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    875\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 877\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cbi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_with\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    890\u001b[0m             )\n\u001b[0;32m    891\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 892\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cbi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_values_tuple\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    925\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 927\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"key of type tuple not found and not a MultiIndex\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m         \u001b[1;31m# If key is contained, would have returned by now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'key of type tuple not found and not a MultiIndex'"
     ]
    }
   ],
   "source": [
    "# Stratifizierter Train/Test-Split\n",
    "# s. https://scikit-learn.org/stable/modules/cross_validation.html#stratification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(daten[\"data\"], targets, test_size=0.33, random_state=36, stratify=targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing & Feature Extraction\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Features:\n",
    "# 1. Zeichen-N-Gramme, char_wb: Unigramme innerhalb von Wortgrenzen (Leerzeichen)\n",
    "vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(1, 4), lowercase=True)\n",
    "X_train = vectorizer.fit_transform(X_train) #vectorizer.fit_transform(train_data[1])\n",
    "X_train = X_train.toarray()\n",
    "X_test = vectorizer.transform(X_test) #vectorizer.transform(test_data[1])\n",
    "X_test = X_test.toarray()\n",
    "# X_train, X_test, y_train, y_test = X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. doc2vec (in der gensim-Implementierung)\n",
    "\n",
    "# s. https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html\n",
    "\n",
    "\n",
    "import gensim\n",
    "#from gensim import corpora, models, similarities, downloader\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "\n",
    "\n",
    "def read_corpus(datei, tokens_only=False):\n",
    "    with open(datei, encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            id, tweet, anno = line.strip().split(\"\\t\")\n",
    "            # TODO: ändern, so dass nicht alles lowercase ist\n",
    "            tokens = gensim.utils.simple_preprocess(tweet)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                # Tag z.B. Zeilennummer ([i]), oder: Korpus-ID\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [id])\n",
    "\n",
    "train_corpus = list(read_corpus(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Train_HATE_Handlung.txt\"))\n",
    "test_corpus = list(read_corpus(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Test_HATE_Handlung.txt\"))\n",
    "\n",
    "\n",
    "# Distributed Memory\n",
    "model_dm = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=2, epochs=30)\n",
    "model_dm.build_vocab(train_corpus)\n",
    "model_dm.train(train_corpus, total_examples=model_dm.corpus_count, epochs=model_dm.epochs)\n",
    "\n",
    "# Distributed Bag of Words\n",
    "model_dbow = gensim.models.doc2vec.Doc2Vec(dm=0, vector_size=300, min_count=2, epochs=30)\n",
    "model_dbow.build_vocab(train_corpus)\n",
    "model_dbow.train(train_corpus, total_examples=model_dbow.corpus_count, epochs=model_dbow.epochs)\n",
    "\n",
    "# zur Kombination von DBOW und DM\n",
    "concat_model = ConcatenatedDoc2Vec([model_dbow, model_dm])\n",
    "\n",
    "# Modell speichern\n",
    "\n",
    "\n",
    "# Einzelnen Vektor berechnen (für einen bereits vorverarbeiteten, tokenisierten String): \n",
    "#vector = concat_model.infer_vector(gensim.utils.simple_preprocess( \"Die gr\\u00f6\\u00dfte Terrororganisation der Welt ist die USA und ihrer Ostk\\u00fcste!\"))\n",
    "#print(vector)\n",
    "\n",
    "# Alle Vektoren berechnen\n",
    "\n",
    "# von https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4;\n",
    "# regressors? steps?\n",
    "def feature_vectors(model, docs, tags):\n",
    "    targets, tags_final = zip(*[(model.infer_vector(doc.words, epochs=20), tags[line]) for line, doc in enumerate(docs)])\n",
    "    return targets, tags_final\n",
    "\n",
    "\n",
    "# Train/Test Split; durch die Dateien vorgegeben, ca 70/30\n",
    "X_train, y_train  = feature_vectors(concat_model, train_corpus, tags_train)\n",
    "X_test, y_test  = feature_vectors(concat_model, test_corpus, tags_test)\n",
    "\n",
    "# Alternativ: mit der gensim-eigenen API für sklearn\n",
    "# s. https://radimrehurek.com/gensim_3.8.3/sklearn_api/d2vmodel.html\n",
    "# from gensim.sklearn_api import D2VTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression # war wohl ganz gut (Projekt FU)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Gruppe       0.82      0.61      0.70       413\n",
      " KeineGruppe       0.77      0.91      0.83       594\n",
      "\n",
      "    accuracy                           0.78      1007\n",
      "   macro avg       0.80      0.76      0.77      1007\n",
      "weighted avg       0.79      0.78      0.78      1007\n",
      "\n",
      "[[250 163]\n",
      " [ 54 540]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n    VVH-Allg       0.40      0.19      0.26        21\\n       Keine       0.94      0.98      0.96       286\\n\\n    accuracy                           0.93       307\\n   macro avg       0.67      0.58      0.61       307\\nweighted avg       0.91      0.93      0.91       307\\n\\n[[  4  17]\\n [  6 280]]\\n\\n precision    recall  f1-score   support\\n\\n    VVH-Allg       0.00      0.00      0.00        23\\n       Keine       0.93      1.00      0.96       284\\n\\n    accuracy                           0.93       307\\n   macro avg       0.46      0.50      0.48       307\\nweighted avg       0.86      0.93      0.89       307\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import PrecisionRecallDisplay, precision_recall_curve\n",
    "\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "# np.mean(predicted==y_test) # Accuracy\n",
    "\n",
    "#precision, recall, _ = precision_recall_curve(y_test, predicted, pos_label={'Gruppe', 'KeineGruppe'})\n",
    "#disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
    "#disp.plot()\n",
    "\n",
    "#fpr, tpr, thresholds = metrics.roc_curve(y_test, predicted, pos_label={'Gruppe', 'KeineGruppe'})\n",
    "#metrics.auc(fpr, tpr)\n",
    "\n",
    "print(str(metrics.classification_report(y_test, predicted, target_names=labels_gruppe)))\n",
    "print(str(metrics.confusion_matrix(y_test, predicted)))\n",
    "\n",
    "#Bayes, Probeklassifikation\n",
    "\"\"\"              precision    recall  f1-score   support\n",
    "\n",
    "    VVH-Allg       0.40      0.19      0.26        21\n",
    "       Keine       0.94      0.98      0.96       286\n",
    "\n",
    "    accuracy                           0.93       307\n",
    "   macro avg       0.67      0.58      0.61       307\n",
    "weighted avg       0.91      0.93      0.91       307\n",
    "\n",
    "[[  4  17]\n",
    " [  6 280]]\n",
    "\n",
    " precision    recall  f1-score   support\n",
    "\n",
    "    VVH-Allg       0.00      0.00      0.00        23\n",
    "       Keine       0.93      1.00      0.96       284\n",
    "\n",
    "    accuracy                           0.93       307\n",
    "   macro avg       0.46      0.50      0.48       307\n",
    "weighted avg       0.86      0.93      0.89       307\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-197-786ce2d56e4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mcvmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcvmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'f1_macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Y' is not defined"
     ]
    }
   ],
   "source": [
    "# Alternative: Cross validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "cvmodel = MultinomialNB()\n",
    "scores = cross_val_score(cvmodel, X=X.toarray(), y=Y, cv=5, scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bfe21998ac49805528ba5c524b8c99ad95f34df1263c0d2d07976ea5fcbab9a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
