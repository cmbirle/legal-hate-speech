{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referenzdatensatz erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Lösung zweier Formatierungsprobleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Besonderheit der HASOC 2020 Daten: Zeilenumbrüche innerhalb der Tweets\n",
    "# --> die Dateien so aufbereiten, dass nur ein Tweet pro Zeile steht\n",
    "\n",
    "def rem_white(filename):\n",
    "    with open(filename, mode=\"r\", encoding=\"utf-16\") as f:\n",
    "        content = f.readlines()\n",
    "        content = content[1:] # Erklärungszeile ignorieren\n",
    "        newcont = []\n",
    "        for i in range(len(content)):\n",
    "            # Fall 1: Zeile ist komplett\n",
    "            if len(content[i].split(\"\\t\")) == 5:\n",
    "                newcont.append(content[i])\n",
    "            # Fall 2: Zeile ist nicht komplett\n",
    "            else:\n",
    "                # letzter Teil einer Zeile erreicht\n",
    "                if \"hasoc_2020_de_\" in content[i]:\n",
    "                    comp_line += content[i]\n",
    "                    comp_line = comp_line.replace(\"\\n\",\" \")\n",
    "                    comp_line += \"\\n\"\n",
    "                    newcont.append(comp_line)\n",
    "                # erster Teil einer Zeile \n",
    "                elif content[i].startswith(\"11\") :\n",
    "                    comp_line = content[i]\n",
    "                # mittlerer Teil einer Zeile, manchmal nur \\n\n",
    "                else: comp_line += content[i]\n",
    "    nwfilename = filename[:len(filename)-4] + \"_formatted\" + \".txt\"\n",
    "    with open(nwfilename, mode=\"w\", encoding=\"utf-8\") as outfile:\n",
    "        for line in newcont:\n",
    "            outfile.write(line)\n",
    "    return True\n",
    "\n",
    "# Bereits formatiert, nicht nochmals durchführen\n",
    "#rem_white(\"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_train_new.txt\")\n",
    "#rem_white(\"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_test_new.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Besonderheit des GermEval2019-Datensatzes:\n",
    "# Erstes Zeichen der Tweets in den Testdaten fehlt zum Teil\n",
    "# Abgeschnitten von GermEval2019 beim Labeln der Daten (von \"Testdata_Subtask12\" zu \"GoldLabelSubtask12\")\n",
    "# z.B. Zeile 341: \"enschen, die etwas auf eBay-Kleinanzeigen verticken, ... OTHER\tOTHER\"\n",
    "# Aufbereiten: Testdaten mit den Originaltweets aus der Datei \"germeval2019_Testdata_Subtask12.txt\" speichern\n",
    "# (Bereits ausgeführt, nicht nochmals durchführen)\n",
    "\n",
    "with open(\"..\\Korpora\\GermEval-2019-Data\\germeval2019GoldLabelsSubtask1_2.txt\", mode=\"r\", encoding=\"utf-8\") as in_test:\n",
    "    cont_test = in_test.readlines()\n",
    "    sep_cont = [line.strip().split(\"\\t\") for line in cont_test]\n",
    "\n",
    "# Tweets ohne abgeschnitte Anfänge einlesen\n",
    "with open(\"..\\Korpora\\GermEval-2019-Data\\germeval2019_Testdata_Subtask12.txt\", mode=\"r\", encoding=\"utf-8\") as in_tweets:\n",
    "    tweets = in_tweets.readlines()\n",
    "    tweets = [tweet.strip() for tweet in tweets]\n",
    "\n",
    "# Tweets mit den Labels zusammenführen und neu speichern\n",
    "tweets_replaced = [(tweets[i],sep_cont[i][1],sep_cont[i][2]) for i in range(len(sep_cont))]\n",
    "with open(\"..\\Korpora\\GermEval-2019-Data\\germeval2019GoldLabelsSubtask1_2_ersetzt.txt\", mode=\"w\", encoding=\"utf-8\") as out_test:\n",
    "    for line in tweets_replaced:\n",
    "        out_test.write(\"\\t\".join(line)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Angleichen der Formatierung und Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_to_refcorp(filename, corp_id, mod):\n",
    "    \"\"\"\n",
    "    GermEval-Daten und HASOC-Daten in ein einheitliches Format übertragen.\n",
    "\n",
    "    Input: Datei mit Tabstopp-getrennten Werten, Korpus-ID, train/test-Information\n",
    "    Output: Liste von Tupeln im Format (Referenzkorpus-ID, Tweet, Label1, Label2)\n",
    "            - ReferenzkorpusID; setzt sich zusammen aus der Korpus-ID,\n",
    "                                md_id = \"11\", falls es um Trainingsdaten (mod=train), \"22\", falls es um Testdaten (mod=test) geht\n",
    "                                und der Zeilennummer in der Ursprungsdatei;\n",
    "                                also z.B.: \"01220034\" - für einen Tweet der Zeile 34, aus den Testdaten des GermEval2018-Datensatzes\n",
    "            - Tweet; formatierter Tweet\n",
    "            - Label1       \n",
    "            - Label2\n",
    "    \"\"\"\n",
    "    newcorp = []\n",
    "    with open(filename, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.readlines()\n",
    "        \n",
    "        # erste Zeile ignorieren bei HASOC2019 (\"03\")\n",
    "        if corp_id == \"03\": text = text[1:]\n",
    "\n",
    "        # Bestimmen, welche Formatierungsfunktion genutzt wird\n",
    "        if corp_id == \"01\" or corp_id == \"02\": form_func = format_germeval\n",
    "        elif corp_id == \"03\" or corp_id == \"04\": form_func = format_hasoc\n",
    "\n",
    "        url_pattern = re.compile('https:\\/\\/.*?(?: |$)')\n",
    "\n",
    "        for num, entry in enumerate(text):\n",
    "            entry = entry.strip()\n",
    "            tag1, tag2 = \"NOT\", \"NOT\"\n",
    "\n",
    "            tweet, tag1, tag2 = form_func(entry, tag1, tag2)\n",
    "\n",
    "            # URLs mit generischer Twitter-URL ersetzen\n",
    "            tweet = url_pattern.sub(\"https://t.co \", tweet)\n",
    "            tweet = tweet.strip()\n",
    "\n",
    "            # Tweet von HTML-Resten entfernen und Emoji-Codierung mit Emojis ersetzen\n",
    "            tweet = clean_tweet(tweet)\n",
    "\n",
    "            # gedoppelte und überflüssige Anführungszeichen entfernen\n",
    "            tweet = tweet.replace('\"\"',\"'\")\n",
    "            tweet = tweet.strip('\"')\n",
    "\n",
    "            # ID erstellen\n",
    "            if mod == \"train\": md_id = \"11\"\n",
    "            elif mod ==\"test\": md_id = \"22\"\n",
    "            id_num = f'{num+1:04d}'\n",
    "            tweet_id = str(corp_id) + str(md_id) + str(id_num)\n",
    "            \n",
    "            # der neuen Sammlung hinzufügen\n",
    "            newcorp.append((tweet_id, tweet, tag1, tag2))\n",
    "    return newcorp\n",
    "\n",
    "def format_germeval(entry, tag1, tag2):\n",
    "    \"\"\"GermEval-Annotation auf die neue Annotation abbilden & Token |LBR| ersetzen\"\"\"\n",
    "    tweet, label1, label2 = entry.split(\"\\t\")\n",
    "    if label1 == \"OFFENSE\": tag1 = \"NEG\"\n",
    "    if label2 == \"INSULT\": tag2 = \"INSOFF\"\n",
    "    elif label2 == \"PROFANITY\": tag2 = \"PRFN\"\n",
    "    elif label2 == \"ABUSE\": tag2 = \"HATE\"\n",
    "    tweet = tweet.replace(\"|LBR|\", \" \")\n",
    "    return tweet, tag1, tag2\n",
    "\n",
    "\n",
    "def format_hasoc(entry, tag1, tag2):\n",
    "    \"\"\"HASOC-Annotation auf die neue Annotation abbilden\"\"\"\n",
    "    sep = entry.split(\"\\t\")\n",
    "    tweet, l1, l2 = sep[1], sep[2], sep[3]\n",
    "    if l1 == \"HOF\": tag1 = \"NEG\"\n",
    "    if l2 not in [\"HATE\", \"OFFN\", \"PRFN\"] and l1!=\"NOT\": print(l2)\n",
    "    if l2 == \"HATE\": tag2 = \"HATE\"\n",
    "    elif l2 == \"OFFN\": tag2 = \"INSOFF\"\n",
    "    elif l2 == \"PRFN\": tag2 = \"PRFN\"\n",
    "    return tweet, tag1, tag2\n",
    "\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    \"\"\"Emojis finden und ersetzen und HTML-Reste entfernen\"\"\"\n",
    "    cleaned = tweet\n",
    "    # Emojis, die als Text, z.B. \"<U+0001F60A>\", gespeichert sind: als utf-8 formatieren\n",
    "    # s. https://stackoverflow.com/questions/67507017/replace-unicode-code-point-with-actual-character-using-regex\n",
    "    cleaned = re.sub(r'<U\\+([A-F0-9]+)>', lambda x: chr(int(x.group(1), 16)), cleaned)\n",
    "    cleaned = re.sub(r\"&lt;\" , \"<\", cleaned)\t \n",
    "    cleaned = re.sub(r\"&gt;\" , \">\", cleaned)    \n",
    "    cleaned = re.sub(r\"&amp;\" , \"&\", cleaned)\n",
    "    cleaned = re.sub(r'\\\"', '\"', cleaned)\n",
    "    cleaned = re.sub(r'\\\"\"', '\"', cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GermEval2018\n",
    "germeval2018train_converted = convert_to_refcorp(\"..\\Korpora\\GermEval-2018-Data-master\\germeval2018.training.txt\", \"01\", \"train\")\n",
    "germeval2018test_converted = convert_to_refcorp(\"..\\Korpora\\GermEval-2018-Data-master\\germeval2018.test.txt\", \"01\", \"test\")\n",
    "\n",
    "# GermEval2019\n",
    "germeval2019train_converted = convert_to_refcorp(\"..\\Korpora\\GermEval-2019-Data\\germeval2019.training_subtask1_2_korrigiert.txt\", \"02\", \"train\")\n",
    "germeval2019test_converted = convert_to_refcorp(\"..\\Korpora\\GermEval-2019-Data\\germeval2019GoldLabelsSubtask1_2_ersetzt.txt\", \"02\", \"test\")\n",
    "\n",
    "# HASOC 2019\n",
    "hasoc2019train_converted = convert_to_refcorp(\"..\\Korpora\\german_dataset_hasoc2019\\german_dataset\\german_dataset.tsv\", \"03\", \"train\")\n",
    "hasoc2019test_converted = convert_to_refcorp(\"..\\Korpora\\german_dataset_hasoc2019\\german_dataset\\hasoc_de_test_gold.tsv\", \"03\", \"test\")\n",
    "\n",
    "# HASOC 2020\n",
    "hasoc2020train_converted = convert_to_refcorp(\"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_train_new_formatted.txt\", \"04\", \"train\")\n",
    "hasoc2020test_converted = convert_to_refcorp(\"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_test_new_formatted.txt\", \"04\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Referenzdatensatz zusammenstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referenzdatensatz zusammenstellen\n",
    "# (Bereits ausgeführt, nicht nochmals ausführen)\n",
    "\n",
    "refcorp_train = germeval2018train_converted + germeval2019train_converted + hasoc2019train_converted + hasoc2020train_converted\n",
    "refcorp_test = germeval2018test_converted + germeval2019test_converted + hasoc2019test_converted + hasoc2020test_converted\n",
    "\n",
    "import random\n",
    "\n",
    "random.shuffle(refcorp_train)\n",
    "random.shuffle(refcorp_test)\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train.txt\", mode=\"w\", encoding=\"utf-8\") as reftrainout:\n",
    "    reftrainout.write(\"corpus_id\\ttweet\\tbinarylabel\\tfinelabel\\n\")\n",
    "    for reftweet in refcorp_train:\n",
    "        reftrainout.write(\"\\t\".join(reftweet)+\"\\n\")\n",
    "    \n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test.txt\", mode=\"w\", encoding=\"utf-8\") as reftestout:\n",
    "    reftestout.write(\"corpus_id\\ttweet\\tbinarylabel\\tfinelabel\\n\")\n",
    "    for reftweet in refcorp_test:\n",
    "        reftestout.write(\"\\t\".join(reftweet)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Duplikate entfernen und @user-Erwähnungen anonymisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def anonym_atuser(tweet):\n",
    "    \"\"\"Ersetzen von @user-Erwähnungen\n",
    "    Von Inputs der Form '@rspctfl@houelle_beck @ergroovt'\n",
    "    zu Outputs der Form '@user@user @user'\n",
    "    \"\"\"\n",
    "    tweet_anonym = re.sub('@[^@ ]+?@', '@user@', tweet)\n",
    "    tweet_anonym = re.sub('@[^@ ]+? ', '@user ', tweet_anonym)\n",
    "    tweet_anonym = re.sub('@[^@ ]+?$', '@user', tweet_anonym)\n",
    "    return tweet_anonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung des Jaccard-Index für zwei Multisets und einige Hilfsfunktionen\n",
    "\n",
    "def jaccard_multisets(bag1, bag2):\n",
    "    \"\"\" Den Jaccard-Index, ein Ähnlichkeitsmaß, für zwei Multisets (von zwei Strings) berechnen.\n",
    "    Input: zwei Multisets im Format\n",
    "        {   'e': 3, 'a': 4, ..., 'x': 1,\n",
    "            'tok': {'e', 'a', ..., 'x'},\n",
    "            'len': 23 \n",
    "        }\n",
    "    Output: Jaccard-Index = Länge der Schnittmenge / Länge der Vereinigung;\n",
    "            max. 0.5 (sehr ähnlich), min 0.0 (gar nicht ähnlich)\n",
    "    \"\"\"\n",
    "    # Schnittmenge bauen, die Gesamtlänge speichern\n",
    "    schnitt_len = 0\n",
    "    schnitt = bag1[\"tok\"] & bag2[\"tok\"]\n",
    "    for gram in schnitt:\n",
    "        schnitt_len += min(bag1[gram],bag2[gram])\n",
    "    # Länge der Vereinigung ermitteln\n",
    "    vereinigung_len = bag1[\"len\"] + bag2[\"len\"]\n",
    "    # Jaccard-Index berechnen\n",
    "    jaccard_index = schnitt_len / vereinigung_len\n",
    "    return jaccard_index\n",
    "    \n",
    "\n",
    "def make_bag(string1):\n",
    "    \"\"\" String als Multiset speichern. Hilfsfunktion zur Beschleunigung der Berechnung des Jaccard-Index für Multisets.\n",
    "\n",
    "    Input: String\n",
    "    Output: Dictionary im Format\n",
    "        {   'e': 3, 'a': 4, ..., 'x': 1,    # jedes Unigramm als Key, die Frequenz als Value\n",
    "            'tok': {'e', 'a', ..., 'x'},    # das Set aller Unigramme\n",
    "            'len': 23                       # die Länge des Strings\n",
    "        }\n",
    "    \"\"\"\n",
    "    stringbag = dict()\n",
    "    stringset = set(string1)\n",
    "    for ch1 in stringset: stringbag[ch1] = string1.count(ch1)\n",
    "    stringbag[\"len\"] = len(string1)\n",
    "    stringbag[\"tok\"] = stringset\n",
    "    return stringbag\n",
    "\n",
    "\n",
    "def calculate_bags(corpus):\n",
    "    \"\"\" Multisets (bags) für alle Strings in einem Korpus berechen.\n",
    "    Input:  Korpus\n",
    "    Output: Liste von Multisets (Index in der Liste == Index in der Korpusliste)\n",
    "    \"\"\"\n",
    "    bags = [make_bag(tweet[1]) for tweet in corpus]\n",
    "    return bags\n",
    "\n",
    "\n",
    "def jaccard(set1, set2):\n",
    "    \"\"\"Jaccard-Index von zwei Sets (von zwei Strings) berechnen.\n",
    "       Max.: 1 (sehr ähnlich), Min.: 0 (sehr verschieden)\n",
    "    \"\"\"\n",
    "    jaccard_index = len(set1 & set2) / len(set1 | set2)\n",
    "    return jaccard_index\n",
    "\n",
    "\n",
    "def calculate_sets(corpus):\n",
    "    \"\"\"Set für alle Tweets in einem Korpus berechnen; als Liste der Sets speichern.\n",
    "    \"\"\"\n",
    "    sets = [set(tweet[1]) for tweet in corpus]\n",
    "    return sets\n",
    "\n",
    "\n",
    "\n",
    "def collect_duplicates(corpus, sets, simfunc, cutoff):\n",
    "    \"\"\" Duplikate sammeln\n",
    "    Input:  corpus  - Korpus,\n",
    "            sets    - Liste von Multisets für alle Tweets im Korpus,\n",
    "            simfunc - Vergleichsfunktion,\n",
    "            cutoff  - Ähnlichkeitsgrenzwert\n",
    "    Output: Liste potentieller Duplikate (als Tupel der Korpuseinträge)\n",
    "    \"\"\"\n",
    "    duplicates = []\n",
    "    for i in range(len(corpus)):\n",
    "        if i % 1000 == 0: print(\"i\",i) # Zur Zeitmessung\n",
    "        for j in range(len(corpus)):\n",
    "            if i >= j: continue # Kein Vergleich mit sich selbst, und jedes Paar nur in eine Richtung\n",
    "            else: # Ähnlichkeitswert ermitteln\n",
    "                jacc = simfunc(sets[i],sets[j])\n",
    "                if jacc >= cutoff:\n",
    "                    duplicates.append((corpus[i],corpus[j]))\n",
    "    return duplicates\n",
    "\n",
    "\n",
    "def sim_clusters(duplicates):\n",
    "    \"\"\" Aus einer Liste von Duplikaten Duplikat-Cluster bestimmen.\n",
    "    Input:  Liste von Duplikatpaaren im Format (Text1, Text2);\n",
    "            mit Text 1 und 2 im Format (Korpus_ID, ...)\n",
    "    Output: Liste der Clustersets, Anzahl der vorkommenden Tweets\n",
    "    \"\"\"\n",
    "    # Falls ID1 ähnlich ID2: zusammen in ein Set, und jede weitere ID, die einer der beiden IDs ähnlich ist dazu\n",
    "    clusters = []\n",
    "    ID_cluster_ref = dict()\n",
    "    for (tweet1, tweet2) in duplicates:\n",
    "        #tweet1, tweet2 = duplicate_tup\n",
    "        ID1, ID2 = tweet1[0], tweet2[0]\n",
    "        # Fall 1: Weder ID1 noch ID2 vorhanden: neues Cluster\n",
    "        if (ID1 not in ID_cluster_ref) and (ID2 not in ID_cluster_ref):\n",
    "            cluster_num = len(clusters)\n",
    "            ID_cluster_ref[ID1], ID_cluster_ref[ID2] = cluster_num, cluster_num\n",
    "            clusters.append({tweet1, tweet2})\n",
    "        # Fall 2: Eine der beiden IDs vorhanden: beide IDs ins vorhandene Cluster integrieren\n",
    "        elif ID1 in ID_cluster_ref:\n",
    "            cluster_num = ID_cluster_ref[ID1]\n",
    "            ID_cluster_ref[ID2] = cluster_num\n",
    "            clusters[cluster_num].add(tweet2)\n",
    "        elif ID2 in ID_cluster_ref:\n",
    "            cluster_num = ID_cluster_ref[ID2]\n",
    "            ID_cluster_ref[ID1] = cluster_num\n",
    "            clusters[cluster_num].add(tweet1)\n",
    "        # Fall 3: Beide bereits vorhanden: Cluster-IDs überprüfen\n",
    "        else:\n",
    "            assert ID_cluster_ref[ID1] == ID_cluster_ref[ID2]\n",
    "\n",
    "    return clusters, len(ID_cluster_ref), ID_cluster_ref.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard-Index: \t \t       0.9523809523809523\n",
      "Jaccard-Index für Multimengen: 0.36054421768707484\n"
     ]
    }
   ],
   "source": [
    "# Veranschaulichung des Jaccard-Indexes für Mengen und für Multimengen\n",
    "\n",
    "tweet1 = \"\"\n",
    "tweet2 = \"@user einigen wir uns doch darauf, dass linker antikommunismus ein nogo ist\"\n",
    "tweet3 = \"@user ne mach doch was du willst, kann dem thema eh nicht ausm weg gehen\"\n",
    "tweet4 = \"@ThomasOppermann SPD - SCHMAROTZER,PÄDOPHILE UND DENUNZIANTEN   oder   SCHEINHEILGSTE PARTEI DEUTSCHLANDS !!!\"\n",
    "tweet5 = \"SPD - SCHMAROTZER, PÄDOPHILE UND DENUNZIANTEN\"\n",
    "tweet6 = \"@user - SCHMAROTZER, PÄDOPHILE UND DENUNZIANTEN\"\n",
    "tweet7 = \"Wer hier Naziverhalten sieht, hat ein Rederecht. Auf dem Klo. Scheisshausparolen gehören dort hin. 1683 haben wir die Türken davongejagt. Ja\"\n",
    "tweet8 = \"Wer hier Naziverhalten sieht, hat ein Rederecht. Auf dem Klo. Scheisshausparolen gehören dort hin. Schon 1683 wurden die Türken davongejagt.\"\n",
    "\n",
    "print(f\"Jaccard-Index: \\t \\t       {jaccard(set(tweet2), set(tweet3))}\")\n",
    "print(f\"Jaccard-Index für Multimengen: {jaccard_multisets(make_bag(tweet2), make_bag(tweet3))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datensatz laden\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train.txt\", mode=\"r\", encoding=\"utf-8\") as in_train:\n",
    "    train = in_train.readlines()\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test.txt\", mode=\"r\", encoding=\"utf-8\") as in_test:\n",
    "    test = in_test.readlines()\n",
    "\n",
    "daten = train[1:] + test[1:] # ohne Erklärungszeile\n",
    "daten = [entry.strip().split(\"\\t\") for entry in daten]\n",
    "\n",
    "# @user-Erwähnungen anonymisieren (für eine verbesserte Vergleichbarkeit)\n",
    "daten_anonym = [(entry[0],anonym_atuser(entry[1]),entry[2],entry[3]) for entry in daten]\n",
    "\n",
    "# Multisets aller Tweets berechnen\n",
    "bags = calculate_bags(daten_anonym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "i 1000\n",
      "i 2000\n",
      "i 3000\n",
      "i 4000\n",
      "i 5000\n",
      "i 6000\n",
      "i 7000\n",
      "i 8000\n",
      "i 9000\n",
      "i 10000\n",
      "i 11000\n",
      "i 12000\n",
      "i 13000\n",
      "i 14000\n",
      "i 15000\n",
      "i 16000\n",
      "i 17000\n",
      "i 18000\n",
      "i 19000\n",
      "i 20000\n",
      "i 21000\n",
      "i 22000\n",
      "i 23000\n"
     ]
    }
   ],
   "source": [
    "# Jaccard-Index berechnen\n",
    "dups_multisets = collect_duplicates(daten_anonym, bags, jaccard_multisets, 0.47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insg. 320 einander in irgendeiner Konstellation ähnliche Tweets\n",
      "Insg. 101 Cluster\n",
      "Insg. 220 Tweets, die aus dem Datensatz entfernt werden\n"
     ]
    }
   ],
   "source": [
    "# Duplikat-Cluster berechnen\n",
    "clusters, num_dup, num_ids = sim_clusters(dups_multisets)\n",
    "\n",
    "# Schwarze Liste zu löschender Korpus-IDs erstellen und speichern\n",
    "schwarzeListe = []\n",
    "for gruppe in clusters[2:]:\n",
    "    grp = list(gruppe)\n",
    "    for tweet in grp[1:]:\n",
    "        schwarzeListe.append(tweet[0])\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\schwarze_Liste.txt\", mode=\"w\", encoding=\"utf-8\") as schwout:\n",
    "    for id in schwarzeListe:\n",
    "        schwout.write(id+\"\\n\")\n",
    "\n",
    "# Informationen zu den ähnlichen Tweets\n",
    "print(f\"Insg. {num_dup} einander in irgendeiner Konstellation ähnliche Tweets\") # 320\n",
    "print(f\"Insg. {len(clusters)} Cluster\") # 101 Cluster\n",
    "print(f\"Insg. {len(schwarzeListe)} Tweets, die aus dem Datensatz entfernt werden\") # 220\n",
    "\n",
    "\n",
    "# Ohne die Duplikate in neue Dateien schreiben (nicht mehr anonymisiert)\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train_OD.txt\", mode=\"w\", encoding=\"utf-8\") as out_train:\n",
    "    for tweet in train:\n",
    "        if tweet[0] not in schwarzeListe:\n",
    "            out_train.write(tweet)\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test_OD.txt\", mode=\"w\", encoding=\"utf-8\") as out_test:\n",
    "    for tweet in test:\n",
    "        if tweet[0] not in schwarzeListe:\n",
    "            out_test.write(tweet)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bfe21998ac49805528ba5c524b8c99ad95f34df1263c0d2d07976ea5fcbab9a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
