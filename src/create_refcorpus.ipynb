{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referenzdatensatz erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. L√∂sung zweier Formatierungsprobleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Besonderheit der HASOC 2020 Daten: Zeilenumbr√ºche innerhalb der Tweets\n",
    "# --> die Dateien so aufbereiten, dass nur ein Tweet pro Zeile steht\n",
    "\n",
    "def rem_white(filename):\n",
    "    with open(filename, mode=\"r\", encoding=\"utf-16\") as f:\n",
    "        content = f.readlines()\n",
    "        content = content[1:] # Erkl√§rungszeile ignorieren\n",
    "        newcont = []\n",
    "        for i in range(len(content)):\n",
    "            # Fall 1: Zeile ist komplett\n",
    "            if len(content[i].split(\"\\t\")) == 5:\n",
    "                newcont.append(content[i])\n",
    "            # Fall 2: Zeile ist nicht komplett\n",
    "            else:\n",
    "                # letzter Teil einer Zeile erreicht\n",
    "                if \"hasoc_2020_de_\" in content[i]:\n",
    "                    comp_line += content[i]\n",
    "                    comp_line = comp_line.replace(\"\\n\",\" \")\n",
    "                    comp_line += \"\\n\"\n",
    "                    newcont.append(comp_line)\n",
    "                # erster Teil einer Zeile \n",
    "                elif content[i].startswith(\"11\") :\n",
    "                    comp_line = content[i]\n",
    "                # mittlerer Teil einer Zeile, manchmal nur \\n\n",
    "                else: comp_line += content[i]\n",
    "    nwfilename = filename[:len(filename)-4] + \"_formatted\" + \".txt\"\n",
    "    with open(nwfilename, mode=\"w\", encoding=\"utf-8\") as outfile:\n",
    "        for line in newcont:\n",
    "            outfile.write(line)\n",
    "    return True\n",
    "\n",
    "# Bereits formatiert, nicht nochmals durchf√ºhren\n",
    "#rem_white(\"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_train_new.txt\")\n",
    "#rem_white(\"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_test_new.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Besonderheit des GermEval2019-Datensatzes:\n",
    "# Erstes Zeichen der Tweets in den Testdaten fehlt zum Teil\n",
    "# Abgeschnitten von GermEval2019 beim Labeln der Daten (von \"Testdata_Subtask12\" zu \"GoldLabelSubtask12\")\n",
    "# z.B. Zeile 341: \"enschen, die etwas auf eBay-Kleinanzeigen verticken, ... OTHER\tOTHER\"\n",
    "# Aufbereiten: Testdaten mit den Originaltweets aus der Datei \"germeval2019_Testdata_Subtask12.txt\" speichern\n",
    "# (Bereits ausgef√ºhrt, nicht nochmals durchf√ºhren)\n",
    "\n",
    "with open(\"..\\Korpora\\GermEval-2019-Data\\germeval2019GoldLabelsSubtask1_2.txt\", mode=\"r\", encoding=\"utf-8\") as in_test:\n",
    "    cont_test = in_test.readlines()\n",
    "    sep_cont = [line.strip().split(\"\\t\") for line in cont_test]\n",
    "\n",
    "# Tweets ohne abgeschnitte Anf√§nge einlesen\n",
    "with open(\"..\\Korpora\\GermEval-2019-Data\\germeval2019_Testdata_Subtask12.txt\", mode=\"r\", encoding=\"utf-8\") as in_tweets:\n",
    "    tweets = in_tweets.readlines()\n",
    "    tweets = [tweet.strip() for tweet in tweets]\n",
    "\n",
    "# Tweets mit den Labels zusammenf√ºhren und neu speichern\n",
    "tweets_replaced = [(tweets[i],sep_cont[i][1],sep_cont[i][2]) for i in range(len(sep_cont))]\n",
    "with open(\"..\\Korpora\\GermEval-2019-Data\\germeval2019GoldLabelsSubtask1_2_ersetzt.txt\", mode=\"w\", encoding=\"utf-8\") as out_test:\n",
    "    for line in tweets_replaced:\n",
    "        out_test.write(\"\\t\".join(line)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Angleichen der Formatierung und Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_to_refcorp(filename, corp_id, mod):\n",
    "    \"\"\"\n",
    "    GermEval-Daten und HASOC-Daten in ein einheitliches Format √ºbertragen.\n",
    "\n",
    "    Input: Datei mit Tabstopp-getrennten Werten, Korpus-ID, train/test-Information\n",
    "    Output: Liste von Tupeln im Format (Referenzkorpus-ID, Tweet, Label1, Label2)\n",
    "            - ReferenzkorpusID; setzt sich zusammen aus der Korpus-ID,\n",
    "                                md_id = \"11\", falls es um Trainingsdaten (mod=train), \"22\", falls es um Testdaten (mod=test) geht\n",
    "                                und der Zeilennummer in der Ursprungsdatei;\n",
    "                                also z.B.: \"01220034\" - f√ºr einen Tweet der Zeile 34, aus den Testdaten des GermEval2018-Datensatzes\n",
    "            - Tweet; formatierter Tweet\n",
    "            - Label1       \n",
    "            - Label2\n",
    "    \"\"\"\n",
    "    newcorp = []\n",
    "    with open(filename, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.readlines()\n",
    "        \n",
    "        # erste Zeile ignorieren bei HASOC2019 (\"03\")\n",
    "        if corp_id == \"03\": text = text[1:]\n",
    "\n",
    "        # Bestimmen, welche Formatierungsfunktion genutzt wird\n",
    "        if corp_id == \"01\" or corp_id == \"02\": form_func = format_germeval\n",
    "        elif corp_id == \"03\" or corp_id == \"04\": form_func = format_hasoc\n",
    "\n",
    "        url_pattern = re.compile('https:\\/\\/.*?(?: |$)')\n",
    "\n",
    "        for num, entry in enumerate(text):\n",
    "            entry = entry.strip()\n",
    "            tag1, tag2 = \"NOT\", \"NOT\"\n",
    "\n",
    "            tweet, tag1, tag2 = form_func(entry, tag1, tag2)\n",
    "\n",
    "            # URLs mit generischer Twitter-URL ersetzen\n",
    "            tweet = url_pattern.sub(\"https://t.co \", tweet)\n",
    "            tweet = tweet.strip()\n",
    "\n",
    "            # Tweet von HTML-Resten entfernen und Emoji-Codierung mit Emojis ersetzen\n",
    "            tweet = clean_tweet(tweet)\n",
    "\n",
    "            # gedoppelte und √ºberfl√ºssige Anf√ºhrungszeichen entfernen\n",
    "            tweet = tweet.replace('\"\"',\"'\")\n",
    "            tweet = tweet.strip('\"')\n",
    "\n",
    "            # ID erstellen\n",
    "            if mod == \"train\": md_id = \"11\"\n",
    "            elif mod ==\"test\": md_id = \"22\"\n",
    "            id_num = f'{num+1:04d}'\n",
    "            tweet_id = str(corp_id) + str(md_id) + str(id_num)\n",
    "            \n",
    "            # der neuen Sammlung hinzuf√ºgen\n",
    "            newcorp.append((tweet_id, tweet, tag1, tag2))\n",
    "    return newcorp\n",
    "\n",
    "def format_germeval(entry, tag1, tag2):\n",
    "    \"\"\"GermEval-Annotation auf die neue Annotation abbilden & Token |LBR| ersetzen\"\"\"\n",
    "    tweet, label1, label2 = entry.split(\"\\t\")\n",
    "    if label1 == \"OFFENSE\": tag1 = \"NEG\"\n",
    "    if label2 == \"INSULT\": tag2 = \"INSOFF\"\n",
    "    elif label2 == \"PROFANITY\": tag2 = \"PRFN\"\n",
    "    elif label2 == \"ABUSE\": tag2 = \"HATE\"\n",
    "    tweet = tweet.replace(\"|LBR|\", \" \")\n",
    "    return tweet, tag1, tag2\n",
    "\n",
    "\n",
    "def format_hasoc(entry, tag1, tag2):\n",
    "    \"\"\"HASOC-Annotation auf die neue Annotation abbilden\"\"\"\n",
    "    sep = entry.split(\"\\t\")\n",
    "    tweet, l1, l2 = sep[1], sep[2], sep[3]\n",
    "    if l1 == \"HOF\": tag1 = \"NEG\"\n",
    "    if l2 not in [\"HATE\", \"OFFN\", \"PRFN\"] and l1!=\"NOT\": print(l2)\n",
    "    if l2 == \"HATE\": tag2 = \"HATE\"\n",
    "    elif l2 == \"OFFN\": tag2 = \"INSOFF\"\n",
    "    elif l2 == \"PRFN\": tag2 = \"PRFN\"\n",
    "    return tweet, tag1, tag2\n",
    "\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    \"\"\"Emojis finden und ersetzen und HTML-Reste entfernen\"\"\"\n",
    "    cleaned = tweet\n",
    "    # Emojis, die als Text, z.B. \"<U+0001F60A>\", gespeichert sind: als utf-8 formatieren\n",
    "    # s. https://stackoverflow.com/questions/67507017/replace-unicode-code-point-with-actual-character-using-regex\n",
    "    cleaned = re.sub(r'<U\\+([A-F0-9]+)>', lambda x: chr(int(x.group(1), 16)), cleaned)\n",
    "    cleaned = re.sub(r\"&lt;\" , \"<\", cleaned)\t \n",
    "    cleaned = re.sub(r\"&gt;\" , \">\", cleaned)    \n",
    "    cleaned = re.sub(r\"&amp;\" , \"&\", cleaned)\n",
    "    cleaned = re.sub(r'\\\"', '\"', cleaned)\n",
    "    cleaned = re.sub(r'\\\"\"', '\"', cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GermEval2018\n",
    "germeval2018train_converted = convert_to_refcorp(\"..\\Korpora\\GermEval-2018-Data-master\\germeval2018.training.txt\", \"01\", \"train\")\n",
    "germeval2018test_converted = convert_to_refcorp(\"..\\Korpora\\GermEval-2018-Data-master\\germeval2018.test.txt\", \"01\", \"test\")\n",
    "\n",
    "# GermEval2019\n",
    "germeval2019train_converted = convert_to_refcorp(\"..\\Korpora\\GermEval-2019-Data\\germeval2019.training_subtask1_2_korrigiert.txt\", \"02\", \"train\")\n",
    "germeval2019test_converted = convert_to_refcorp(\"..\\Korpora\\GermEval-2019-Data\\germeval2019GoldLabelsSubtask1_2_ersetzt.txt\", \"02\", \"test\")\n",
    "\n",
    "# HASOC 2019\n",
    "hasoc2019train_converted = convert_to_refcorp(\"..\\Korpora\\german_dataset_hasoc2019\\german_dataset\\german_dataset.tsv\", \"03\", \"train\")\n",
    "hasoc2019test_converted = convert_to_refcorp(\"..\\Korpora\\german_dataset_hasoc2019\\german_dataset\\hasoc_de_test_gold.tsv\", \"03\", \"test\")\n",
    "\n",
    "# HASOC 2020\n",
    "hasoc2020train_converted = convert_to_refcorp(\"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_train_new_formatted.txt\", \"04\", \"train\")\n",
    "hasoc2020test_converted = convert_to_refcorp(\"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_test_new_formatted.txt\", \"04\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Referenzdatensatz zusammenstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referenzdatensatz zusammenstellen\n",
    "# (Bereits ausgef√ºhrt, nicht nochmals ausf√ºhren)\n",
    "\n",
    "refcorp_train = germeval2018train_converted + germeval2019train_converted + hasoc2019train_converted + hasoc2020train_converted\n",
    "refcorp_test = germeval2018test_converted + germeval2019test_converted + hasoc2019test_converted + hasoc2020test_converted\n",
    "\n",
    "import random\n",
    "\n",
    "random.shuffle(refcorp_train)\n",
    "random.shuffle(refcorp_test)\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train.txt\", mode=\"w\", encoding=\"utf-8\") as reftrainout:\n",
    "    reftrainout.write(\"corpus_id\\ttweet\\tbinarylabel\\tfinelabel\\n\")\n",
    "    for reftweet in refcorp_train:\n",
    "        reftrainout.write(\"\\t\".join(reftweet)+\"\\n\")\n",
    "    \n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test.txt\", mode=\"w\", encoding=\"utf-8\") as reftestout:\n",
    "    reftestout.write(\"corpus_id\\ttweet\\tbinarylabel\\tfinelabel\\n\")\n",
    "    for reftweet in refcorp_test:\n",
    "        reftestout.write(\"\\t\".join(reftweet)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Duplikate entfernen und @user-Erw√§hnungen anonymisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def anonym_atuser(tweet):\n",
    "    \"\"\"Ersetzen von @user-Erw√§hnungen\n",
    "    Von Inputs der Form '@rspctfl@houelle_beck @ergroovt'\n",
    "    zu Outputs der Form '@user@user @user'\n",
    "    \"\"\"\n",
    "    tweet_anonym = re.sub('@[^@ ]+?@', '@user@', tweet)\n",
    "    tweet_anonym = re.sub('@[^@ ]+? ', '@user ', tweet_anonym)\n",
    "    tweet_anonym = re.sub('@[^@ ]+?$', '@user', tweet_anonym)\n",
    "    return tweet_anonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung des Jaccard-Index f√ºr zwei Multisets und einige Hilfsfunktionen\n",
    "\n",
    "def jaccard_multisets(bag1, bag2):\n",
    "    \"\"\" Den Jaccard-Index, ein √Ñhnlichkeitsma√ü, f√ºr zwei Multisets (von zwei Strings) berechnen.\n",
    "    Input: zwei Multisets im Format\n",
    "        {   'e': 3, 'a': 4, ..., 'x': 1,\n",
    "            'tok': {'e', 'a', ..., 'x'},\n",
    "            'len': 23 \n",
    "        }\n",
    "    Output: Jaccard-Index = L√§nge der Schnittmenge / L√§nge der Vereinigung;\n",
    "            max. 0.5 (sehr √§hnlich), min 0.0 (gar nicht √§hnlich)\n",
    "    \"\"\"\n",
    "    # Schnittmenge bauen, die Gesamtl√§nge speichern\n",
    "    schnitt_len = 0\n",
    "    schnitt = bag1[\"tok\"] & bag2[\"tok\"]\n",
    "    for gram in schnitt:\n",
    "        schnitt_len += min(bag1[gram],bag2[gram])\n",
    "    # L√§nge der Vereinigung ermitteln\n",
    "    vereinigung_len = bag1[\"len\"] + bag2[\"len\"]\n",
    "    # Jaccard-Index berechnen\n",
    "    jaccard_index = schnitt_len / vereinigung_len\n",
    "    return jaccard_index\n",
    "    \n",
    "\n",
    "def make_bag(string1):\n",
    "    \"\"\" String als Multiset speichern. Hilfsfunktion zur Beschleunigung der Berechnung des Jaccard-Index f√ºr Multisets.\n",
    "\n",
    "    Input: String\n",
    "    Output: Dictionary im Format\n",
    "        {   'e': 3, 'a': 4, ..., 'x': 1,    # jedes Unigramm als Key, die Frequenz als Value\n",
    "            'tok': {'e', 'a', ..., 'x'},    # das Set aller Unigramme\n",
    "            'len': 23                       # die L√§nge des Strings\n",
    "        }\n",
    "    \"\"\"\n",
    "    stringbag = dict()\n",
    "    stringset = set(string1)\n",
    "    for ch1 in stringset: stringbag[ch1] = string1.count(ch1)\n",
    "    stringbag[\"len\"] = len(string1)\n",
    "    stringbag[\"tok\"] = stringset\n",
    "    return stringbag\n",
    "\n",
    "\n",
    "def calculate_bags(corpus):\n",
    "    \"\"\" Multisets (bags) f√ºr alle Strings in einem Korpus berechen.\n",
    "    Input:  Korpus\n",
    "    Output: Liste von Multisets (Index in der Liste == Index in der Korpusliste)\n",
    "    \"\"\"\n",
    "    bags = [make_bag(tweet[1]) for tweet in corpus]\n",
    "    return bags\n",
    "\n",
    "\n",
    "def collect_duplicates(corpus, sets, simfunc, cutoff):\n",
    "    \"\"\" Duplikate sammeln\n",
    "    Input:  corpus  - Korpus,\n",
    "            sets    - Liste von Multisets f√ºr alle Tweets im Korpus,\n",
    "            simfunc - Vergleichsfunktion,\n",
    "            cutoff  - √Ñhnlichkeitsgrenzwert\n",
    "    Output: Liste potentieller Duplikate (als Tupel der Korpuseintr√§ge)\n",
    "    \"\"\"\n",
    "    duplicates = []\n",
    "    for i in range(len(corpus)):\n",
    "        if i % 100 == 0: print(\"i\",i) # Zur Zeitmessung\n",
    "        for j in range(len(corpus)):\n",
    "            if i >= j: continue # Kein Vergleich mit sich selbst, und jedes Paar nur in eine Richtung\n",
    "            else: # √Ñhnlichkeitswert ermitteln\n",
    "                jacc = simfunc(sets[i],sets[j])\n",
    "                if jacc >= cutoff:\n",
    "                    duplicates.append((corpus[i],corpus[j]))\n",
    "    return duplicates\n",
    "\n",
    "\n",
    "def sim_clusters(duplicates):\n",
    "    \"\"\" Aus einer Liste von Duplikaten Duplikat-Cluster bestimmen.\n",
    "    Input:  Liste von Duplikatpaaren im Format (Text1, Text2);\n",
    "            mit Text 1 und 2 im Format (Korpus_ID, ...)\n",
    "    Output: Liste der Clustersets, Anzahl der vorkommenden Tweets\n",
    "    \"\"\"\n",
    "    # Falls ID1 √§hnlich ID2: zusammen in ein Set, und jede weitere ID, die einer der beiden IDs √§hnlich ist dazu\n",
    "    clusters = []\n",
    "    ID_cluster_ref = dict()\n",
    "    for (tweet1, tweet2) in duplicates:\n",
    "        #tweet1, tweet2 = duplicate_tup\n",
    "        ID1, ID2 = tweet1[0], tweet2[0]\n",
    "        # Fall 1: Weder ID1 noch ID2 vorhanden: neues Cluster\n",
    "        if (ID1 not in ID_cluster_ref) and (ID2 not in ID_cluster_ref):\n",
    "            cluster_num = len(clusters)\n",
    "            ID_cluster_ref[ID1], ID_cluster_ref[ID2] = cluster_num, cluster_num\n",
    "            clusters.append({tweet1, tweet2})\n",
    "        # Fall 2: Eine der beiden IDs vorhanden: beide IDs ins vorhandene Cluster integrieren\n",
    "        elif ID1 in ID_cluster_ref:\n",
    "            cluster_num = ID_cluster_ref[ID1]\n",
    "            ID_cluster_ref[ID2] = cluster_num\n",
    "            clusters[cluster_num].add(tweet2)\n",
    "        elif ID2 in ID_cluster_ref:\n",
    "            cluster_num = ID_cluster_ref[ID2]\n",
    "            ID_cluster_ref[ID1] = cluster_num\n",
    "            clusters[cluster_num].add(tweet1)\n",
    "        # Fall 3: Beide bereits vorhanden: Cluster-IDs √ºberpr√ºfen\n",
    "        else:\n",
    "            assert ID_cluster_ref[ID1] == ID_cluster_ref[ID2]\n",
    "\n",
    "    return clusters, len(ID_cluster_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6382978723404256\n",
      "0.2777777777777778\n"
     ]
    }
   ],
   "source": [
    "# Veranschaulichung des Jaccard-Indexes f√ºr Sets und f√ºr Multisets\n",
    "\n",
    "def jaccard(set1, set2):\n",
    "    \"\"\"Jaccard-Index von zwei Sets (von zwei Strings) berechnen.\n",
    "       Max.: 1 (sehr √§hnlich), Min.: 0 (sehr verschieden)\n",
    "    \"\"\"\n",
    "    jaccard_index = len(set1 & set2) / len(set1 | set2)\n",
    "    return jaccard_index\n",
    "\n",
    "def calculate_sets(corpus):\n",
    "    \"\"\"Set f√ºr alle Tweets in einem Korpus berechnen; als Liste der Sets speichern.\n",
    "    \"\"\"\n",
    "    sets = [set(tweet[1]) for tweet in corpus]\n",
    "    return sets\n",
    "\n",
    "\n",
    "tweet1 = \"@Karl_Lauterbach Besser ein Amateur der lernf√§hig ist, als das verlauste Pack der abgefuckten SPD - SCHMAROTZER, P√ÑDOPHILE und DENUNZIANTEN !!!\"\n",
    "tweet2 = \"@ThomasOppermann SPD - SCHMAROTZER,P√ÑDOPHILE UND DENUNZIANTEN   oder   SCHEINHEILGSTE PARTEI DEUTSCHLANDS !!!\"\n",
    "tweet3 = \"\"\n",
    "tweet4 = \"@spdde @hubertus_heil SPD - SCHMAROTZER, P√ÑDOPHILE UND DENUNZIANTEN\"\n",
    "tweet5 = \"SPD - SCHMAROTZER, P√ÑDOPHILE UND DENUNZIANTEN\"\n",
    "tweet6 = \"@user - SCHMAROTZER, P√ÑDOPHILE UND DENUNZIANTEN\"\n",
    "tweet7 = \"Deutsche Medien, Halbwahrheiten und einseitige Betrachtung, wie bei allen vom Staat finanzierten 'billigen' Propagandainstitutionen üòú\"\n",
    "tweet8 = \"Bevor sich PL an Angies Krimigranten-Quoten beteiligen wird, verlassen sie auch die EU u. Konzerne k√∂nnten ihren Mist ins Baltikum fliegen üòú\"\n",
    "\n",
    "print(jaccard(set(tweet1), set(tweet2)))\n",
    "#print(jaccard(set(tweet6), set(tweet5)))\n",
    "print(jaccard_multisets(make_bag(tweet1), make_bag(tweet2)))\n",
    "#print(jaccard_multisets(make_bag(tweet6), make_bag(tweet5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datensatz laden\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train.txt\", mode=\"r\", encoding=\"utf-8\") as in_train:\n",
    "    train = in_train.readlines()\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test.txt\", mode=\"r\", encoding=\"utf-8\") as in_test:\n",
    "    test = in_test.readlines()\n",
    "\n",
    "daten = train[1:] + test[1:] # ohne Erkl√§rungszeile\n",
    "daten = [entry.strip().split(\"\\t\") for entry in daten]\n",
    "\n",
    "# @user-Erw√§hnungen anonymisieren\n",
    "daten_anonym = [(entry[0],anonym_atuser(entry[1]),entry[2],entry[3]) for entry in daten]\n",
    "\n",
    "# Multisets aller Tweets berechnen\n",
    "bags = calculate_bags(daten_anonym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "i 100\n",
      "i 200\n",
      "i 300\n",
      "i 400\n",
      "i 500\n",
      "i 600\n",
      "i 700\n",
      "i 800\n",
      "i 900\n",
      "i 1000\n",
      "i 1100\n",
      "i 1200\n",
      "i 1300\n",
      "i 1400\n",
      "i 1500\n",
      "i 1600\n",
      "i 1700\n",
      "i 1800\n",
      "i 1900\n",
      "i 2000\n",
      "i 2100\n",
      "i 2200\n",
      "i 2300\n",
      "i 2400\n",
      "i 2500\n",
      "i 2600\n",
      "i 2700\n",
      "i 2800\n",
      "i 2900\n",
      "i 3000\n",
      "i 3100\n",
      "i 3200\n",
      "i 3300\n",
      "i 3400\n",
      "i 3500\n",
      "i 3600\n",
      "i 3700\n",
      "i 3800\n",
      "i 3900\n",
      "i 4000\n",
      "i 4100\n",
      "i 4200\n",
      "i 4300\n",
      "i 4400\n",
      "i 4500\n",
      "i 4600\n",
      "i 4700\n",
      "i 4800\n",
      "i 4900\n",
      "i 5000\n",
      "i 5100\n",
      "i 5200\n",
      "i 5300\n",
      "i 5400\n",
      "i 5500\n",
      "i 5600\n",
      "i 5700\n",
      "i 5800\n",
      "i 5900\n",
      "i 6000\n",
      "i 6100\n",
      "i 6200\n",
      "i 6300\n",
      "i 6400\n",
      "i 6500\n",
      "i 6600\n",
      "i 6700\n",
      "i 6800\n",
      "i 6900\n",
      "i 7000\n",
      "i 7100\n",
      "i 7200\n",
      "i 7300\n",
      "i 7400\n",
      "i 7500\n",
      "i 7600\n",
      "i 7700\n",
      "i 7800\n",
      "i 7900\n",
      "i 8000\n",
      "i 8100\n",
      "i 8200\n",
      "i 8300\n",
      "i 8400\n",
      "i 8500\n",
      "i 8600\n",
      "i 8700\n",
      "i 8800\n",
      "i 8900\n",
      "i 9000\n",
      "i 9100\n",
      "i 9200\n",
      "i 9300\n",
      "i 9400\n",
      "i 9500\n",
      "i 9600\n",
      "i 9700\n",
      "i 9800\n",
      "i 9900\n",
      "i 10000\n",
      "i 10100\n",
      "i 10200\n",
      "i 10300\n",
      "i 10400\n",
      "i 10500\n",
      "i 10600\n",
      "i 10700\n",
      "i 10800\n",
      "i 10900\n",
      "i 11000\n",
      "i 11100\n",
      "i 11200\n",
      "i 11300\n",
      "i 11400\n",
      "i 11500\n",
      "i 11600\n",
      "i 11700\n",
      "i 11800\n",
      "i 11900\n",
      "i 12000\n",
      "i 12100\n",
      "i 12200\n",
      "i 12300\n",
      "i 12400\n",
      "i 12500\n",
      "i 12600\n",
      "i 12700\n",
      "i 12800\n",
      "i 12900\n",
      "i 13000\n",
      "i 13100\n",
      "i 13200\n",
      "i 13300\n",
      "i 13400\n",
      "i 13500\n",
      "i 13600\n",
      "i 13700\n",
      "i 13800\n",
      "i 13900\n",
      "i 14000\n",
      "i 14100\n",
      "i 14200\n",
      "i 14300\n",
      "i 14400\n",
      "i 14500\n",
      "i 14600\n",
      "i 14700\n",
      "i 14800\n",
      "i 14900\n",
      "i 15000\n",
      "i 15100\n",
      "i 15200\n",
      "i 15300\n",
      "i 15400\n",
      "i 15500\n",
      "i 15600\n",
      "i 15700\n",
      "i 15800\n",
      "i 15900\n",
      "i 16000\n",
      "i 16100\n",
      "i 16200\n",
      "i 16300\n",
      "i 16400\n",
      "i 16500\n",
      "i 16600\n",
      "i 16700\n",
      "i 16800\n",
      "i 16900\n",
      "i 17000\n",
      "i 17100\n",
      "i 17200\n",
      "i 17300\n",
      "i 17400\n",
      "i 17500\n",
      "i 17600\n",
      "i 17700\n",
      "i 17800\n",
      "i 17900\n",
      "i 18000\n",
      "i 18100\n",
      "i 18200\n",
      "i 18300\n",
      "i 18400\n",
      "i 18500\n",
      "i 18600\n",
      "i 18700\n",
      "i 18800\n",
      "i 18900\n",
      "i 19000\n",
      "i 19100\n",
      "i 19200\n",
      "i 19300\n",
      "i 19400\n",
      "i 19500\n",
      "i 19600\n",
      "i 19700\n",
      "i 19800\n",
      "i 19900\n",
      "i 20000\n",
      "i 20100\n",
      "i 20200\n",
      "i 20300\n",
      "i 20400\n",
      "i 20500\n",
      "i 20600\n",
      "i 20700\n",
      "i 20800\n",
      "i 20900\n",
      "i 21000\n",
      "i 21100\n",
      "i 21200\n",
      "i 21300\n",
      "i 21400\n",
      "i 21500\n",
      "i 21600\n",
      "i 21700\n",
      "i 21800\n",
      "i 21900\n",
      "i 22000\n",
      "i 22100\n",
      "i 22200\n",
      "i 22300\n",
      "i 22400\n",
      "i 22500\n",
      "i 22600\n",
      "i 22700\n",
      "i 22800\n",
      "i 22900\n",
      "i 23000\n",
      "i 23100\n",
      "i 23200\n",
      "i 23300\n",
      "i 23400\n",
      "i 23500\n",
      "i 23600\n"
     ]
    }
   ],
   "source": [
    "# Jaccard-Index berechnen\n",
    "dups_multisets = collect_duplicates(daten_anonym, bags, jaccard_multisets, 0.47)\n",
    "# Berechnung (anfangs): ca. 30 Sekunden/200 Tweets --> insg. ca 70 Minuten\n",
    "# Aber: Wird mit der Zeit schneller (da nach und nach bereits alle Vergleiche in eine Richtung bereits geschehen) --> insg.: 42m 9.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insg. 320 einander in irgendeiner Konstellation √§hnliche Tweets\n",
      "Insg. 101 Cluster\n",
      "Insg. 220 Tweets, die aus dem Datensatz entfernt werden\n"
     ]
    }
   ],
   "source": [
    "# Duplikat-Cluster berechnen\n",
    "clusters, num_dup = sim_clusters(dups_multisets)\n",
    "\n",
    "# Schwarze Liste zu l√∂schender Korpus-IDs erstellen und speichern\n",
    "schwarzeListe = []\n",
    "for gruppe in clusters[2:]:\n",
    "    grp = list(gruppe)\n",
    "    for tweet in grp[1:]:\n",
    "        schwarzeListe.append(tweet[0])\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\schwarze_Liste.txt\", mode=\"w\", encoding=\"utf-8\") as schwout:\n",
    "    for id in schwarzeListe:\n",
    "        schwout.write(id+\"\\n\")\n",
    "\n",
    "# Informationen zu den √§hnlichen Tweets\n",
    "print(f\"Insg. {num_dup} einander in irgendeiner Konstellation √§hnliche Tweets\") # 320\n",
    "print(f\"Insg. {len(clusters)} Cluster\") # 101 Cluster\n",
    "print(f\"Insg. {len(schwarzeListe)} Tweets, die aus dem Datensatz entfernt werden\") # 220\n",
    "\n",
    "\n",
    "# Ohne die Duplikate in neue Dateien schreiben (nicht mehr anonymisiert)\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train_OD.txt\", mode=\"w\", encoding=\"utf-8\") as out_train:\n",
    "    for tweet in train:\n",
    "        if tweet[0] not in schwarzeListe:\n",
    "            out_train.write(tweet)\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test_OD.txt\", mode=\"w\", encoding=\"utf-8\") as out_test:\n",
    "    for tweet in test:\n",
    "        if tweet[0] not in schwarzeListe:\n",
    "            out_test.write(tweet)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bfe21998ac49805528ba5c524b8c99ad95f34df1263c0d2d07976ea5fcbab9a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
