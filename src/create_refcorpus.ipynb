{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referenzdatensatz erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Besonderheit HASOC 2020 Daten: Zeilenumbrüche innerhalb der Tweets\n",
    "# --> notwendig, die Dateien erst so aufzubereiten, dass nur ein Tweet pro Zeile steht\n",
    "\n",
    "# HASOC 2020 Train: \"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_train_new.txt\"\n",
    "# HASOC 2020 Test: \"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_test_new.txt\"\n",
    "\n",
    "def rem_white(filename):\n",
    "    with open(filename, mode=\"r\", encoding=\"utf-16\") as f:\n",
    "        content = f.readlines()\n",
    "        content = content[1:] # Erklärungszeile ignorieren\n",
    "        newcont = []\n",
    "        for i in range(len(content)):\n",
    "            # Fall 1: Zeile ist komplett\n",
    "            if len(content[i].split(\"\\t\")) == 5:\n",
    "                newcont.append(content[i])\n",
    "            # Fall 2: Zeile ist nicht komplett\n",
    "            else:\n",
    "                # letzter Teil einer Zeile erreicht\n",
    "                if \"hasoc_2020_de_\" in content[i]:\n",
    "                    comp_line += content[i]\n",
    "                    comp_line = comp_line.replace(\"\\n\",\" \")\n",
    "                    comp_line += \"\\n\"\n",
    "                    newcont.append(comp_line)\n",
    "                # erster Teil einer Zeile \n",
    "                elif content[i].startswith(\"11\") :\n",
    "                    comp_line = content[i]\n",
    "                # mittlerer Teil einer Zeile, manchmal nur \\n\n",
    "                else: comp_line += content[i]\n",
    "    nwfilename = filename[:len(filename)-4] + \"_formatted\" + \".txt\"\n",
    "    with open(nwfilename, mode=\"w\", encoding=\"utf-8\") as outfile:\n",
    "        for line in newcont:\n",
    "            outfile.write(line)\n",
    "    return True\n",
    "\n",
    "# Bereits formatiert, nicht nochmals durchführen\n",
    "#rem_white(\"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_train_new.txt\")\n",
    "#rem_white(\"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_test_new.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Besonderheit des Covid19-Abusive-Datensatzes: noch nicht unterteilt in Trainings- und Testdaten\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open(\"..\\Korpora\\german-abusive-language-covid-19-main\\covid_2021_dataset.csv\", mode=\"r\", encoding=\"utf-8\") as infile:\n",
    "    content = infile.readlines()\n",
    "    explanation = content[0].split(\"\\t\")\n",
    "    explanation = \"\\t\".join([explanation[0],explanation[1],explanation[2],explanation[3]]) +\"\\n\"\n",
    "    content = content[1:]\n",
    "    sep_content = [entry.split(\"\\t\") for entry in content]\n",
    "    tweets = [(entry[0], entry[1], entry[2]) for entry in sep_content]\n",
    "    labels = [entry[3] for entry in sep_content]\n",
    "\n",
    "    # Separate into train/test\n",
    "    # insg. 4960 Tweets, laut Paper unterteilt in Trainings-Set mit 3485 Tweets (70%), Validations-Set mit 735, und Test-Set mit 740 Tweets\n",
    "    # hier entsprechend der Split 70/30, Validation-Daten werden also im Zweifelsfall von dem Testset abgespalten\n",
    "    traintweets, testtweets, trainlabels, testlabels = train_test_split(tweets, labels, test_size=0.3, train_size=0.7, random_state=4, stratify=labels)\n",
    "\n",
    "    # Reassemble into filewritable content\n",
    "    train = []\n",
    "    test = []\n",
    "    for i, tweet in enumerate(traintweets):\n",
    "        train.append(\"\\t\".join([tweet[0],tweet[1],tweet[2],trainlabels[i]]))\n",
    "    for j, tweet in enumerate(testtweets):\n",
    "        test.append(\"\\t\".join([tweet[0],tweet[1],tweet[2],testlabels[j]]))\n",
    "\n",
    "with open(\"..\\Korpora\\german-abusive-language-covid-19-main\\covid_2021_dataset_train.txt\", mode=\"w\", encoding=\"utf-8\") as trainout:\n",
    "    trainout.write(explanation)\n",
    "    for train_point in train:\n",
    "        trainout.write(train_point+\"\\n\")\n",
    "\n",
    "with open(\"..\\Korpora\\german-abusive-language-covid-19-main\\covid_2021_dataset_test.txt\", mode=\"w\", encoding=\"utf-8\") as testout:\n",
    "    testout.write(explanation)\n",
    "    for test_point in test:\n",
    "        testout.write(test_point+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Besonderheit des GermEval2019-Datensatzes: abgeschnittene erste Zeilen in den Testdaten\n",
    "# --> notwendig, die Dateien aufzubereiten\n",
    "\n",
    "# fehlende Buchstaben am Anfang von Tweets: abgeschnitten von GermEval2019 intern beim Übergang der Daten von\n",
    "# \"Testdata_Subtask12\" zu \"GoldLabelSubtask12\"; Zeilennr. sind die gleichen\n",
    "# z.B. für Zeile 341:\n",
    "# \"enschen, die etwas auf eBay-Kleinanzeigen verticken, wohnen entweder in einer dickichten Gartenlaube oder im obersten Stock Altbau - Hinterhaus. #issso\tOTHER\tOTHER\"\n",
    "\n",
    "\n",
    "# Testdaten: mit den Originaltweets aus der Datei \"germeval2019_Testdata_Subtask12.txt\" (dort ohne Labels) speichern\n",
    "\n",
    "with open(\"..\\Korpora\\GermEval-2019-Data\\germeval2019GoldLabelsSubtask1_2.txt\", mode=\"r\", encoding=\"utf-8\") as in_test:\n",
    "    cont_test = in_test.readlines()\n",
    "    sep_cont = [line.strip().split(\"\\t\") for line in cont_test]\n",
    "\n",
    "# Tweets ohne abgeschnitte Anfänge einlesen\n",
    "with open(\"..\\Korpora\\GermEval-2019-Data\\germeval2019_Testdata_Subtask12.txt\", mode=\"r\", encoding=\"utf-8\") as in_tweets:\n",
    "    tweets = in_tweets.readlines()\n",
    "    tweets = [tweet.strip() for tweet in tweets]\n",
    "\n",
    "# Tweets mit den Labels zusammenführen\n",
    "tweets_replaced = [(tweets[i],sep_cont[i][1],sep_cont[i][2]) for i in range(len(sep_cont))]\n",
    "\n",
    "with open(\"..\\Korpora\\GermEval-2019-Data\\germeval2019GoldLabelsSubtask1_2_ersetzt.txt\", mode=\"w\", encoding=\"utf-8\") as out_test:\n",
    "    for line in tweets_replaced:\n",
    "        out_test.write(\"\\t\".join(line)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_to_refcorp(filename, corp_id, mod):\n",
    "    \"\"\"\n",
    "    GermEval-Daten, HASOC-Daten und die Daten des Covid19-Abusive-Datensatzes in ein einheitliches Format zu übertragen.\n",
    "    Input: Datei mit Tabstopp-getrennten Werten (ob txt, csv oder tsv), Korpus-ID, train/test-Information\n",
    "    Output: Liste von Tupeln à (Referenzkorpus-ID, Tweet, Label1, Label2)\n",
    "            - ReferenzkorpusID - setzt sich zusammen aus der Korpus-ID,\n",
    "                                 md_id = \"11\", falls es um Trainingsdaten (mod=train), \"22\", falls es um Testdaten (mod=test) geht\n",
    "                                 und der Zeilennummer in der Ursprungsdatei;\n",
    "                                 also z.B.: \"01220034\" - für einen Tweet der Zeile 34, aus den Testdaten des GermEval2018-Datensatzes\n",
    "            - Tweet            - String des Tweets, URLs sind mit der generischen Twitter-URL \"https://t.co\" ersetzt\n",
    "            - Label1           -\n",
    "            - Label2           -\n",
    "    \"\"\"\n",
    "    newcorp = []\n",
    "    with open(filename, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.readlines()\n",
    "        \n",
    "        # erste Zeile ignorieren bei Covid19 (\"05\") und HASOC2019 (\"03\")\n",
    "        if corp_id == \"05\" or corp_id == \"03\": text = text[1:]\n",
    "\n",
    "        # Bestimmen, welche Formatierungsfunktion genutzt wird\n",
    "        if corp_id == \"01\" or corp_id == \"02\": form_func = format_germeval\n",
    "        elif corp_id == \"03\" or corp_id == \"04\": form_func = format_hasoc\n",
    "        else: form_func = format_covidabusive\n",
    "\n",
    "        url_pattern = re.compile('https:\\/\\/.*?(?: |$)')\n",
    "\n",
    "        for num, entry in enumerate(text):\n",
    "            entry = entry.strip()\n",
    "            tag1, tag2 = \"NOT\", \"NOT\"\n",
    "\n",
    "            tweet, tag1, tag2 = form_func(entry, tag1, tag2)\n",
    "\n",
    "            # URLs mit generischer Twitter-URL ersetzen\n",
    "            tweet = url_pattern.sub(\"https://t.co \", tweet)\n",
    "            tweet = tweet.strip()\n",
    "\n",
    "            # Tweet von HTML-Resten entfernen und Emoji-Codierung mit Emojis ersetzen\n",
    "            tweet = clean_tweet(tweet)\n",
    "\n",
    "            # gedoppelte und überflüssige Anführungszeichen entfernen\n",
    "            tweet = tweet.replace('\"\"',\"'\")\n",
    "            tweet = tweet.strip('\"')\n",
    "\n",
    "            # ID erstellen\n",
    "            if mod == \"train\": md_id = \"11\"\n",
    "            elif mod ==\"test\": md_id = \"22\"\n",
    "            id_num = f'{num+1:04d}'\n",
    "            tweet_id = str(corp_id) + str(md_id) + str(id_num)\n",
    "            \n",
    "            # der neuen Sammlung hinzufügen\n",
    "            newcorp.append((tweet_id, tweet, tag1, tag2))\n",
    "    return newcorp\n",
    "\n",
    "def format_germeval(entry, tag1, tag2):\n",
    "    tweet, label1, label2 = entry.split(\"\\t\")\n",
    "    if label1 == \"OFFENSE\": tag1 = \"NEG\"\n",
    "    if label2 == \"INSULT\": tag2 = \"INSOFF\"\n",
    "    elif label2 == \"PROFANITY\": tag2 = \"PRFN\"\n",
    "    elif label2 == \"ABUSE\": tag2 = \"HATE\"\n",
    "    tweet = tweet.replace(\"|LBR|\", \" \")\n",
    "    return tweet, tag1, tag2\n",
    "\n",
    "def format_covidabusive(entry, tag1, tag2):\n",
    "    sep = entry.split(\"\\t\")\n",
    "    tweet, l1 = sep[1], sep[3]\n",
    "    tag2 = \"NAN\"\n",
    "    if l1 == \"abusive\": tag1 = \"NEG\"\n",
    "    return tweet, tag1, tag2\n",
    "\n",
    "def format_hasoc(entry, tag1, tag2):\n",
    "    sep = entry.split(\"\\t\")\n",
    "    tweet, l1, l2 = sep[1], sep[2], sep[3]\n",
    "    if l1 == \"HOF\": tag1 = \"NEG\"\n",
    "    if l2 not in [\"HATE\", \"OFFN\", \"PRFN\"] and l1!=\"NOT\": print(l2)\n",
    "    if l2 == \"HATE\": tag2 = \"HATE\"\n",
    "    elif l2 == \"OFFN\": tag2 = \"INSOFF\"\n",
    "    elif l2 == \"PRFN\": tag2 = \"PRFN\"\n",
    "    return tweet, tag1, tag2\n",
    "\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    \"\"\"Emojis finden und ersetzen (gefunden: https://stackoverflow.com/questions/67507017/replace-unicode-code-point-with-actual-character-using-regex)\n",
    "        und HTML-Reste entfernen.\n",
    "    \"\"\"\n",
    "    cleaned = tweet\n",
    "    # Emojis, die als Text, z.B. \"<U+0001F60A>\", gespeichert sind: als utf-8 formatieren\n",
    "    cleaned = re.sub(r'<U\\+([A-F0-9]+)>', lambda x: chr(int(x.group(1), 16)), cleaned)\n",
    "    cleaned = re.sub(r\"&lt;\" , \"<\", cleaned)\t \n",
    "    cleaned = re.sub(r\"&gt;\" , \">\", cleaned)    \n",
    "    cleaned = re.sub(r\"&amp;\" , \"&\", cleaned)\n",
    "    cleaned = re.sub(r'\\\"', '\"', cleaned)\n",
    "    cleaned = re.sub(r'\\\"\"', '\"', cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GermEval2018\n",
    "germeval2018train_converted = convert_to_refcorp(\"..\\Korpora\\GermEval-2018-Data-master\\germeval2018.training.txt\", \"01\", \"train\")\n",
    "germeval2018test_converted = convert_to_refcorp(\"..\\Korpora\\GermEval-2018-Data-master\\germeval2018.test.txt\", \"01\", \"test\")\n",
    "\n",
    "# GermEval2019\n",
    "germeval2019train_converted = convert_to_refcorp(\"..\\Korpora\\GermEval-2019-Data\\germeval2019.training_subtask1_2_korrigiert.txt\", \"02\", \"train\")\n",
    "germeval2019test_converted = convert_to_refcorp(\"..\\Korpora\\GermEval-2019-Data\\germeval2019GoldLabelsSubtask1_2_ersetzt.txt\", \"02\", \"test\")\n",
    "\n",
    "# HASOC 2019\n",
    "hasoc2019train_converted = convert_to_refcorp(\"..\\Korpora\\german_dataset_hasoc2019\\german_dataset\\german_dataset.tsv\", \"03\", \"train\")\n",
    "hasoc2019test_converted = convert_to_refcorp(\"..\\Korpora\\german_dataset_hasoc2019\\german_dataset\\hasoc_de_test_gold.tsv\", \"03\", \"test\")\n",
    "\n",
    "# HASOC 2020\n",
    "hasoc2020train_converted = convert_to_refcorp(\"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_train_new_formatted.txt\", \"04\", \"train\")\n",
    "hasoc2020test_converted = convert_to_refcorp(\"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_test_new_formatted.txt\", \"04\", \"test\")\n",
    "\n",
    "# Covid19 Abusive\n",
    "covidabusivetrain_converted = convert_to_refcorp(\"..\\Korpora\\german-abusive-language-covid-19-main\\covid_2021_dataset_train.txt\", \"05\", \"train\")\n",
    "covidabusivetest_converted = convert_to_refcorp(\"..\\Korpora\\german-abusive-language-covid-19-main\\covid_2021_dataset_test.txt\", \"05\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referenzdatensatz zusammenstellen\n",
    "# Bereits ausgeführt, nicht noch einmal ausführen\n",
    "\n",
    "refcorp_train = germeval2018train_converted + germeval2019train_converted + hasoc2019train_converted + hasoc2020train_converted + covidabusivetrain_converted\n",
    "refcorp_test = germeval2018test_converted + germeval2019test_converted + hasoc2019test_converted + hasoc2020test_converted + covidabusivetest_converted\n",
    "\n",
    "# refcorp_gsmt = refcorp_train + refcorp_test # insg. 28596 Tweets\n",
    "# refcorp_gsmt_HATE = [entry for entry in refcorp_gsmt if entry[3] == \"HATE\"] # insg. 3060 Tweets\n",
    "\n",
    "import random\n",
    "\n",
    "random.shuffle(refcorp_train)\n",
    "random.shuffle(refcorp_test)\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train.txt\", mode=\"w\", encoding=\"utf-8\") as reftrainout:\n",
    "    reftrainout.write(\"corpus_id\\ttweet\\tbinarylabel\\tfinelabel\\n\")\n",
    "    for reftweet in refcorp_train:\n",
    "        reftrainout.write(\"\\t\".join(reftweet)+\"\\n\")\n",
    "    \n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test.txt\", mode=\"w\", encoding=\"utf-8\") as reftestout:\n",
    "    reftestout.write(\"corpus_id\\ttweet\\tbinarylabel\\tfinelabel\\n\")\n",
    "    for reftweet in refcorp_test:\n",
    "        reftestout.write(\"\\t\".join(reftweet)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Anonymisierung der @user-Erwähnungen zu \"@user\n",
    "2. Duplikate entfernen (bei Paaren sehr ähnlicher Tweets einen entfernen)\n",
    "3. Mit den verschiedenen Annotationsebenen- und detaills separat speichern\n",
    "\n",
    "##### 1. Anonymisierung der @user-Erwähnungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def anonym_atuser(tweet):\n",
    "    tweet_anonym = re.sub('@[^@ ]+?@', '@user@', tweet)\n",
    "    tweet_anonym = re.sub('@[^@ ]+? ', '@user ', tweet_anonym)\n",
    "    tweet_anonym = re.sub('@[^@ ]+?$', '@user', tweet_anonym)\n",
    "    return tweet_anonym\n",
    "\n",
    "# von '@rspctfl@houelle_beck @ergroovt @ThomasMichael71 @ksemann2 @DrKassandraPari Dann hättest Du nichts dagegen, wenn ein Lehrer ein Zeichen d PKK im Unterricht trägt? Oder die Flagge Israels? Oder ein Anstecker der AfD? @Innenwelttramp'\n",
    "# zu '@user@user @user @user @user @user Dann hättest Du nichts dagegen, wenn ein Lehrer ein Zeichen d PKK im Unterricht trägt? Oder die Flagge Israels? Oder ein Anstecker der AfD? @user'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Duplikate entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard-Index-Funktionen zur Berechnung der Ähnlichkeit\n",
    "\n",
    "def make_bag(string1):\n",
    "    \"\"\" String als Multiset speichern; \n",
    "        In Form eines dicts: jedes vorkommende Unigram als Key, die Anzahl der Nutzungen als Value\n",
    "        Als zusätzlichen Wert auch die Länge des Strings speichern\n",
    "    \"\"\"\n",
    "    stringbag = dict()\n",
    "    stringset = set(string1)\n",
    "    for ch1 in stringset:\n",
    "        stringbag[ch1] = string1.count(ch1)\n",
    "    stringbag[\"len\"] = len(string1)\n",
    "    stringbag[\"tok\"] = set()\n",
    "    stringbag[\"tok\"] = set(stringbag.keys()) - {\"len\", \"tok\"}\n",
    "    return stringbag\n",
    "\n",
    "def jaccard(set1, set2):\n",
    "    \"\"\"Jaccard-Index von zwei Sets (von zwei Strings) berechnen.\n",
    "       Max.-Wert: 1 (Gleichheit)\n",
    "    \"\"\"\n",
    "    jaccard_index = len(set1 & set2) / len(set1 | set2)\n",
    "    return jaccard_index\n",
    "\n",
    "def jaccard_multisets(bag1, bag2):\n",
    "    \"\"\"Den Jaccard-Index für zwei Multisets (von zwei Strings) berechnen.\n",
    "       Max.-Wert: 0.5\n",
    "    \"\"\"\n",
    "    # Schnittmenge bauen, die Gesamtlänge speichern\n",
    "    schnitt_len = 0\n",
    "    schnitt = bag1[\"tok\"] & bag2[\"tok\"]\n",
    "    for gram in schnitt:\n",
    "        l = min(bag1[gram],bag2[gram])\n",
    "        schnitt_len += l\n",
    "\n",
    "    # Länge der Vereinigung ermitteln\n",
    "    vereinigung_len = bag1[\"len\"] + bag2[\"len\"]\n",
    "    \n",
    "    jaccard_index = schnitt_len / vereinigung_len\n",
    "    return jaccard_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bags(corpus):\n",
    "    \"\"\"Multisets (bags) für alle Tweets in einem Korpus berechen.\n",
    "    Input: Korpus\n",
    "    Output: Liste von Multisets (Index in der Liste == Index in der Korpusliste)\n",
    "    \"\"\"\n",
    "    bags = [make_bag(tweet[1]) for tweet in corpus]\n",
    "    return bags\n",
    "\n",
    "def calculate_sets(corpus):\n",
    "    \"\"\"Set für alle Tweets in einem Korpus berechnen; als Liste der Sets speichern.\n",
    "    \"\"\"\n",
    "    sets = [set(tweet[1]) for tweet in corpus]\n",
    "    return sets\n",
    "\n",
    "def collect_duplicates(corpus, sets, simfunc, cutoff):\n",
    "    \"\"\" Duplikate sammeln\n",
    "    Input: Korpus, Liste von (Multi)Sets für alle Tweets im Korpus, Vergleichsfunktion, Ähnlichkeitsgrenzwert\n",
    "    Output: Liste potentieller Duplikate (als Tupel der Korpuseinträge)\n",
    "    \"\"\"\n",
    "    duplicates = []\n",
    "    for i in range(len(corpus)):\n",
    "        # if i % 100 == 0: print(\"i\",i) # Zur Zeitmessung\n",
    "        for j in range(len(corpus)):\n",
    "            # Kein Vergleich mit sich selbst oder bereits in die andere Richtung verglichener Paare\n",
    "            if i >= j: continue\n",
    "            else:\n",
    "                jacc = simfunc(sets[i],sets[j])\n",
    "                if jacc >= cutoff: # z.B. 0.47 für multisets\n",
    "                    duplicates.append((corpus[i],corpus[j]))\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train.txt\", mode=\"r\", encoding=\"utf-8\") as in_train:\n",
    "    train = in_train.readlines()\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test.txt\", mode=\"r\", encoding=\"utf-8\") as in_test:\n",
    "    test = in_test.readlines()\n",
    "\n",
    "total = train[1:] + test[1:] # ohne Erklärungszeile\n",
    "total = [entry.strip().split(\"\\t\") for entry in total]\n",
    "\n",
    "\n",
    "# @user-Erwähnungen anonymisieren\n",
    "total_anonym = [(entry[0],anonym_atuser(entry[1]),entry[2],entry[3]) for entry in total]\n",
    "\n",
    "# Sets aller Tweets berechnen\n",
    "tweet_sets = calculate_sets(total_anonym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard-Index berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bags = calculate_bags(total_anonym) # Multimenge für alle Tweets berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard-Index berechnen\n",
    "dups_multisets = collect_duplicates(total_anonym, bags, jaccard_multisets, 0.47)\n",
    "# Berechnung (anfangs): ca. 30 Sekunden/200 Tweets --> insg. ca 70 Minuten\n",
    "# Aber: Wird mit der Zeit schneller (da nach und nach bereits alle Vergleiche in eine Richtung bereits geschehen) --> insg.: 42m 9.9s\n",
    "\n",
    "#import json\n",
    "#dups_json = json.dumps(dups_multisets)\n",
    "#with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\Duplikat_IDs.json\", mode=\"w\", encoding=\"utf-8\") as out_dups:\n",
    "#    out_dups.write(dups_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplikat-Cluster bestimmen, um Kettenvergleiche zu vermeiden\n",
    "# --> falls ID1 ähnlich ID2: zusammen in ein Set, und jede weitere ID, die einer der beiden IDs ähnlich ist dazu\n",
    "\n",
    "cluster = []\n",
    "ID_cluster_ref = dict()\n",
    "for duplicate_tup in dups_multisets:\n",
    "    tweet1, tweet2 = duplicate_tup\n",
    "    ID1, ID2 = tweet1[0], tweet2[0]\n",
    "    # 1. weder ID1 noch ID2 vorhanden\n",
    "    if (ID1 not in ID_cluster_ref) and (ID2 not in ID_cluster_ref):\n",
    "        cluster_num = len(cluster)\n",
    "        ID_cluster_ref[ID1], ID_cluster_ref[ID2] = cluster_num, cluster_num\n",
    "        cluster.append({tweet1, tweet2})\n",
    "    # 2. eine von beiden vorhanden --> beide IDs ins vorhandene Cluster integrieren\n",
    "    elif ID1 in ID_cluster_ref:\n",
    "        cluster_num = ID_cluster_ref[ID1]\n",
    "        ID_cluster_ref[ID2] = cluster_num\n",
    "        cluster[cluster_num].add(tweet2)\n",
    "    elif ID2 in ID_cluster_ref:\n",
    "        cluster_num = ID_cluster_ref[ID2]\n",
    "        ID_cluster_ref[ID1] = cluster_num\n",
    "        cluster[cluster_num].add(tweet1)\n",
    "    # 3. Beide bereits vorhanden\n",
    "    else:\n",
    "        assert ID_cluster_ref[ID1] == ID_cluster_ref[ID2]\n",
    "\n",
    "# schwarze Liste zu löschender IDs\n",
    "schwarzeListe = []\n",
    "for gruppe in cluster[2:]:\n",
    "    grp = list(gruppe)\n",
    "    for tweet in grp[1:]:\n",
    "        schwarzeListe.append(tweet[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ID_cluster_ref)) # 321 Tweets, die einander in irgendeiner Konstellation ähnlich sind\n",
    "print(len(cluster)) # 100 Cluster\n",
    "print(cluster[0]) \n",
    "print(len(schwarzeListe)) # 223 zu löschende Tweets\n",
    "\n",
    "# Cluster 0: vergessene erste Zeile (Erklärung) --> TODO nochmal laufen lassen, json neu speichern\n",
    "# Cluster 1: Ähnlichkeit durch wiederholte @user Erwähnungen, Text selbst nicht ähnlich\n",
    "# tatsächlich ähnlich, nur einen der Tweets behalten (Zufall): Clusters 2 - 8 \n",
    "# Cluster 4: 04110200 und zwei weitere HASOC2020 Tweets: \"https:/…\" als Rest -> mit generischem Link ersetzten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ohne die Duplikate in neue Dateien schreiben (nicht mehr anonymisiert)\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train_OD.txt\", mode=\"w\", encoding=\"utf-8\") as out_train:\n",
    "    for tweet in train:\n",
    "        if tweet[0] not in schwarzeListe:\n",
    "            out_train.write(tweet)\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test_OD.txt\", mode=\"w\", encoding=\"utf-8\") as out_test:\n",
    "    for tweet in test:\n",
    "        if tweet[0] not in schwarzeListe:\n",
    "            out_test.write(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bfe21998ac49805528ba5c524b8c99ad95f34df1263c0d2d07976ea5fcbab9a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
