{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referenzdatensatz erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Besonderheit HASOC 2020 Daten: Zeilenumbrüche innerhalb der Tweets\n",
    "# --> notwendig, die Dateien erst so aufzubereiten, dass nur ein Tweet pro Zeile steht\n",
    "\n",
    "# HASOC 2020 Train: \"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_train_new.txt\"\n",
    "# HASOC 2020 Test: \"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_test_new.txt\"\n",
    "\n",
    "def rem_white(filename):\n",
    "    with open(filename, mode=\"r\", encoding=\"utf-16\") as f:\n",
    "        content = f.readlines()\n",
    "        content = content[1:] # Erklärungszeile ignorieren\n",
    "        newcont = []\n",
    "        for i in range(len(content)):\n",
    "            # Fall 1: Zeile ist komplett\n",
    "            if len(content[i].split(\"\\t\")) == 5:\n",
    "                newcont.append(content[i])\n",
    "            # Fall 2: Zeile ist nicht komplett\n",
    "            else:\n",
    "                # letzter Teil einer Zeile erreicht\n",
    "                if \"hasoc_2020_de_\" in content[i]:\n",
    "                    comp_line += content[i]\n",
    "                    comp_line = comp_line.replace(\"\\n\",\" \")\n",
    "                    comp_line += \"\\n\"\n",
    "                    newcont.append(comp_line)\n",
    "                    comp_line = \"\" # Zeilenakkumulator zurücksetzen\n",
    "                # erster Teil einer Zeile \n",
    "                elif content[i].startswith(\"11\") :\n",
    "                    comp_line = content[i]\n",
    "                # mittlerer Teil einer Zeile, manchmal nur \\n\n",
    "                else: comp_line += content[i]\n",
    "    nwfilename = filename[:len(filename)-4] + \"_formatted\" + \".txt\"\n",
    "    with open(nwfilename, mode=\"w\", encoding=\"utf-8\") as outfile:\n",
    "        for line in newcont:\n",
    "            outfile.write(line)\n",
    "    return True\n",
    "\n",
    "# Bereits formatiert, nicht nochmals durchführen\n",
    "# rem_white(\"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_train_new.txt\")\n",
    "# rem_white(\"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_test_new.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Besonderheit des Covid19-Abusive-Datensatzes: noch nicht unterteilt in Trainings- und Testdaten\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open(\"..\\Korpora\\german-abusive-language-covid-19-main\\covid_2021_dataset.csv\", mode=\"r\", encoding=\"utf-8\") as infile:\n",
    "    content = infile.readlines()\n",
    "    explanation = content[0].split(\"\\t\")\n",
    "    explanation = \"\\t\".join([explanation[0],explanation[1],explanation[2],explanation[3]]) +\"\\n\"\n",
    "    content = content[1:]\n",
    "    sep_content = [entry.split(\"\\t\") for entry in content]\n",
    "    tweets = [(entry[0], entry[1], entry[2]) for entry in sep_content]\n",
    "    labels = [entry[3] for entry in sep_content]\n",
    "\n",
    "    # Separate into train/test\n",
    "    # insg. 4960 Tweets, laut Paper unterteilt in Trainings-Set mit 3485 Tweets (70%), Validation-Set mit 735, und Test-Set mit 740 Tweets\n",
    "    # hier entsprechend der Split 70/30, Validation-Daten werden also im Zweifelsfall von dem Testset abgespalten\n",
    "    traintweets, testtweets, trainlabels, testlabels = train_test_split(tweets, labels, test_size=0.3, train_size=0.7, random_state=4, stratify=labels)\n",
    "\n",
    "    # Reassemble into filewritable content\n",
    "    train = []\n",
    "    test = []\n",
    "    for i, tweet in enumerate(traintweets):\n",
    "        train.append(\"\\t\".join([tweet[0],tweet[1],tweet[2],trainlabels[i]]))\n",
    "    for j, tweet in enumerate(testtweets):\n",
    "        test.append(\"\\t\".join([tweet[0],tweet[1],tweet[2],testlabels[j]]))\n",
    "\n",
    "with open(\"..\\Korpora\\german-abusive-language-covid-19-main\\covid_2021_dataset_train.txt\", mode=\"w\", encoding=\"utf-8\") as trainout:\n",
    "    trainout.write(explanation)\n",
    "    for train_point in train:\n",
    "        trainout.write(train_point+\"\\n\")\n",
    "\n",
    "with open(\"..\\Korpora\\german-abusive-language-covid-19-main\\covid_2021_dataset_test.txt\", mode=\"w\", encoding=\"utf-8\") as testout:\n",
    "    testout.write(explanation)\n",
    "    for test_point in test:\n",
    "        testout.write(test_point+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_to_refcorp(filename, corp_id, mod):\n",
    "    \"\"\"\n",
    "    GermEval-Daten, HASOC-Daten und die Daten des Covid19-Abusive-Datensatzes in ein einheitliches Format zu übertragen.\n",
    "    Input: Datei mit Tabstopp-getrennten Werten (ob txt, csv oder tsv), Korpus-ID, train/test-Information\n",
    "    Output: Liste von Tupeln à (Referenzkorpus-ID, Tweet, Label1, Label2)\n",
    "            - ReferenzkorpusID - setzt sich zusammen aus der Korpus-ID,\n",
    "                                 md_id = \"11\", falls es um Trainingsdaten (mod=train), \"22\", falls es um Testdaten (mod=test) geht\n",
    "                                 und der Zeilennummer in der Ursprungsdatei;\n",
    "                                 also z.B.: \"01220034\" - für einen Tweet der Zeile 34, aus den Testdaten des GermEval2018-Datensatzes\n",
    "            - Tweet            - String des Tweets, URLs sind mit der generischen Twitter-URL \"https://t.co\" ersetzt\n",
    "            - Label1           -\n",
    "            - Label2           -\n",
    "    \"\"\"\n",
    "    newcorp = []\n",
    "    with open(filename, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.readlines()\n",
    "        \n",
    "        # erste Zeile ignorieren bei Covid19 (\"05\") und HASOC2019 (\"03\")\n",
    "        if corp_id == \"05\" or corp_id == \"03\": text = text[1:]\n",
    "\n",
    "        # Bestimmen, welche Formatierungsfunktion genutzt wird\n",
    "        if corp_id == \"01\" or corp_id == \"02\": form_func = format_germeval\n",
    "        elif corp_id == \"03\" or corp_id == \"04\": form_func = format_hasoc\n",
    "        else: form_func = format_covidabusive\n",
    "\n",
    "        url_pattern = re.compile('https:\\/\\/.*?(?: |$)')\n",
    "\n",
    "        for num, entry in enumerate(text):\n",
    "            entry = entry.strip()\n",
    "            tag1, tag2 = \"NOT\", \"NOT\"\n",
    "\n",
    "            tweet, tag1, tag2 = form_func(entry, tag1, tag2)\n",
    "\n",
    "            # URLs mit generischer Twitter-URL ersetzen\n",
    "            tweet = url_pattern.sub(\"https://t.co \", tweet)\n",
    "            tweet = tweet.strip()\n",
    "\n",
    "            # ID erstellen\n",
    "            if mod == \"train\": md_id = \"11\"\n",
    "            elif mod ==\"test\": md_id = \"22\"\n",
    "            id_num = f'{num:04d}'\n",
    "            tweet_id = str(corp_id) + str(md_id) + str(id_num)\n",
    "            \n",
    "            # der neuen Sammlung hinzufügen\n",
    "            newcorp.append((tweet_id, tweet, tag1, tag2))\n",
    "    return newcorp\n",
    "\n",
    "def format_germeval(entry, tag1, tag2):\n",
    "    tweet, label1, label2 = entry.split(\"\\t\")\n",
    "    if label1 == \"OFFENSE\": tag1 = \"NEG\"\n",
    "    if label2 == \"INSULT\": tag2 = \"INSOFF\"\n",
    "    elif label2 == \"PROFANITY\": tag2 = \"PRFN\"\n",
    "    elif label2 == \"ABUSE\": tag2 = \"HATE\"\n",
    "    tweet = tweet.replace(\"|LBR|\", \" \")\n",
    "    return tweet, tag1, tag2\n",
    "\n",
    "def format_covidabusive(entry, tag1, tag2):\n",
    "    sep = entry.split(\"\\t\")\n",
    "    tweet, l1 = sep[1], sep[3]\n",
    "    tag2 = \"NAN\"\n",
    "    if l1 == \"abuse\": tag1 = \"NEG\"\n",
    "    return tweet, tag1, tag2\n",
    "\n",
    "def format_hasoc(entry, tag1, tag2):\n",
    "    sep = entry.split(\"\\t\")\n",
    "    tweet, l1, l2 = sep[1], sep[2], sep[3]\n",
    "    if l1 == \"HOF\": tag1 = \"NEG\"\n",
    "    if l2 == \"HATE\": tag2 = \"HATE\"\n",
    "    elif l2 == \"OFFN\": tag2 = \"INSOFF\"\n",
    "    elif l2 == \"PRFN\": tag2 = \"PRFN\"\n",
    "    return tweet, tag1, tag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GermEval2018\n",
    "# Train: \"..\\Korpora\\GermEval-2018-Data-master\\germeval2018.training.txt\"\n",
    "# Test: \"..\\Korpora\\GermEval-2018-Data-master\\germeval2018.test.txt\"\n",
    "germeval2018train_converted = convert_to_refcorp(\"..\\Korpora\\GermEval-2018-Data-master\\germeval2018.training.txt\", \"01\", \"train\")\n",
    "germeval2018test_converted = convert_to_refcorp(\"..\\Korpora\\GermEval-2018-Data-master\\germeval2018.test.txt\", \"01\", \"test\")\n",
    "\n",
    "# GermEval2019\n",
    "# Train: \"..\\Korpora\\GermEval-2019-Data\\germeval2019.training_subtask1_2_korrigiert.txt\"\n",
    "# Test: \"..\\Korpora\\GermEval-2019-Data\\germeval2019GoldLabelsSubtask1_2.txt\"\n",
    "germeval2019train_converted = convert_to_refcorp(\"..\\Korpora\\GermEval-2019-Data\\germeval2019.training_subtask1_2_korrigiert.txt\", \"02\", \"train\")\n",
    "germeval2019test_converted = convert_to_refcorp(\"..\\Korpora\\GermEval-2019-Data\\germeval2019GoldLabelsSubtask1_2.txt\", \"02\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HASOC 2019\n",
    "# Train: Korpora\\german_dataset_hasoc2019\\german_dataset\\german_dataset.tsv\n",
    "# Test: Korpora\\german_dataset_hasoc2019\\german_dataset\\hasoc_de_test_gold.tsv\n",
    "hasoc2019train_converted = convert_to_refcorp(\"..\\Korpora\\german_dataset_hasoc2019\\german_dataset\\german_dataset.tsv\", \"03\", \"train\")\n",
    "hasoc2019test_converted = convert_to_refcorp(\"..\\Korpora\\german_dataset_hasoc2019\\german_dataset\\hasoc_de_test_gold.tsv\", \"03\", \"test\")\n",
    "\n",
    "# HASOC 2020\n",
    "# Train: Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_train_new_formatted.txt\n",
    "# Test: Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_test_new_formatted.txt\n",
    "hasoc2020train_converted = convert_to_refcorp(\"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_train_new_formatted.txt\", \"04\", \"train\")\n",
    "hasoc2020test_converted = convert_to_refcorp(\"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_test_new_formatted.txt\", \"04\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covid19 Abusive\n",
    "\n",
    "# Korpus ursprünglich: \"..\\Korpora\\german-abusive-language-covid-19-main\\covid_2021_dataset.csv\"\n",
    "# Train (neu): \"..\\Korpora\\german-abusive-language-covid-19-main\\covid_2021_dataset_train.txt\"\n",
    "# Test (neu): \"..\\Korpora\\german-abusive-language-covid-19-main\\covid_2021_dataset_test.txt\"\n",
    "\n",
    "covidabusivetrain_converted = convert_to_refcorp(\"..\\Korpora\\german-abusive-language-covid-19-main\\covid_2021_dataset_train.txt\", \"05\", \"train\")\n",
    "covidabusivetest_converted = convert_to_refcorp(\"..\\Korpora\\german-abusive-language-covid-19-main\\covid_2021_dataset_test.txt\", \"05\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Referenzdatensatz zusammenstellen\n",
    "# Bereits ausgeführt, nicht noch einmal ausführen\n",
    "\n",
    "#refcorp_train = germeval2018train_converted + germeval2019train_converted + hasoc2019train_converted + hasoc2020train_converted + covidabusivetrain_converted\n",
    "#random.shuffle(refcorp_train)\n",
    "\n",
    "#refcorp_test = germeval2018test_converted + germeval2019test_converted + hasoc2019test_converted + hasoc2020test_converted + covidabusivetest_converted\n",
    "#random.shuffle(refcorp_test)\n",
    "\n",
    "#with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train.txt\", mode=\"w\", encoding=\"utf-8\") as reftrainout:\n",
    "#    reftrainout.write(\"corpus_id\\ttweet\\tbinarylabel\\tfinelabel\\n\")\n",
    "#    for reftweet in refcorp_train:\n",
    "#        reftrainout.write(\"\\t\".join(reftweet)+\"\\n\")\n",
    "    \n",
    "#with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test.txt\", mode=\"w\", encoding=\"utf-8\") as reftestout:\n",
    "#    reftestout.write(\"corpus_id\\ttweet\\tbinarylabel\\tfinelabel\\n\")\n",
    "#    for reftweet in refcorp_test:\n",
    "#        reftestout.write(\"\\t\".join(reftweet)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausschnittdatensätze (Train, Test) erstellen, in dem nur die Einträge mit dem feinen Label \"HATE\" vorkommen\n",
    "# Bereits erstellt, nicht nochmals ausführen\n",
    "\n",
    "#with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test.txt\", mode=\"r\", encoding=\"utf-8\") as hatetrainin:\n",
    "#    all_cont = hatetrainin.readlines()\n",
    "#    all_cont = all_cont[1:]\n",
    "#    sep_cont = [entry.strip().split(\"\\t\") for entry in all_cont]\n",
    "#    hate = []\n",
    "#    for tweet in sep_cont:\n",
    "#        if tweet[3] == \"HATE\": hate.append(tweet)\n",
    "\n",
    "#with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test_HATE.txt\", mode=\"w\", encoding=\"utf-8\") as hatetrainout:\n",
    "#    for line in hate:\n",
    "#        hatetrainout.write(\"\\t\".join(line)+\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f5bd9176b6ece2dd16a47700ecec5d54d78ec8bf8f49cccf5029b25d8b95c52"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
