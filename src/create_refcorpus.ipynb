{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referenzdatensatz erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Besonderheit HASOC 2020 Daten: Zeilenumbrüche innerhalb der Tweets\n",
    "# --> notwendig, die Dateien erst so aufzubereiten, dass nur ein Tweet pro Zeile steht\n",
    "\n",
    "# HASOC 2020 Train: \"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_train_new.txt\"\n",
    "# HASOC 2020 Test: \"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_test_new.txt\"\n",
    "\n",
    "def rem_white(filename):\n",
    "    with open(filename, mode=\"r\", encoding=\"utf-16\") as f:\n",
    "        content = f.readlines()\n",
    "        content = content[1:] # Erklärungszeile ignorieren\n",
    "        newcont = []\n",
    "        for i in range(len(content)):\n",
    "            # Fall 1: Zeile ist komplett\n",
    "            if len(content[i].split(\"\\t\")) == 5:\n",
    "                newcont.append(content[i])\n",
    "            # Fall 2: Zeile ist nicht komplett\n",
    "            else:\n",
    "                # letzter Teil einer Zeile erreicht\n",
    "                if \"hasoc_2020_de_\" in content[i]:\n",
    "                    comp_line += content[i]\n",
    "                    comp_line = comp_line.replace(\"\\n\",\" \")\n",
    "                    comp_line += \"\\n\"\n",
    "                    newcont.append(comp_line)\n",
    "                    comp_line = \"\" # Zeilenakkumulator zurücksetzen\n",
    "                # erster Teil einer Zeile \n",
    "                elif content[i].startswith(\"11\") :\n",
    "                    comp_line = content[i]\n",
    "                # mittlerer Teil einer Zeile, manchmal nur \\n\n",
    "                else: comp_line += content[i]\n",
    "    nwfilename = filename[:len(filename)-4] + \"_formatted\" + \".txt\"\n",
    "    with open(nwfilename, mode=\"w\", encoding=\"utf-8\") as outfile:\n",
    "        for line in newcont:\n",
    "            outfile.write(line)\n",
    "    return True\n",
    "\n",
    "# Bereits formatiert, nicht nochmals durchführen\n",
    "# rem_white(\"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_train_new.txt\")\n",
    "# rem_white(\"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_test_new.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Besonderheit des Covid19-Abusive-Datensatzes: noch nicht unterteilt in Trainings- und Testdaten\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open(\"..\\Korpora\\german-abusive-language-covid-19-main\\covid_2021_dataset.csv\", mode=\"r\", encoding=\"utf-8\") as infile:\n",
    "    content = infile.readlines()\n",
    "    explanation = content[0].split(\"\\t\")\n",
    "    explanation = \"\\t\".join([explanation[0],explanation[1],explanation[2],explanation[3]]) +\"\\n\"\n",
    "    content = content[1:]\n",
    "    sep_content = [entry.split(\"\\t\") for entry in content]\n",
    "    tweets = [(entry[0], entry[1], entry[2]) for entry in sep_content]\n",
    "    labels = [entry[3] for entry in sep_content]\n",
    "\n",
    "    # Separate into train/test\n",
    "    # insg. 4960 Tweets, laut Paper unterteilt in Trainings-Set mit 3485 Tweets (70%), Validations-Set mit 735, und Test-Set mit 740 Tweets\n",
    "    # hier entsprechend der Split 70/30, Validation-Daten werden also im Zweifelsfall von dem Testset abgespalten\n",
    "    traintweets, testtweets, trainlabels, testlabels = train_test_split(tweets, labels, test_size=0.3, train_size=0.7, random_state=4, stratify=labels)\n",
    "\n",
    "    # Reassemble into filewritable content\n",
    "    train = []\n",
    "    test = []\n",
    "    for i, tweet in enumerate(traintweets):\n",
    "        train.append(\"\\t\".join([tweet[0],tweet[1],tweet[2],trainlabels[i]]))\n",
    "    for j, tweet in enumerate(testtweets):\n",
    "        test.append(\"\\t\".join([tweet[0],tweet[1],tweet[2],testlabels[j]]))\n",
    "\n",
    "with open(\"..\\Korpora\\german-abusive-language-covid-19-main\\covid_2021_dataset_train.txt\", mode=\"w\", encoding=\"utf-8\") as trainout:\n",
    "    trainout.write(explanation)\n",
    "    for train_point in train:\n",
    "        trainout.write(train_point+\"\\n\")\n",
    "\n",
    "with open(\"..\\Korpora\\german-abusive-language-covid-19-main\\covid_2021_dataset_test.txt\", mode=\"w\", encoding=\"utf-8\") as testout:\n",
    "    testout.write(explanation)\n",
    "    for test_point in test:\n",
    "        testout.write(test_point+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Besonderheit des GermEval2019-Datensatzes: abgeschnittene erste Zeilen in den Testdaten\n",
    "# --> notwendig, die Dateien aufzubereiten\n",
    "\n",
    "# fehlende Buchstaben am Anfang von Tweets: abgeschnitten von GermEval2019 intern beim Übergang der Daten von\n",
    "# \"Testdata_Subtask12\" zu \"GoldLabelSubtask12\"; Zeilennr. sind die gleichen\n",
    "# z.B. für Zeile 341:\n",
    "# \"enschen, die etwas auf eBay-Kleinanzeigen verticken, wohnen entweder in einer dickichten Gartenlaube oder im obersten Stock Altbau - Hinterhaus. #issso\tOTHER\tOTHER\"\n",
    "\n",
    "\n",
    "# Testdaten: mit den Originaltweets aus der Datei \"germeval2019_Testdata_Subtask12.txt\" (dort ohne Labels) speichern\n",
    "\n",
    "with open(\"..\\Korpora\\GermEval-2019-Data\\germeval2019GoldLabelsSubtask1_2.txt\", mode=\"r\", encoding=\"utf-8\") as in_test:\n",
    "    cont_test = in_test.readlines()\n",
    "    sep_cont = [line.strip().split(\"\\t\") for line in cont_test]\n",
    "\n",
    "# Tweets ohne abgeschnitte Anfänge einlesen\n",
    "with open(\"..\\Korpora\\GermEval-2019-Data\\germeval2019_Testdata_Subtask12.txt\", mode=\"r\", encoding=\"utf-8\") as in_tweets:\n",
    "    tweets = in_tweets.readlines()\n",
    "    tweets = [tweet.strip() for tweet in tweets]\n",
    "\n",
    "# Tweets mit den Labels zusammenführen\n",
    "tweets_replaced = [(tweets[i],sep_cont[i][1],sep_cont[i][2]) for i in range(len(sep_cont))]\n",
    "\n",
    "with open(\"..\\Korpora\\GermEval-2019-Data\\germeval2019GoldLabelsSubtask1_2_ersetzt.txt\", mode=\"w\", encoding=\"utf-8\") as out_test:\n",
    "    for line in tweets_replaced:\n",
    "        out_test.write(\"\\t\".join(line)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_to_refcorp(filename, corp_id, mod):\n",
    "    \"\"\"\n",
    "    GermEval-Daten, HASOC-Daten und die Daten des Covid19-Abusive-Datensatzes in ein einheitliches Format zu übertragen.\n",
    "    Input: Datei mit Tabstopp-getrennten Werten (ob txt, csv oder tsv), Korpus-ID, train/test-Information\n",
    "    Output: Liste von Tupeln à (Referenzkorpus-ID, Tweet, Label1, Label2)\n",
    "            - ReferenzkorpusID - setzt sich zusammen aus der Korpus-ID,\n",
    "                                 md_id = \"11\", falls es um Trainingsdaten (mod=train), \"22\", falls es um Testdaten (mod=test) geht\n",
    "                                 und der Zeilennummer in der Ursprungsdatei;\n",
    "                                 also z.B.: \"01220034\" - für einen Tweet der Zeile 34, aus den Testdaten des GermEval2018-Datensatzes\n",
    "            - Tweet            - String des Tweets, URLs sind mit der generischen Twitter-URL \"https://t.co\" ersetzt\n",
    "            - Label1           -\n",
    "            - Label2           -\n",
    "    \"\"\"\n",
    "    newcorp = []\n",
    "    with open(filename, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.readlines()\n",
    "        \n",
    "        # erste Zeile ignorieren bei Covid19 (\"05\") und HASOC2019 (\"03\")\n",
    "        if corp_id == \"05\" or corp_id == \"03\": text = text[1:]\n",
    "\n",
    "        # Bestimmen, welche Formatierungsfunktion genutzt wird\n",
    "        if corp_id == \"01\" or corp_id == \"02\": form_func = format_germeval\n",
    "        elif corp_id == \"03\" or corp_id == \"04\": form_func = format_hasoc\n",
    "        else: form_func = format_covidabusive\n",
    "\n",
    "        url_pattern = re.compile('https:\\/\\/.*?(?: |$)')\n",
    "\n",
    "        for num, entry in enumerate(text):\n",
    "            entry = entry.strip()\n",
    "            tag1, tag2 = \"NOT\", \"NOT\"\n",
    "\n",
    "            tweet, tag1, tag2 = form_func(entry, tag1, tag2)\n",
    "\n",
    "            # URLs mit generischer Twitter-URL ersetzen\n",
    "            tweet = url_pattern.sub(\"https://t.co \", tweet)\n",
    "            tweet = tweet.strip()\n",
    "\n",
    "            # Tweet von HTML-Resten entfernen und Emoji-Codierung mit Emojis ersetzen\n",
    "            tweet = clean_tweet(tweet)\n",
    "\n",
    "            # ID erstellen\n",
    "            if mod == \"train\": md_id = \"11\"\n",
    "            elif mod ==\"test\": md_id = \"22\"\n",
    "            id_num = f'{num+1:04d}'\n",
    "            tweet_id = str(corp_id) + str(md_id) + str(id_num)\n",
    "            \n",
    "            # der neuen Sammlung hinzufügen\n",
    "            newcorp.append((tweet_id, tweet, tag1, tag2))\n",
    "    return newcorp\n",
    "\n",
    "def format_germeval(entry, tag1, tag2):\n",
    "    tweet, label1, label2 = entry.split(\"\\t\")\n",
    "    if label1 == \"OFFENSE\": tag1 = \"NEG\"\n",
    "    if label2 == \"INSULT\": tag2 = \"INSOFF\"\n",
    "    elif label2 == \"PROFANITY\": tag2 = \"PRFN\"\n",
    "    elif label2 == \"ABUSE\": tag2 = \"HATE\"\n",
    "    tweet = tweet.replace(\"|LBR|\", \" \")\n",
    "    return tweet, tag1, tag2\n",
    "\n",
    "def format_covidabusive(entry, tag1, tag2):\n",
    "    sep = entry.split(\"\\t\")\n",
    "    tweet, l1 = sep[1], sep[3]\n",
    "    tag2 = \"NAN\"\n",
    "    if l1 == \"abusive\": tag1 = \"NEG\"\n",
    "    return tweet, tag1, tag2\n",
    "\n",
    "def format_hasoc(entry, tag1, tag2):\n",
    "    sep = entry.split(\"\\t\")\n",
    "    tweet, l1, l2 = sep[1], sep[2], sep[3]\n",
    "    if l1 == \"HOF\": tag1 = \"NEG\"\n",
    "    if l2 == \"HATE\": tag2 = \"HATE\"\n",
    "    elif l2 == \"OFFN\": tag2 = \"INSOFF\"\n",
    "    elif l2 == \"PRFN\": tag2 = \"PRFN\"\n",
    "    return tweet, tag1, tag2\n",
    "\n",
    "\n",
    "\n",
    "# - \n",
    "# - html-Reste: \"&amp;\" anstatt \"&\", \"\\''\" ansatt '\"' und \"&gt;\" anstatt \">\" usw. >> in die utf-8-Formatierung überführen\n",
    "def clean_tweet(tweet):\n",
    "    \"\"\"Emojis finden und ersetzen (gefunden: https://stackoverflow.com/questions/67507017/replace-unicode-code-point-with-actual-character-using-regex)\n",
    "        und HTML-Reste entfernen.\n",
    "    \"\"\"\n",
    "    cleaned = tweet\n",
    "    # Emojis, die als Text, z.B. \"<U+0001F60A>\", gespeichert sind: als utf-8 formatieren\n",
    "    cleaned = re.sub(r'<U\\+([A-F0-9]+)>', lambda x: chr(int(x.group(1), 16)), cleaned)\n",
    "    cleaned = re.sub(r\"&lt;\" , \"<\", cleaned)\t \n",
    "    cleaned = re.sub(r\"&gt;\" , \">\", cleaned)    \n",
    "    cleaned = re.sub(r\"&amp;\" , \"&\", cleaned)\n",
    "    cleaned = re.sub(r'\\\"', '\"', cleaned)\n",
    "    cleaned = re.sub(r'\\\"\"', '\"', cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GermEval2018\n",
    "# Train: \"..\\Korpora\\GermEval-2018-Data-master\\germeval2018.training.txt\"\n",
    "# Test: \"..\\Korpora\\GermEval-2018-Data-master\\germeval2018.test.txt\"\n",
    "germeval2018train_converted = convert_to_refcorp(\"..\\Korpora\\GermEval-2018-Data-master\\germeval2018.training.txt\", \"01\", \"train\")\n",
    "germeval2018test_converted = convert_to_refcorp(\"..\\Korpora\\GermEval-2018-Data-master\\germeval2018.test.txt\", \"01\", \"test\")\n",
    "\n",
    "# GermEval2019\n",
    "# Train: \"..\\Korpora\\GermEval-2019-Data\\germeval2019.training_subtask1_2_korrigiert.txt\"\n",
    "# Test: \"..\\Korpora\\GermEval-2019-Data\\germeval2019GoldLabelsSubtask1_2_ersetzt.txt\"\n",
    "germeval2019train_converted = convert_to_refcorp(\"..\\Korpora\\GermEval-2019-Data\\germeval2019.training_subtask1_2_korrigiert.txt\", \"02\", \"train\")\n",
    "germeval2019test_converted = convert_to_refcorp(\"..\\Korpora\\GermEval-2019-Data\\germeval2019GoldLabelsSubtask1_2_ersetzt.txt\", \"02\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HASOC 2019\n",
    "# Train: Korpora\\german_dataset_hasoc2019\\german_dataset\\german_dataset.tsv\n",
    "# Test: Korpora\\german_dataset_hasoc2019\\german_dataset\\hasoc_de_test_gold.tsv\n",
    "hasoc2019train_converted = convert_to_refcorp(\"..\\Korpora\\german_dataset_hasoc2019\\german_dataset\\german_dataset.tsv\", \"03\", \"train\")\n",
    "hasoc2019test_converted = convert_to_refcorp(\"..\\Korpora\\german_dataset_hasoc2019\\german_dataset\\hasoc_de_test_gold.tsv\", \"03\", \"test\")\n",
    "\n",
    "# HASOC 2020\n",
    "# Train: Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_train_new_formatted.txt\n",
    "# Test: Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_test_new_formatted.txt\n",
    "hasoc2020train_converted = convert_to_refcorp(\"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_train_new_formatted.txt\", \"04\", \"train\")\n",
    "hasoc2020test_converted = convert_to_refcorp(\"..\\Korpora\\German_2020_hasoc\\German\\hasoc_2020_de_test_new_formatted.txt\", \"04\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covid19 Abusive\n",
    "\n",
    "# Korpus ursprünglich: \"..\\Korpora\\german-abusive-language-covid-19-main\\covid_2021_dataset.csv\"\n",
    "# Train (neu): \"..\\Korpora\\german-abusive-language-covid-19-main\\covid_2021_dataset_train.txt\"\n",
    "# Test (neu): \"..\\Korpora\\german-abusive-language-covid-19-main\\covid_2021_dataset_test.txt\"\n",
    "\n",
    "covidabusivetrain_converted = convert_to_refcorp(\"..\\Korpora\\german-abusive-language-covid-19-main\\covid_2021_dataset_train.txt\", \"05\", \"train\")\n",
    "covidabusivetest_converted = convert_to_refcorp(\"..\\Korpora\\german-abusive-language-covid-19-main\\covid_2021_dataset_test.txt\", \"05\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referenzdatensatz zusammenstellen\n",
    "# Bereits ausgeführt, nicht noch einmal ausführen\n",
    "# zweimal ausgeführt (korrigierte Formatierungen und IDs) --> Shuffle hat die Reihenfolge verändert\n",
    "# (allerdings irrelevant, da alle Tweets IDs haben)\n",
    "\n",
    "refcorp_train = germeval2018train_converted + germeval2019train_converted + hasoc2019train_converted + hasoc2020train_converted + covidabusivetrain_converted\n",
    "refcorp_test = germeval2018test_converted + germeval2019test_converted + hasoc2019test_converted + hasoc2020test_converted + covidabusivetest_converted\n",
    "\n",
    "# refcorp_gsmt = refcorp_train + refcorp_test # insg. 28596 Tweets\n",
    "# refcorp_gsmt_HATE = [entry for entry in refcorp_gsmt if entry[3] == \"HATE\"] # insg. 3060 Tweets\n",
    "\n",
    "import random\n",
    "\n",
    "random.shuffle(refcorp_train)\n",
    "random.shuffle(refcorp_test)\n",
    "\n",
    "#with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train.txt\", mode=\"w\", encoding=\"utf-8\") as reftrainout:\n",
    "#    reftrainout.write(\"corpus_id\\ttweet\\tbinarylabel\\tfinelabel\\n\")\n",
    "#    for reftweet in refcorp_train:\n",
    "#        reftrainout.write(\"\\t\".join(reftweet)+\"\\n\")\n",
    "    \n",
    "#with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test.txt\", mode=\"w\", encoding=\"utf-8\") as reftestout:\n",
    "#    reftestout.write(\"corpus_id\\ttweet\\tbinarylabel\\tfinelabel\\n\")\n",
    "#    for reftweet in refcorp_test:\n",
    "#        reftestout.write(\"\\t\".join(reftweet)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausschnittdatensätze (Train, Test) erstellen, in dem nur die Einträge mit dem feinen Label \"HATE\" vorkommen\n",
    "# Bereits erstellt, nicht nochmals ausführen\n",
    "# zweimal ausgeführt (korrigierte Formatierungen und IDs) --> Shuffle hat die Reihenfolge der Ausgangsdatei verändert\n",
    "# (allerdings irrelevant, da alle Tweets IDs haben)\n",
    "\n",
    "\n",
    "#with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test.txt\", mode=\"r\", encoding=\"utf-8\") as hatetrainin:\n",
    "#    all_cont = hatetrainin.readlines()\n",
    "#    all_cont = all_cont[1:]\n",
    "#    sep_cont = [entry.strip().split(\"\\t\") for entry in all_cont]\n",
    "#    hate = []\n",
    "#    for tweet in sep_cont:\n",
    "#        if tweet[3] == \"HATE\": hate.append(tweet)\n",
    "\n",
    "#with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test_HATE.txt\", mode=\"w\", encoding=\"utf-8\") as hatetrainout:\n",
    "#    for line in hate:\n",
    "#        hatetrainout.write(\"\\t\".join(line)+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test - HATE Dateien des Referenzdatensatzes als JSON-Dateien speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test_HATE.txt\", mode=\"r\", encoding=\"utf-8\") as intxt:\n",
    "#    tws = intxt.readlines()\n",
    "#    conts = [elem.strip(\"\\n\").split(\"\\t\") for elem in tws]\n",
    "#    cont_dicts = [{\"id\":elem[0], \"text\":elem[1], \"tag1\":elem[2], \"tag2\":elem[3]} for elem in conts]\n",
    "\n",
    "#import json\n",
    "\n",
    "#with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test_HATE.json\", mode=\"w\", encoding=\"utf-8\") as jsonout:\n",
    "#    prep_cont = json.dumps(cont_dicts)\n",
    "#    jsonout.write(prep_cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation: Logikprüfung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_anno(datensatz):\n",
    "    \"\"\"Annotationslogik checken und Problemfälle sammeln\"\"\"\n",
    "    probleme = []\n",
    "    for entry in datensatz:\n",
    "        if not anno_logik_check(entry[\"label\"]): probleme.append(entry)\n",
    "    return probleme\n",
    "\n",
    "\n",
    "def anno_logik_check(labelset):\n",
    "    \"\"\" Die Annotationslogik checken:\n",
    "        Input: Labelmenge als Liste\n",
    "        Output: True (falls alles korrekt) / False (falls irgendein Problem vorliegt)    \n",
    "    \"\"\"\n",
    "    labelset = set(labelset)\n",
    "    korrekt = True\n",
    "    if len(labelset) == 0:\n",
    "        korrekt = False\n",
    "    elif (len(labelset) == 1) and (\"KEINE\" not in labelset):\n",
    "        korrekt = False\n",
    "    else:\n",
    "        # 1. VVH-ALLG interne Logik\n",
    "        if \"VVH-ALLG\" in labelset:\n",
    "            # Mind. eine Art der Tathandlung ggb.\n",
    "            if {\"Aufstachelung zu Hass\", \"Aufforderung zu Gewalt- oder Willkürmaßnahmen\", \"Angriff der Menschenwürde\"} & labelset == set():\n",
    "                korrekt = False\n",
    "            # Mind. eine Gruppe genannt\n",
    "            if {\"Nationalität\", 'ethnische Herkunft / \"Rasse\"', \"Religion / Weltanschauung\",\n",
    "                    \"Politische Einstellung\", \"Geschlecht\", \"Anderes Merkmal\"} & labelset == set():\n",
    "                korrekt = False\n",
    "            # keine VVH-NS Labels\n",
    "            if {\"VVH-NS\", \"Billigen\", \"Verherrlichen\", \"Verharmlosen\", \"Leugnen\", \"Rechtfertigen\"} & labelset != set():\n",
    "                korrekt = False\n",
    "            if \"KEINE\" in labelset: korrekt = False\n",
    "\n",
    "        # 2. VVH-NS interne Logik\n",
    "        if \"VVH-NS\" in labelset:\n",
    "            # Mind. eine Art der Tathandlung ggb.\n",
    "            if {\"Billigen\", \"Verherrlichen\", \"Verharmlosen\", \"Leugnen\", \"Rechtfertigen\"} & labelset == set(): \n",
    "                korrekt = False\n",
    "            # keine VVH-ALLG Labels\n",
    "            if {\"VVH-ALLG\", \"Aufstachelung zu Hass\", \"Aufforderung zu Gewalt- oder Willkürmaßnahmen\", \"Angriff der Menschenwürde\"} & labelset != set():\n",
    "                korrekt = False \n",
    "            if \"KEINE\" in labelset: korrekt = False\n",
    "        \n",
    "        # 3. Gruppenmerkmale - Logik (in beide Richtungen)\n",
    "\n",
    "        nation = {\"Türkischstämmige Deutsche\", \"Marokkaner:innen\", \"In Deutschland lebende Ausländer:innen\",\n",
    "                    \"Asiat:innen\", \"Pol:innen\", \"Afrikaner:innen\", \"Syrer:innen\", \"Palästinenser:innen\"}\n",
    "        herkunft = {\"POC\", \"Araber:in\"}\n",
    "        religion = {\"Muslim:e/innen\", \"Juden/Jüdinnen\", \"Christ:innen\"}\n",
    "        polit = {\"Die Grünen\", \"die SPD\", \"die Linke\", \"CDU/CSU\", \"AfD\", \"Nazis\", \"Islamist:innen\", \"Kommunist:innen\",\n",
    "                    \"Zionist:innen\", \"NPD\", \"FDP\", \"FridaysForFuture\", \"Pegida\", \"Anarchist:in\"}\n",
    "        geschlecht = {\"Trans/NB-Personen\", \"Frauen\", \"Männer\"}\n",
    "        andere = {\"Flüchtlinge\", \"Asylbewerber:innen\", \"Sich illegal in Deutschland aufhaltende Personen\", \"Migrant:innen\", \"Vorbestrafte\",\n",
    "                    \"Veganer\", \"Senior:innen\", \"Lesben, Schwule, Bi\", \"Kinder\", \"Jugendliche\", \"Polizist:innen\", \"Obdachlose\",\n",
    "                    \"Richter:innen\", \"Analphabet:innen\", \"Soldat:innen\", \"Behinderte\"}\n",
    "\n",
    "        # Richtung 1: falls Gruppe vorhanden, korrektes Merkmal auch vorhanden\n",
    "        for i in nation & labelset:\n",
    "            if \"Nationalität\" not in labelset:\n",
    "                korrekt = False\n",
    "        for j in herkunft & labelset:\n",
    "            if 'ethnische Herkunft / \"Rasse\"' not in labelset:\n",
    "                korrekt = False\n",
    "        for k in religion & labelset:\n",
    "            if \"Religion / Weltanschauung\" not in labelset:\n",
    "                korrekt = False\n",
    "        for l in polit & labelset:\n",
    "            if \"Politische Einstellung\" not in labelset:\n",
    "                korrekt = False\n",
    "        for m in geschlecht & labelset:\n",
    "            if \"Geschlecht\" not in labelset:\n",
    "                korrekt = False\n",
    "        for n in andere & labelset:\n",
    "            if \"Anderes Merkmal\" not in labelset:\n",
    "                korrekt = False\n",
    "\n",
    "        # Richtung 2: falls Merkmal vorhanden, auch eine passende Gruppe vorhanden\n",
    "        if \"Nationalität\" in labelset:\n",
    "            if nation & labelset == set():\n",
    "                korrekt = False\n",
    "        if 'ethnische Herkunft / \"Rasse\"' in labelset:\n",
    "            if herkunft & labelset == set():\n",
    "                korrekt = False\n",
    "        if \"Religion / Weltanschauung\" in labelset:\n",
    "            if religion & labelset == set():\n",
    "                korrekt = False\n",
    "        if \"Politische Einstellung\" in labelset:\n",
    "            if polit & labelset == set():\n",
    "                korrekt = False\n",
    "        if \"Geschlecht\" in labelset:\n",
    "            if geschlecht & labelset == set():\n",
    "                korrekt = False\n",
    "        if \"Anderes Merkmal\" in labelset:\n",
    "            if andere & labelset == set():\n",
    "                korrekt = False\n",
    "\n",
    "    return korrekt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Probleme in der Annotationslogik händisch durchgehen\n",
    "\n",
    "import json\n",
    "\n",
    "# Jeweils für die Trainingsdaten und die Testdaten\n",
    "# Beispieleintrag der Annotations-Json-Datei:\n",
    "# {\"id\": \"01112520\", \"data\": \"@SteiblBarbara @Thomas_S_Wagner @RitaKratzert Weitaus schlimmer. Heute ist es nicht mehr Dummheit.  Ein ganzes Volk ist zu (m)wutlosen Zombies dressiert worden.\", \"label\": [\"KEINE\"], \"tag1\": \"NEG\", \"tag2\": \"HATE\"}\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train_HATE_anno_korrigiert.json\", mode=\"r\", encoding=\"utf-8\") as trainf:\n",
    "    train_anno = trainf.read()\n",
    "    train_annotations = json.loads(train_anno)\n",
    "\n",
    "probleme = check_anno(train_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Korrekturen für die Trainingsdaten\n",
    " \n",
    "new01110262 = {'id': '01110262', 'label': ['KEINE', 'Flüchtlinge', 'Afrikaner:innen', 'Nationalität', 'Anderes Merkmal']}\n",
    "new03113327 = {'id': '03113327', 'label': ['Angriff der Menschenwürde', 'VVH-ALLG', 'POC', 'ethnische Herkunft / \"Rasse\"', 'Kinder', 'Die Grünen', 'Politische Einstellung', 'Anderes Merkmal'] }\n",
    "new01114326 = {'id': '01114326', 'label': ['Muslim:e/innen', 'Religion / Weltanschauung', 'Islamist:innen', 'Aufstachelung zu Hass', 'VVH-ALLG', 'Politische Einstellung']}\n",
    "new01111028 = {'id': '01111028', 'label': ['KEINE', 'Araber:in', 'ethnische Herkunft / \"Rasse\"']}\n",
    "new03113648 = {'id': '03113648', 'label': ['KEINE', 'Politische Einstellung', 'Nazis']}\n",
    "new01112254 = {'id': '01112254', 'label': ['KEINE', 'Pol:innen', 'In Deutschland lebende Ausländer:innen', 'Nationalität']}\n",
    "new02110710 = {'id': '02110710', 'label': ['KEINE', 'Araber:in', 'In Deutschland lebende Ausländer:innen', 'ethnische Herkunft / \"Rasse\"', 'POC', 'Afrikaner:innen', 'Nationalität']}\n",
    "new04111621 = {'id': '04111621', 'label': ['KEINE']}\n",
    "new03112728 = {'id': '03112728', 'label': ['KEINE']}\n",
    "\n",
    "# Entspricht damit nicht mehr der ursprünglichen Logik; Entscheidung: Palästinenser:innen eher Nationalität als Polit. Einstellung\n",
    "# >> alle entsprechenden Einträge umschreiben\n",
    "new02113786 = {'id': '02113786', 'label': ['KEINE', 'Palästinenser:innen', 'Nationalität']}\n",
    "new01113048 = {'id': '01113048', 'label': ['Palästinenser:innen', 'Nationalität', 'VVH-ALLG', 'Aufstachelung zu Hass'], 'tag1': 'NEG', 'tag2': 'HATE'}\n",
    "new01110156 = {'id': '01110156', 'label': ['Palästinenser:innen', 'Nationalität', 'VVH-ALLG', 'Aufstachelung zu Hass'], 'tag1': 'NEG', 'tag2': 'HATE'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Korrekturen für die Testdaten\n",
    "\n",
    "new01222281 = {'id': '01222281', 'label': ['Die Grünen', 'Türkischstämmige Deutsche','Nationalität', 'Politische Einstellung', 'KEINE']}\n",
    "new02220290 = {'id': '02220290', 'label': ['KEINE', 'die SPD', 'Politische Einstellung']}\n",
    "new01221146  = {'id': '01221146', 'label': ['KEINE', 'die SPD', 'Politische Einstellung']}\n",
    "new01220517 = {'id': '01220517', 'label': ['KEINE', 'In Deutschland lebende Ausländer:innen', 'Pol:innen', 'Migrant:innen', 'Nationalität', 'Anderes Merkmal']}\n",
    "new02221882 = {'id': '02221882', 'label': ['KEINE', 'Zionist:innen', 'Palästinenser:innen', 'Politische Einstellung', 'Nationalität']}\n",
    "new01223120 = {'id': '01223120', 'label': ['KEINE']}\n",
    "new01221025 = {'id': '01221025', 'label': ['Verharmlosen', 'VVH-NS']}\n",
    "new02222036 = {'id': '02222036', 'label': ['KEINE', 'Palästinenser:innen', 'Nationalität']}\n",
    "new01220250 = {'id': '01220250', 'label': ['KEINE', 'Palästinenser:innen', 'Nationalität']}\n",
    "new02221762 = {'id': '02221762', 'label': ['KEINE', 'AfD', 'Politische Einstellung', 'Islamist:innen', 'Araber:in', 'In Deutschland lebende Ausländer:innen', 'Nationalität', 'ethnische Herkunft / \"Rasse\"']}\n",
    "new01223162 = {'id': '01223162', 'label': ['KEINE', 'Islamist:innen', 'Politische Einstellung']}\n",
    "new02221146 = {'id': '02221146', 'label': ['Nazis', 'Aufforderung zu Gewalt- oder Willkürmaßnahmen', 'VVH-ALLG', 'Politische Einstellung']}\n",
    "new01220249 = {'id': '01220249', 'label': ['KEINE', 'In Deutschland lebende Ausländer:innen', 'Nationalität']}\n",
    "new02221991 = {'id': '02221991', 'label': ['KEINE', 'Palästinenser:innen', 'Zionist:innen', 'Politische Einstellung', 'Nationalität']}\n",
    "new02220270 = {'id': '02220270', 'label': ['KEINE', 'Araber:in', 'Afrikaner:innen', 'In Deutschland lebende Ausländer:innen', 'Nationalität', 'Religion / Weltanschauung', 'Muslim:e/innen', 'ethnische Herkunft / \"Rasse\"']}\n",
    "new02220311 = {'id': '02220311', 'label': ['KEINE', 'Araber:in', 'In Deutschland lebende Ausländer:innen', 'Afrikaner:innen', 'Muslim:e/innen', 'Religion / Weltanschauung', 'Nationalität', 'ethnische Herkunft / \"Rasse\"']}\n",
    "new01222167 = {'id': '01222167', 'label': ['KEINE', 'In Deutschland lebende Ausländer:innen', 'Islamist:innen', 'Die Grünen', 'Politische Einstellung', 'Asylbewerber:innen', 'Anderes Merkmal', 'Nationalität']}\n",
    "new01222700 = {'id': '01222700', 'label': ['KEINE', 'Anderes Merkmal', 'Sich illegal in Deutschland aufhaltende Personen']}\n",
    "new01222612 = {'id': '01222612', 'label': ['KEINE', 'Palästinenser:innen', 'Nationalität']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erkannte Fehler: Trainings- und Testannotationsdateien umschreiben, inkl. der neuen IDs (je +1)\n",
    "# in neue Dateien\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train_HATE_annotiert.json\", mode=\"r\", encoding=\"utf-8\") as trainf:\n",
    "    train_anno = trainf.read()\n",
    "    train_annotations = json.loads(train_anno)\n",
    "\n",
    "korrekturen_train = [new01110262, new03113327, new01114326, new01111028, new03113648, new01112254, new02110710, new04111621, new03112728, new02113786, new01113048, new01110156]\n",
    "for i in train_annotations:\n",
    "    for j in korrekturen_train:\n",
    "        if i[\"id\"] == j[\"id\"]:\n",
    "            i[\"label\"] = j[\"label\"]\n",
    "    i[\"id\"] = i[\"id\"][:4] + f'{int(i[\"id\"][4:])+1:04d}'\n",
    "\n",
    "\n",
    "train_anno_k = json.dumps(train_annotations)\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train_HATE_anno_korrigiert.json\", mode=\"w\", encoding=\"utf-8\") as trainf_k:\n",
    "    trainf_k.write(train_anno_k)\n",
    "\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test_HATE_annotiert.json\", mode=\"r\", encoding=\"utf-8\") as testf:\n",
    "    test_anno = testf.read()\n",
    "    test_annotations = json.loads(test_anno)\n",
    "\n",
    "korrekturen_test = [new01222281, new02220290, new01221146, new01220517, new02221882, new01223120, new01221025, new02222036, new01220250,\n",
    "                    new02221762, new01223162, new02221146, new01220249, new02221991, new02220270, new02220311, new01222167, new01222700, new01222612]\n",
    "\n",
    "for i in test_annotations:\n",
    "    for j in korrekturen_test:\n",
    "        if i[\"id\"] == j[\"id\"]:\n",
    "            i[\"label\"] = j[\"label\"]\n",
    "    i[\"id\"] = i[\"id\"][:4] + f'{int(i[\"id\"][4:])+1:04d}'\n",
    "\n",
    "test_anno_k = json.dumps(test_annotations)\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test_HATE_anno_korrigiert.json\", mode=\"w\", encoding=\"utf-8\") as testf_k:\n",
    "    testf_k.write(test_anno_k)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bfe21998ac49805528ba5c524b8c99ad95f34df1263c0d2d07976ea5fcbab9a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
