{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing des Referenzdatensatzes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Anonymisierung der @user-Erw√§hnungen zu \"@user\n",
    "2. Duplikate entfernen (bei Paaren sehr √§hnlicher Tweets einen entfernen)\n",
    "3. Mit den verschiedenen Annotationsebenen- und detaills separat speichern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Anonymisierung der @user-Erw√§hnungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def anonym_atuser(tweet):\n",
    "    tweet_anonym = re.sub('@[^@ ]+?@', '@user@', tweet)\n",
    "    tweet_anonym = re.sub('@[^@ ]+? ', '@user ', tweet_anonym)\n",
    "    tweet_anonym = re.sub('@[^@ ]+?$', '@user', tweet_anonym)\n",
    "    return tweet_anonym\n",
    "\n",
    "# von '@rspctfl@houelle_beck @ergroovt @ThomasMichael71 @ksemann2 @DrKassandraPari Dann h√§ttest Du nichts dagegen, wenn ein Lehrer ein Zeichen d PKK im Unterricht tr√§gt? Oder die Flagge Israels? Oder ein Anstecker der AfD? @Innenwelttramp'\n",
    "# zu '@user@user @user @user @user @user Dann h√§ttest Du nichts dagegen, wenn ein Lehrer ein Zeichen d PKK im Unterricht tr√§gt? Oder die Flagge Israels? Oder ein Anstecker der AfD? @user'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Duplikate entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard-Index-Funktionen zur Berechnung der √Ñhnlichkeit\n",
    "\n",
    "def make_bag(string1):\n",
    "    \"\"\" String als Multiset speichern; \n",
    "        In Form eines dicts: jedes vorkommende Unigram als Key, die Anzahl der Nutzungen als Value\n",
    "        Als zus√§tzlichen Wert auch die L√§nge des Strings speichern\n",
    "    \"\"\"\n",
    "    stringbag = dict()\n",
    "    stringset = set(string1)\n",
    "    for ch1 in stringset:\n",
    "        stringbag[ch1] = string1.count(ch1)\n",
    "    stringbag[\"len\"] = len(string1)\n",
    "    return stringbag\n",
    "\n",
    "def jaccard(set1, set2):\n",
    "    \"\"\"Jaccard-Index von zwei Sets (von zwei Strings) berechnen.\n",
    "       Max.-Wert: 1 (Gleichheit)\n",
    "    \"\"\"\n",
    "    jaccard_index = len(set1 & set2) / len(set1 | set2)\n",
    "    return jaccard_index\n",
    "\n",
    "def jaccard_multisets(bag1, bag2):\n",
    "    \"\"\"Den Jaccard-Index f√ºr zwei Multisets (von zwei Strings) berechnen.\n",
    "       Max.-Wert: 0.5\n",
    "    \"\"\"\n",
    "    # Schnittmenge bauen, die Gesamtl√§nge speichern\n",
    "    schnitt_len = 0\n",
    "    tok1 = set(bag1.keys()) - {\"len\"}\n",
    "    tok2 = set(bag2.keys()) - {\"len\"}\n",
    "    for gram in tok1:\n",
    "        if gram in tok2:\n",
    "            l = min(bag1[gram],bag2[gram])\n",
    "            schnitt_len += l\n",
    "\n",
    "    # L√§nge der Vereinigung ermitteln\n",
    "    vereinigung_len = bag1[\"len\"] + bag2[\"len\"]\n",
    "    \n",
    "    jaccard_index = schnitt_len / vereinigung_len\n",
    "    return jaccard_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6341463414634146\n"
     ]
    }
   ],
   "source": [
    "tweet1 = \"@Karl_Lauterbach Besser ein Amateur der lernf√§hig ist, als das verlauste Pack der abgefuckten SPD - SCHMAROTZER, P√ÑDOPHILE und DENUNZIANTEN !!!\"\n",
    "tweet2 = \"@ThomasOppermann SPD - SCHMAROTZER,P√ÑDOPHILE UND DENUNZIANTEN   oder   SCHEINHEILGSTE PARTEI DEUTSCHLANDS !!!\"\n",
    "tweet3 = \"\"\n",
    "tweet4 = \"@spdde @hubertus_heil SPD - SCHMAROTZER, P√ÑDOPHILE UND DENUNZIANTEN\"\n",
    "tweet5 = \"SPD - SCHMAROTZER, P√ÑDOPHILE UND DENUNZIANTEN\"\n",
    "tweet8 = \"@user - SCHMAROTZER, P√ÑDOPHILE UND DENUNZIANTEN\"\n",
    "tweet6 = \"Deutsche Medien, Halbwahrheiten und einseitige Betrachtung, wie bei allen vom Staat finanzierten 'billigen' Propagandainstitutionen üòú\"\n",
    "tweet7 = \"Bevor sich PL an Angies Krimigranten-Quoten beteiligen wird, verlassen sie auch die EU u. Konzerne k√∂nnten ihren Mist ins Baltikum fliegen üòú\"\n",
    "\n",
    "#jind = jaccard_multisets(make_bag(tweet6), make_bag(tweet7))\n",
    "#print(jind)\n",
    "\n",
    "jacc = jaccard(set(tweet6), set(tweet7))\n",
    "print(jacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bags(corpus):\n",
    "    \"\"\"Multisets (bags) f√ºr alle Tweets in einem Korpus berechen.\n",
    "    Input: Korpus\n",
    "    Output: Liste von Multisets (Index in der Liste == Index in der Korpusliste)\n",
    "    \"\"\"\n",
    "    bags = [make_bag(tweet[1]) for tweet in corpus]\n",
    "    return bags\n",
    "\n",
    "def calculate_sets(corpus):\n",
    "    \"\"\"Set f√ºr alle Tweets in einem Korpus berechnen; als Liste der Sets speichern.\n",
    "    \"\"\"\n",
    "    sets = [set(tweet[1]) for tweet in corpus]\n",
    "    return sets\n",
    "\n",
    "def collect_duplicates(corpus, sets, simfunc, cutoff):\n",
    "    \"\"\" Duplikate/sehr √§hnliche Tweetpaare sammeln\n",
    "    Input: Korpus, Liste von (Multi)Sets f√ºr alle Tweets im Korpus\n",
    "    Output: Liste potentieller Duplikate (als Tupel der Korpuseintr√§ge)\n",
    "    \"\"\"\n",
    "    duplicates = []\n",
    "    for i in range(len(corpus)):\n",
    "        if i % 100 == 0: print(\"i\",i)\n",
    "        for j in range(len(corpus)):\n",
    "            if i == j: continue\n",
    "            else:\n",
    "                jacc = simfunc(sets[i],sets[j])\n",
    "                if jacc >= cutoff: # z.B. 0.47 f√ºr multisets\n",
    "                    #print(jacc)\n",
    "                    duplicates.append((corpus[i],corpus[j]))\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "def rem_duplicates(duplicates):\n",
    "    \"\"\"Sortiert die Liste der Duplikate, so dass Paare von IDs nur noch einmal vorkommen.\n",
    "    (Sie werden zun√§chst doppelt gefunden, jeweils einmal in jede Vergleichsrichtung)\n",
    "    \"\"\"\n",
    "    dups = []\n",
    "    dup_ids = []\n",
    "    for i in duplicates:\n",
    "        if not {i[0][0], i[1][0]} in dup_ids:\n",
    "            dup_ids.append({i[0][0], i[1][0]})\n",
    "            dups.append(i)\n",
    "    return dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train.txt\", mode=\"r\", encoding=\"utf-8\") as in_train:\n",
    "    train = in_train.readlines()\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test.txt\", mode=\"r\", encoding=\"utf-8\") as in_test:\n",
    "    test = in_test.readlines()\n",
    "\n",
    "total = train + test\n",
    "total = [entry.strip().split(\"\\t\") for entry in total]\n",
    "\n",
    "\n",
    "# @user-Erw√§hnungen anonymisieren\n",
    "total_anonym = [(entry[0],anonym_atuser(entry[1]),entry[2],entry[3]) for entry in total]\n",
    "\n",
    "# Sets aller Tweets berechnen\n",
    "tweet_sets = calculate_sets(total_anonym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "i 100\n",
      "i 200\n",
      "i 300\n",
      "i 400\n",
      "i 500\n",
      "i 600\n",
      "i 700\n",
      "i 800\n",
      "i 900\n",
      "i 1000\n",
      "i 1100\n",
      "i 1200\n",
      "i 1300\n",
      "i 1400\n",
      "i 1500\n",
      "i 1600\n",
      "i 1700\n",
      "i 1800\n",
      "i 1900\n",
      "i 2000\n",
      "i 2100\n",
      "i 2200\n",
      "i 2300\n",
      "i 2400\n",
      "i 2500\n",
      "i 2600\n",
      "i 2700\n",
      "i 2800\n",
      "i 2900\n",
      "i 3000\n",
      "i 3100\n",
      "i 3200\n",
      "i 3300\n",
      "i 3400\n",
      "i 3500\n",
      "i 3600\n",
      "i 3700\n",
      "i 3800\n",
      "i 3900\n",
      "i 4000\n",
      "i 4100\n",
      "i 4200\n",
      "i 4300\n",
      "i 4400\n",
      "i 4500\n",
      "i 4600\n",
      "i 4700\n",
      "i 4800\n",
      "i 4900\n",
      "i 5000\n",
      "i 5100\n",
      "i 5200\n",
      "i 5300\n",
      "i 5400\n",
      "i 5500\n",
      "i 5600\n",
      "i 5700\n",
      "i 5800\n",
      "i 5900\n",
      "i 6000\n",
      "i 6100\n",
      "i 6200\n",
      "i 6300\n",
      "i 6400\n",
      "i 6500\n",
      "i 6600\n",
      "i 6700\n",
      "i 6800\n",
      "i 6900\n",
      "i 7000\n",
      "i 7100\n",
      "i 7200\n",
      "i 7300\n",
      "i 7400\n",
      "i 7500\n",
      "i 7600\n",
      "i 7700\n",
      "i 7800\n",
      "i 7900\n",
      "i 8000\n",
      "i 8100\n",
      "i 8200\n",
      "i 8300\n",
      "i 8400\n",
      "i 8500\n",
      "i 8600\n",
      "i 8700\n",
      "i 8800\n",
      "i 8900\n",
      "i 9000\n",
      "i 9100\n",
      "i 9200\n",
      "i 9300\n",
      "i 9400\n",
      "i 9500\n",
      "i 9600\n",
      "i 9700\n",
      "i 9800\n",
      "i 9900\n",
      "i 10000\n",
      "i 10100\n",
      "i 10200\n",
      "i 10300\n",
      "i 10400\n",
      "i 10500\n",
      "i 10600\n",
      "i 10700\n",
      "i 10800\n",
      "i 10900\n",
      "i 11000\n",
      "i 11100\n",
      "i 11200\n",
      "i 11300\n",
      "i 11400\n",
      "i 11500\n",
      "i 11600\n",
      "i 11700\n",
      "i 11800\n",
      "i 11900\n",
      "i 12000\n",
      "i 12100\n",
      "i 12200\n",
      "i 12300\n",
      "i 12400\n",
      "i 12500\n",
      "i 12600\n",
      "i 12700\n",
      "i 12800\n",
      "i 12900\n",
      "i 13000\n",
      "i 13100\n",
      "i 13200\n",
      "i 13300\n",
      "i 13400\n",
      "i 13500\n",
      "i 13600\n",
      "i 13700\n",
      "i 13800\n",
      "i 13900\n",
      "i 14000\n",
      "i 14100\n",
      "i 14200\n",
      "i 14300\n",
      "i 14400\n",
      "i 14500\n",
      "i 14600\n",
      "i 14700\n",
      "i 14800\n",
      "i 14900\n",
      "i 15000\n",
      "i 15100\n",
      "i 15200\n",
      "i 15300\n",
      "i 15400\n",
      "i 15500\n",
      "i 15600\n",
      "i 15700\n",
      "i 15800\n",
      "i 15900\n",
      "i 16000\n",
      "i 16100\n",
      "i 16200\n",
      "i 16300\n",
      "i 16400\n",
      "i 16500\n",
      "i 16600\n",
      "i 16700\n",
      "i 16800\n",
      "i 16900\n",
      "i 17000\n",
      "i 17100\n",
      "i 17200\n",
      "i 17300\n",
      "i 17400\n",
      "i 17500\n",
      "i 17600\n",
      "i 17700\n",
      "i 17800\n",
      "i 17900\n",
      "i 18000\n",
      "i 18100\n",
      "i 18200\n",
      "i 18300\n",
      "i 18400\n",
      "i 18500\n",
      "i 18600\n",
      "i 18700\n",
      "i 18800\n",
      "i 18900\n",
      "i 19000\n",
      "i 19100\n",
      "i 19200\n",
      "i 19300\n",
      "i 19400\n",
      "i 19500\n",
      "i 19600\n",
      "i 19700\n",
      "i 19800\n",
      "i 19900\n",
      "i 20000\n",
      "i 20100\n",
      "i 20200\n",
      "i 20300\n",
      "i 20400\n",
      "i 20500\n",
      "i 20600\n",
      "i 20700\n",
      "i 20800\n",
      "i 20900\n",
      "i 21000\n",
      "i 21100\n",
      "i 21200\n",
      "i 21300\n",
      "i 21400\n",
      "i 21500\n",
      "i 21600\n",
      "i 21700\n",
      "i 21800\n",
      "i 21900\n",
      "i 22000\n",
      "i 22100\n",
      "i 22200\n",
      "i 22300\n",
      "i 22400\n",
      "i 22500\n",
      "i 22600\n",
      "i 22700\n",
      "i 22800\n",
      "i 22900\n",
      "i 23000\n",
      "i 23100\n",
      "i 23200\n",
      "i 23300\n",
      "i 23400\n",
      "i 23500\n",
      "i 23600\n",
      "i 23700\n",
      "i 23800\n",
      "i 23900\n",
      "i 24000\n",
      "i 24100\n",
      "i 24200\n",
      "i 24300\n",
      "i 24400\n",
      "i 24500\n",
      "i 24600\n",
      "i 24700\n",
      "i 24800\n",
      "i 24900\n",
      "i 25000\n",
      "i 25100\n",
      "i 25200\n",
      "i 25300\n",
      "i 25400\n",
      "i 25500\n",
      "i 25600\n",
      "i 25700\n",
      "i 25800\n",
      "i 25900\n",
      "i 26000\n",
      "i 26100\n",
      "i 26200\n",
      "i 26300\n",
      "i 26400\n",
      "i 26500\n",
      "i 26600\n",
      "i 26700\n",
      "i 26800\n",
      "i 26900\n",
      "i 27000\n",
      "i 27100\n",
      "i 27200\n",
      "i 27300\n",
      "i 27400\n",
      "i 27500\n",
      "i 27600\n",
      "i 27700\n",
      "i 27800\n",
      "i 27900\n",
      "i 28000\n",
      "i 28100\n",
      "i 28200\n",
      "i 28300\n",
      "i 28400\n",
      "i 28500\n"
     ]
    }
   ],
   "source": [
    "# Jaccard-Index berechnen\n",
    "dups = collect_duplicates(total_anonym,tweet_sets,jaccard,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bags = calculate_bags(total_anonym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "i 100\n",
      "i 200\n",
      "i 300\n",
      "i 400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-cb67d01ba07e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdups_multisets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollect_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_anonym\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjaccard_multisets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.47\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-7d6f43d784a2>\u001b[0m in \u001b[0;36mcollect_duplicates\u001b[1;34m(corpus, sets, simfunc, cutoff)\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[0mjacc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mjacc\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# z.B. 0.47 f√ºr multisets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                     \u001b[1;31m#print(jacc)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-760cd9e4eee6>\u001b[0m in \u001b[0;36mjaccard_multisets\u001b[1;34m(bag1, bag2)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mgram\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtok1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgram\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtok2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbag1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgram\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbag2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgram\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[0mschnitt_len\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dups_multisets = collect_duplicates(total_anonym, bags, jaccard_multisets, 0.47)\n",
    "# Berechnung: ca. 45 Sekunden/200 Tweets --> insg. ca 150 Minuten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28598\n",
      "(('01110813', '@user Das ist Erpressung am deutschen Volk, keiner will das, tun Sie endlich was ‚ÄºÔ∏è‚ÄºÔ∏è‚ÄºÔ∏è', 'NOT', 'NOT'), ('02220549', '@user Das ewige Spiel, der Versuch der personifizierung des kapitalistischen normalzustands!', 'NOT', 'NOT'))\n",
      "67665.0\n"
     ]
    }
   ],
   "source": [
    "print(len(total_anonym))\n",
    "print(dups[1])\n",
    "print(len(dups)/2)\n",
    "#dups_nodups = rem_duplicates(dups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplikat-Cluster finden\n",
    "\n",
    "# Duplikat-ID-Paare speichern\n",
    "\n",
    "# Duplikate entfernen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Annotationen formatieren und speichern\n",
    "\n",
    "- K√ºrzel mit Legende f√ºr die Labels erstellen\n",
    "\n",
    "- in unterschiedlicher Detailgenauigkeit speichern:\n",
    "    - VVH Ja/Nein, VVH-Allg Ja/Nein, VVH-NS Ja/Nein\n",
    "    - Gruppe Ja/Nein, Welches Gruppenmerkmal/Keine Gruppe, Welche Gruppe genau/Keine Gruppe \n",
    "    - Handlung VVH-Allg Ja/Nein, Welche Handlung genau/Keine Handlung\n",
    "    - Handlung VVH-NS Ja/Nein, Welche Handlung genau/Keine Handlung\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train_HATE_anno_korrigiert.json\", mode=\"r\", encoding=\"utf-8\") as in_hate_train:\n",
    "    hate_train = in_hate_train.read()\n",
    "    hate_train_anno = json.loads(hate_train)\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test_HATE_anno_korrigiert.json\", mode=\"r\", encoding=\"utf-8\") as in_hate_test:\n",
    "    hate_test = in_hate_test.read()\n",
    "    hate_test_anno = json.loads(hate_test)\n",
    "\n",
    "# Beispiel-Eintrag: {\"id\": \"01222380\", \"data\": \"den #Zentralrat der #Muslime sollte man komplett rausschmei\\u00dfen, er hat hier nichts zu suchen @aktuelleStunde #WDR\", \"label\": [\"KEINE\"], \"tag1\": \"NEG\", \"tag2\": \"HATE\"}\n",
    "\n",
    "# Verallgemeinerte Annotation\n",
    "\n",
    "# 1. VVH Ja/Nein:\n",
    "def transform_anno_vvh(corp):\n",
    "    new_anno = []\n",
    "    for entry in corp:\n",
    "        if \"KEINE\" in entry[\"label\"]:\n",
    "            entry[\"label\"] = \"KEINE\"\n",
    "        else:\n",
    "            entry[\"label\"] = \"VVH\"\n",
    "        new_anno.append(entry)\n",
    "    return new_anno\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Train_HATE_VVH.txt\", mode=\"w\", encoding=\"utf-8\") as out_vvh_train:\n",
    "    for i in transform_anno_vvh(hate_train_anno):\n",
    "        out_vvh_train.write(\"\\t\".join((i[\"id\"], i[\"data\"], i[\"label\"]))+\"\\n\")\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Test_HATE_VVH.txt\", mode=\"w\", encoding=\"utf-8\") as out_vvh_test:\n",
    "    for i in transform_anno_vvh(hate_test_anno):\n",
    "        out_vvh_test.write(\"\\t\".join((i[\"id\"], i[\"data\"], i[\"label\"]))+\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# 2.1   Gruppe Ja/Nein\n",
    "def transform_anno_gruppe(corp):\n",
    "    gruppe = {\"Nationalit√§t\", 'ethnische Herkunft / \"Rasse\"', \"Religion / Weltanschauung\",\n",
    "                    \"Politische Einstellung\", \"Geschlecht\", \"Anderes Merkmal\"}\n",
    "    new_anno = []\n",
    "    for entry in corp:\n",
    "        if gruppe & entry[\"label\"] != set():\n",
    "            entry[\"label\"] = \"Gruppe\"\n",
    "        else:\n",
    "            entry[\"label\"] = \"KeineGruppe\"\n",
    "        new_anno.append(entry)\n",
    "    return new_anno\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Train_HATE_Gruppe.txt\", mode=\"w\", encoding=\"utf-8\") as out_gr_train:\n",
    "    for i in transform_anno_gruppe(hate_train_anno):\n",
    "        out_gr_train.write(\"\\t\".join((i[\"id\"], i[\"data\"], i[\"label\"]))+\"\\n\")\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Test_HATE_Gruppe.txt\", mode=\"w\", encoding=\"utf-8\") as out_gr_test:\n",
    "    for i in transform_anno_gruppe(hate_test_anno):\n",
    "        out_gr_test.write(\"\\t\".join((i[\"id\"], i[\"data\"], i[\"label\"]))+\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# 2.2   Welche Gruppe / Keine Gruppe\n",
    "def transform_anno_gruppe_det(corp):\n",
    "    gruppe = {\"Nationalit√§t\", 'ethnische Herkunft / \"Rasse\"', \"Religion / Weltanschauung\",\n",
    "                    \"Politische Einstellung\", \"Geschlecht\", \"Anderes Merkmal\"}\n",
    "    new_anno = []\n",
    "    for entry in corp:\n",
    "        if gruppe & entry[\"label\"] != set():\n",
    "            entry[\"label\"] = gruppe & entry[\"label\"]\n",
    "        else:\n",
    "            entry[\"label\"] = \"KeineGruppe\"\n",
    "        new_anno.append(entry)\n",
    "    return new_anno\n",
    "\n",
    "#with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Train_HATE_GruppeDetail.txt\", mode=\"w\", encoding=\"utf-8\") as out_grd_train:\n",
    "#    for i in transform_anno_gruppe_det(hate_train_anno):\n",
    "#        out_grd_train.write()\n",
    "\n",
    "#with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Test_HATE_GruppeDetail.txt\", mode=\"w\", encoding=\"utf-8\") as out_grd_test:\n",
    "#    for i in transform_anno_gruppe_det(hate_test_anno):\n",
    "#        out_grd_test.write()\n",
    "\n",
    "\n",
    "# 3.1   Handlung VVH-Allg Ja/Nein\n",
    "def transform_anno_handlung(corp):\n",
    "    handlung = {\"Aufstachelung zu Hass\", \"Aufforderung zu Gewalt- oder Willk√ºrma√ünahmen\", \"Angriff der Menschenw√ºrde\"}\n",
    "    new_anno = []\n",
    "    for entry in corp:\n",
    "        if handlung & entry[\"label\"] != set():\n",
    "            entry[\"label\"] = \"HandlungVVH\"\n",
    "        else:\n",
    "            entry[\"label\"] = \"KeineHandlung\"\n",
    "        new_anno.append(entry)\n",
    "    return new_anno\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Train_HATE_Handlung.txt\", mode=\"w\", encoding=\"utf-8\") as out_handl_train:\n",
    "    for i in transform_anno_handlung(hate_train_anno):\n",
    "        out_handl_train.write(\"\\t\".join((i[\"id\"], i[\"data\"], i[\"label\"]))+\"\\n\")\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Test_HATE_Handlung.txt\", mode=\"w\", encoding=\"utf-8\") as out_handl_test:\n",
    "    for i in transform_anno_handlung(hate_test_anno):\n",
    "        out_handl_test.write(\"\\t\".join((i[\"id\"], i[\"data\"], i[\"label\"]))+\"\\n\")\n",
    "\n",
    "\n",
    "# 3.2   Welche Handlung VVH-Allg / Keine Handlung \n",
    "def transform_anno_handlung_det(corp):\n",
    "    handlung = {\"Aufstachelung zu Hass\", \"Aufforderung zu Gewalt- oder Willk√ºrma√ünahmen\", \"Angriff der Menschenw√ºrde\"}\n",
    "    new_anno = []\n",
    "    for entry in corp:\n",
    "        if handlung & entry[\"label\"] != set():\n",
    "            entry[\"label\"] = handlung & entry[\"label\"]\n",
    "        else:\n",
    "            entry[\"label\"] = \"KeineHandlung\"\n",
    "        new_anno.append(entry)\n",
    "    return new_anno\n",
    "\n",
    "#with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Train_HATE_HandlungDetail.txt\", mode=\"w\", encoding=\"utf-8\") as out_handld_train:\n",
    "#    for i in transform_anno_handlung_det(hate_train_anno):\n",
    "#        out_handld_train.write()\n",
    "\n",
    "#with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Test_HATE_HandlungDetail.txt\", mode=\"w\", encoding=\"utf-8\") as out_handld_test:\n",
    "#    for i in transform_anno_handlung_det(hate_test_anno):\n",
    "#        out_handld_test.write()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bfe21998ac49805528ba5c524b8c99ad95f34df1263c0d2d07976ea5fcbab9a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
