{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing des Referenzdatensatzes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Anonymisierung der @user-Erw√§hnungen zu \"@user\n",
    "2. Duplikate entfernen (bei Paaren sehr √§hnlicher Tweets einen entfernen)\n",
    "3. Mit den verschiedenen Annotationsebenen- und detaills separat speichern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Anonymisierung der @user-Erw√§hnungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def anonym_atuser(tweet):\n",
    "    tweet_anonym = re.sub('@[^@ ]+?@', '@user@', tweet)\n",
    "    tweet_anonym = re.sub('@[^@ ]+? ', '@user ', tweet_anonym)\n",
    "    tweet_anonym = re.sub('@[^@ ]+?$', '@user', tweet_anonym)\n",
    "    return tweet_anonym\n",
    "\n",
    "# von '@rspctfl@houelle_beck @ergroovt @ThomasMichael71 @ksemann2 @DrKassandraPari Dann h√§ttest Du nichts dagegen, wenn ein Lehrer ein Zeichen d PKK im Unterricht tr√§gt? Oder die Flagge Israels? Oder ein Anstecker der AfD? @Innenwelttramp'\n",
    "# zu '@user@user @user @user @user @user Dann h√§ttest Du nichts dagegen, wenn ein Lehrer ein Zeichen d PKK im Unterricht tr√§gt? Oder die Flagge Israels? Oder ein Anstecker der AfD? @user'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Duplikate entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard-Index-Funktionen zur Berechnung der √Ñhnlichkeit\n",
    "\n",
    "def make_bag(string1):\n",
    "    \"\"\" String als Multiset speichern; \n",
    "        In Form eines dicts: jedes vorkommende Unigram als Key, die Anzahl der Nutzungen als Value\n",
    "        Als zus√§tzlichen Wert auch die L√§nge des Strings speichern\n",
    "    \"\"\"\n",
    "    stringbag = dict()\n",
    "    stringset = set(string1)\n",
    "    for ch1 in stringset:\n",
    "        stringbag[ch1] = string1.count(ch1)\n",
    "    stringbag[\"len\"] = len(string1)\n",
    "    stringbag[\"tok\"] = set()\n",
    "    stringbag[\"tok\"] = set(stringbag.keys()) - {\"len\", \"tok\"}\n",
    "    return stringbag\n",
    "\n",
    "def jaccard(set1, set2):\n",
    "    \"\"\"Jaccard-Index von zwei Sets (von zwei Strings) berechnen.\n",
    "       Max.-Wert: 1 (Gleichheit)\n",
    "    \"\"\"\n",
    "    jaccard_index = len(set1 & set2) / len(set1 | set2)\n",
    "    return jaccard_index\n",
    "\n",
    "def jaccard_multisets(bag1, bag2):\n",
    "    \"\"\"Den Jaccard-Index f√ºr zwei Multisets (von zwei Strings) berechnen.\n",
    "       Max.-Wert: 0.5\n",
    "    \"\"\"\n",
    "    # Schnittmenge bauen, die Gesamtl√§nge speichern\n",
    "    schnitt_len = 0\n",
    "    schnitt = bag1[\"tok\"] & bag2[\"tok\"]\n",
    "    for gram in schnitt:\n",
    "        l = min(bag1[gram],bag2[gram])\n",
    "        schnitt_len += l\n",
    "\n",
    "    # L√§nge der Vereinigung ermitteln\n",
    "    vereinigung_len = bag1[\"len\"] + bag2[\"len\"]\n",
    "    \n",
    "    jaccard_index = schnitt_len / vereinigung_len\n",
    "    return jaccard_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6341463414634146\n"
     ]
    }
   ],
   "source": [
    "tweet1 = \"@Karl_Lauterbach Besser ein Amateur der lernf√§hig ist, als das verlauste Pack der abgefuckten SPD - SCHMAROTZER, P√ÑDOPHILE und DENUNZIANTEN !!!\"\n",
    "tweet2 = \"@ThomasOppermann SPD - SCHMAROTZER,P√ÑDOPHILE UND DENUNZIANTEN   oder   SCHEINHEILGSTE PARTEI DEUTSCHLANDS !!!\"\n",
    "tweet3 = \"\"\n",
    "tweet4 = \"@spdde @hubertus_heil SPD - SCHMAROTZER, P√ÑDOPHILE UND DENUNZIANTEN\"\n",
    "tweet5 = \"SPD - SCHMAROTZER, P√ÑDOPHILE UND DENUNZIANTEN\"\n",
    "tweet8 = \"@user - SCHMAROTZER, P√ÑDOPHILE UND DENUNZIANTEN\"\n",
    "tweet6 = \"Deutsche Medien, Halbwahrheiten und einseitige Betrachtung, wie bei allen vom Staat finanzierten 'billigen' Propagandainstitutionen üòú\"\n",
    "tweet7 = \"Bevor sich PL an Angies Krimigranten-Quoten beteiligen wird, verlassen sie auch die EU u. Konzerne k√∂nnten ihren Mist ins Baltikum fliegen üòú\"\n",
    "\n",
    "#jind = jaccard_multisets(make_bag(tweet6), make_bag(tweet7))\n",
    "#print(jind)\n",
    "\n",
    "jacc = jaccard(set(tweet6), set(tweet7))\n",
    "print(jacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bags(corpus):\n",
    "    \"\"\"Multisets (bags) f√ºr alle Tweets in einem Korpus berechen.\n",
    "    Input: Korpus\n",
    "    Output: Liste von Multisets (Index in der Liste == Index in der Korpusliste)\n",
    "    \"\"\"\n",
    "    bags = [make_bag(tweet[1]) for tweet in corpus]\n",
    "    return bags\n",
    "\n",
    "def calculate_sets(corpus):\n",
    "    \"\"\"Set f√ºr alle Tweets in einem Korpus berechnen; als Liste der Sets speichern.\n",
    "    \"\"\"\n",
    "    sets = [set(tweet[1]) for tweet in corpus]\n",
    "    return sets\n",
    "\n",
    "def collect_duplicates(corpus, sets, simfunc, cutoff):\n",
    "    \"\"\" Duplikate sammeln\n",
    "    Input: Korpus, Liste von (Multi)Sets f√ºr alle Tweets im Korpus, Vergleichsfunktion, √Ñhnlichkeitsgrenzwert\n",
    "    Output: Liste potentieller Duplikate (als Tupel der Korpuseintr√§ge)\n",
    "    \"\"\"\n",
    "    duplicates = []\n",
    "    for i in range(len(corpus)):\n",
    "        # if i % 100 == 0: print(\"i\",i) # Zur Zeitmessung\n",
    "        for j in range(len(corpus)):\n",
    "            # Kein Vergleich mit sich selbst oder bereits in die andere Richtung verglichener Paare\n",
    "            if i >= j: continue\n",
    "            else:\n",
    "                jacc = simfunc(sets[i],sets[j])\n",
    "                if jacc >= cutoff: # z.B. 0.47 f√ºr multisets\n",
    "                    duplicates.append((corpus[i],corpus[j]))\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train.txt\", mode=\"r\", encoding=\"utf-8\") as in_train:\n",
    "    train = in_train.readlines()\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test.txt\", mode=\"r\", encoding=\"utf-8\") as in_test:\n",
    "    test = in_test.readlines()\n",
    "\n",
    "total = train[1:] + test[1:] # ohne Erkl√§rungszeile\n",
    "total = [entry.strip().split(\"\\t\") for entry in total]\n",
    "\n",
    "\n",
    "# @user-Erw√§hnungen anonymisieren\n",
    "total_anonym = [(entry[0],anonym_atuser(entry[1]),entry[2],entry[3]) for entry in total]\n",
    "\n",
    "# Sets aller Tweets berechnen\n",
    "tweet_sets = calculate_sets(total_anonym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard-Index berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "bags = calculate_bags(total_anonym) # Multimenge f√ºr alle Tweets berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "i 100\n",
      "i 200\n",
      "i 300\n",
      "i 400\n",
      "i 500\n",
      "i 600\n",
      "i 700\n",
      "i 800\n",
      "i 900\n",
      "i 1000\n",
      "i 1100\n",
      "i 1200\n",
      "i 1300\n",
      "i 1400\n",
      "i 1500\n",
      "i 1600\n",
      "i 1700\n",
      "i 1800\n",
      "i 1900\n",
      "i 2000\n",
      "i 2100\n",
      "i 2200\n",
      "i 2300\n",
      "i 2400\n",
      "i 2500\n",
      "i 2600\n",
      "i 2700\n",
      "i 2800\n",
      "i 2900\n",
      "i 3000\n",
      "i 3100\n",
      "i 3200\n",
      "i 3300\n",
      "i 3400\n",
      "i 3500\n",
      "i 3600\n",
      "i 3700\n",
      "i 3800\n",
      "i 3900\n",
      "i 4000\n",
      "i 4100\n",
      "i 4200\n",
      "i 4300\n",
      "i 4400\n",
      "i 4500\n",
      "i 4600\n",
      "i 4700\n",
      "i 4800\n",
      "i 4900\n",
      "i 5000\n",
      "i 5100\n",
      "i 5200\n",
      "i 5300\n",
      "i 5400\n",
      "i 5500\n",
      "i 5600\n",
      "i 5700\n",
      "i 5800\n",
      "i 5900\n",
      "i 6000\n",
      "i 6100\n",
      "i 6200\n",
      "i 6300\n",
      "i 6400\n",
      "i 6500\n",
      "i 6600\n",
      "i 6700\n",
      "i 6800\n",
      "i 6900\n",
      "i 7000\n",
      "i 7100\n",
      "i 7200\n",
      "i 7300\n",
      "i 7400\n",
      "i 7500\n",
      "i 7600\n",
      "i 7700\n",
      "i 7800\n",
      "i 7900\n",
      "i 8000\n",
      "i 8100\n",
      "i 8200\n",
      "i 8300\n",
      "i 8400\n",
      "i 8500\n",
      "i 8600\n",
      "i 8700\n",
      "i 8800\n",
      "i 8900\n",
      "i 9000\n",
      "i 9100\n",
      "i 9200\n",
      "i 9300\n",
      "i 9400\n",
      "i 9500\n",
      "i 9600\n",
      "i 9700\n",
      "i 9800\n",
      "i 9900\n",
      "i 10000\n",
      "i 10100\n",
      "i 10200\n",
      "i 10300\n",
      "i 10400\n",
      "i 10500\n",
      "i 10600\n",
      "i 10700\n",
      "i 10800\n",
      "i 10900\n",
      "i 11000\n",
      "i 11100\n",
      "i 11200\n",
      "i 11300\n",
      "i 11400\n",
      "i 11500\n",
      "i 11600\n",
      "i 11700\n",
      "i 11800\n",
      "i 11900\n",
      "i 12000\n",
      "i 12100\n",
      "i 12200\n",
      "i 12300\n",
      "i 12400\n",
      "i 12500\n",
      "i 12600\n",
      "i 12700\n",
      "i 12800\n",
      "i 12900\n",
      "i 13000\n",
      "i 13100\n",
      "i 13200\n",
      "i 13300\n",
      "i 13400\n",
      "i 13500\n",
      "i 13600\n",
      "i 13700\n",
      "i 13800\n",
      "i 13900\n",
      "i 14000\n",
      "i 14100\n",
      "i 14200\n",
      "i 14300\n",
      "i 14400\n",
      "i 14500\n",
      "i 14600\n",
      "i 14700\n",
      "i 14800\n",
      "i 14900\n",
      "i 15000\n",
      "i 15100\n",
      "i 15200\n",
      "i 15300\n",
      "i 15400\n",
      "i 15500\n",
      "i 15600\n",
      "i 15700\n",
      "i 15800\n",
      "i 15900\n",
      "i 16000\n",
      "i 16100\n",
      "i 16200\n",
      "i 16300\n",
      "i 16400\n",
      "i 16500\n",
      "i 16600\n",
      "i 16700\n",
      "i 16800\n",
      "i 16900\n",
      "i 17000\n",
      "i 17100\n",
      "i 17200\n",
      "i 17300\n",
      "i 17400\n",
      "i 17500\n",
      "i 17600\n",
      "i 17700\n",
      "i 17800\n",
      "i 17900\n",
      "i 18000\n",
      "i 18100\n",
      "i 18200\n",
      "i 18300\n",
      "i 18400\n",
      "i 18500\n",
      "i 18600\n",
      "i 18700\n",
      "i 18800\n",
      "i 18900\n",
      "i 19000\n",
      "i 19100\n",
      "i 19200\n",
      "i 19300\n",
      "i 19400\n",
      "i 19500\n",
      "i 19600\n",
      "i 19700\n",
      "i 19800\n",
      "i 19900\n",
      "i 20000\n",
      "i 20100\n",
      "i 20200\n",
      "i 20300\n",
      "i 20400\n",
      "i 20500\n",
      "i 20600\n",
      "i 20700\n",
      "i 20800\n",
      "i 20900\n",
      "i 21000\n",
      "i 21100\n",
      "i 21200\n",
      "i 21300\n",
      "i 21400\n",
      "i 21500\n",
      "i 21600\n",
      "i 21700\n",
      "i 21800\n",
      "i 21900\n",
      "i 22000\n",
      "i 22100\n",
      "i 22200\n",
      "i 22300\n",
      "i 22400\n",
      "i 22500\n",
      "i 22600\n",
      "i 22700\n",
      "i 22800\n",
      "i 22900\n",
      "i 23000\n",
      "i 23100\n",
      "i 23200\n",
      "i 23300\n",
      "i 23400\n",
      "i 23500\n",
      "i 23600\n",
      "i 23700\n",
      "i 23800\n",
      "i 23900\n",
      "i 24000\n",
      "i 24100\n",
      "i 24200\n",
      "i 24300\n",
      "i 24400\n",
      "i 24500\n",
      "i 24600\n",
      "i 24700\n",
      "i 24800\n",
      "i 24900\n",
      "i 25000\n",
      "i 25100\n",
      "i 25200\n",
      "i 25300\n",
      "i 25400\n",
      "i 25500\n",
      "i 25600\n",
      "i 25700\n",
      "i 25800\n",
      "i 25900\n",
      "i 26000\n",
      "i 26100\n",
      "i 26200\n",
      "i 26300\n",
      "i 26400\n",
      "i 26500\n",
      "i 26600\n",
      "i 26700\n",
      "i 26800\n",
      "i 26900\n",
      "i 27000\n",
      "i 27100\n",
      "i 27200\n",
      "i 27300\n",
      "i 27400\n",
      "i 27500\n",
      "i 27600\n",
      "i 27700\n",
      "i 27800\n",
      "i 27900\n",
      "i 28000\n",
      "i 28100\n",
      "i 28200\n",
      "i 28300\n",
      "i 28400\n",
      "i 28500\n"
     ]
    }
   ],
   "source": [
    "# Jaccard-Index berechnen\n",
    "dups_multisets = collect_duplicates(total_anonym, bags, jaccard_multisets, 0.47)\n",
    "# Berechnung (anfangs): ca. 30 Sekunden/200 Tweets --> insg. ca 70 Minuten\n",
    "# Aber: Wird mit der Zeit schneller (da nach und nach bereits alle Vergleiche in eine Richtung bereits geschehen) --> insg.: 42m 9.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "#dups_json = json.dumps(dups_multisets)\n",
    "#with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\Duplikat_IDs.json\", mode=\"w\", encoding=\"utf-8\") as out_dups:\n",
    "#    out_dups.write(dups_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplikat-Cluster bestimmen, um Kettenvergleiche zu vermeiden\n",
    "# --> falls ID1 √§hnlich ID2: zusammen in ein Set, und jede weitere ID, die einer der beiden IDs √§hnlich ist dazu\n",
    "\n",
    "cluster = []\n",
    "ID_cluster_ref = dict()\n",
    "for duplicate_tup in dups_multisets:\n",
    "    tweet1, tweet2 = duplicate_tup\n",
    "    ID1, ID2 = tweet1[0], tweet2[0]\n",
    "    # 1. weder ID1 noch ID2 vorhanden\n",
    "    if (ID1 not in ID_cluster_ref) and (ID2 not in ID_cluster_ref):\n",
    "        cluster_num = len(cluster)\n",
    "        ID_cluster_ref[ID1], ID_cluster_ref[ID2] = cluster_num, cluster_num\n",
    "        cluster.append({tweet1, tweet2})\n",
    "    # 2. eine von beiden vorhanden --> beide IDs ins vorhandene Cluster integrieren\n",
    "    elif ID1 in ID_cluster_ref:\n",
    "        cluster_num = ID_cluster_ref[ID1]\n",
    "        ID_cluster_ref[ID2] = cluster_num\n",
    "        cluster[cluster_num].add(tweet2)\n",
    "    elif ID2 in ID_cluster_ref:\n",
    "        cluster_num = ID_cluster_ref[ID2]\n",
    "        ID_cluster_ref[ID1] = cluster_num\n",
    "        cluster[cluster_num].add(tweet1)\n",
    "    # 3. Beide bereits vorhanden\n",
    "    else:\n",
    "        assert ID_cluster_ref[ID1] == ID_cluster_ref[ID2]\n",
    "\n",
    "# schwarze Liste zu l√∂schender IDs\n",
    "schwarzeListe = []\n",
    "for gruppe in cluster[2:]:\n",
    "    grp = list(gruppe)\n",
    "    for tweet in grp[1:]:\n",
    "        schwarzeListe.append(tweet[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321\n",
      "100\n",
      "{('corpus_id', 'tweet', 'binarylabel', 'finelabel')}\n",
      "223\n"
     ]
    }
   ],
   "source": [
    "print(len(ID_cluster_ref)) # 321 Tweets, die einander in irgendeiner Konstellation √§hnlich sind\n",
    "print(len(cluster)) # 100 Cluster\n",
    "print(cluster[0]) \n",
    "print(len(schwarzeListe)) # 223 zu l√∂schende Tweets\n",
    "\n",
    "# Cluster 0: vergessene erste Zeile (Erkl√§rung) --> TODO nochmal laufen lassen, json neu speichern\n",
    "# Cluster 1: √Ñhnlichkeit durch wiederholte @user Erw√§hnungen, Text selbst nicht √§hnlich\n",
    "# tats√§chlich √§hnlich, nur einen der Tweets behalten (Zufall): Clusters 2 - 8 \n",
    "# Cluster 4: 04110200 und zwei weitere HASOC2020 Tweets: \"https:/‚Ä¶\" als Rest -> mit generischem Link ersetzten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ohne die Duplikate in neue Dateien schreiben (nicht mehr anonymisiert)\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train_OD.txt\", mode=\"w\", encoding=\"utf-8\") as out_train:\n",
    "    for tweet in train:\n",
    "        if tweet[0] not in schwarzeListe:\n",
    "            out_train.write(tweet)\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test_OD.txt\", mode=\"w\", encoding=\"utf-8\") as out_test:\n",
    "    for tweet in test:\n",
    "        if tweet[0] not in schwarzeListe:\n",
    "            out_test.write(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Annotationen formatieren und speichern\n",
    "\n",
    "- in unterschiedlicher Detailgenauigkeit speichern:\n",
    "    - VVH Ja/Nein, VVH-Allg Ja/Nein, VVH-NS Ja/Nein\n",
    "    - Gruppe Ja/Nein, Welches Gruppenmerkmal/Keine Gruppe, Welche Gruppe genau/Keine Gruppe \n",
    "    - Handlung VVH-Allg Ja/Nein, Welche Handlung genau/Keine Handlung\n",
    "    - Handlung VVH-NS Ja/Nein, Welche Handlung genau/Keine Handlung\n",
    "\n",
    "- gleichzeitig @user-Erw√§hnungen anonymisieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train_HATE_anno_korrigiert.json\", mode=\"r\", encoding=\"utf-8\") as in_hate_train:\n",
    "    hate_train = in_hate_train.read()\n",
    "    hate_train_anno = json.loads(hate_train)\n",
    "    for entry in hate_train_anno:\n",
    "        entry[\"data\"] = anonym_atuser(entry[\"data\"])\n",
    "\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test_HATE_anno_korrigiert.json\", mode=\"r\", encoding=\"utf-8\") as in_hate_test:\n",
    "    hate_test = in_hate_test.read()\n",
    "    hate_test_anno = json.loads(hate_test)\n",
    "    for entry in hate_test_anno:\n",
    "        entry[\"data\"] = anonym_atuser(entry[\"data\"])\n",
    "\n",
    "# Beispiel-Eintrag: {\"id\": \"01222380\", \"data\": \"den #Zentralrat der #Muslime sollte man komplett rausschmei\\u00dfen, er hat hier nichts zu suchen @aktuelleStunde #WDR\", \"label\": [\"KEINE\"], \"tag1\": \"NEG\", \"tag2\": \"HATE\"}\n",
    "\n",
    "# Verallgemeinerte Annotation\n",
    "\n",
    "# 1. VVH Ja/Nein:\n",
    "def transform_anno_vvh(corp):\n",
    "    new_anno = []\n",
    "    for entry in corp:\n",
    "        if \"KEINE\" in entry[\"label\"]:\n",
    "            entry[\"label\"] = \"KEINE\"\n",
    "        else:\n",
    "            entry[\"label\"] = \"VVH\"\n",
    "        new_anno.append(entry)\n",
    "    return new_anno\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Train_HATE_VVH.txt\", mode=\"w\", encoding=\"utf-8\") as out_vvh_train:\n",
    "    for i in transform_anno_vvh(hate_train_anno):\n",
    "        if i[\"id\"] not in schwarzeListe:\n",
    "            out_vvh_train.write(\"\\t\".join((i[\"id\"], i[\"data\"], i[\"label\"]))+\"\\n\")\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Test_HATE_VVH.txt\", mode=\"w\", encoding=\"utf-8\") as out_vvh_test:\n",
    "    for i in transform_anno_vvh(hate_test_anno):\n",
    "        if i[\"id\"] not in schwarzeListe:\n",
    "            out_vvh_test.write(\"\\t\".join((i[\"id\"], i[\"data\"], i[\"label\"]))+\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# 2.1   Gruppe Ja/Nein\n",
    "def transform_anno_gruppe(corp):\n",
    "    gruppe = {\"Nationalit√§t\", 'ethnische Herkunft / \"Rasse\"', \"Religion / Weltanschauung\",\n",
    "                    \"Politische Einstellung\", \"Geschlecht\", \"Anderes Merkmal\"}\n",
    "    new_anno = []\n",
    "    for entry in corp:\n",
    "        if gruppe & set(entry[\"label\"]) != set():\n",
    "            entry[\"label\"] = \"Gruppe\"\n",
    "        else:\n",
    "            entry[\"label\"] = \"KeineGruppe\"\n",
    "        new_anno.append(entry)\n",
    "    return new_anno\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Train_HATE_Gruppe.txt\", mode=\"w\", encoding=\"utf-8\") as out_gr_train:\n",
    "    for i in transform_anno_gruppe(hate_train_anno):\n",
    "        if i[\"id\"] not in schwarzeListe:\n",
    "            out_gr_train.write(\"\\t\".join((i[\"id\"], i[\"data\"], i[\"label\"]))+\"\\n\")\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Test_HATE_Gruppe.txt\", mode=\"w\", encoding=\"utf-8\") as out_gr_test:\n",
    "    for i in transform_anno_gruppe(hate_test_anno):\n",
    "        if i[\"id\"] not in schwarzeListe:\n",
    "            out_gr_test.write(\"\\t\".join((i[\"id\"], i[\"data\"], i[\"label\"]))+\"\\n\")\n",
    "\n",
    "\n",
    "# 2.2   Welche Gruppe / Keine Gruppe\n",
    "def transform_anno_gruppe_det(corp):\n",
    "    gruppe = {\"Nationalit√§t\", 'ethnische Herkunft / \"Rasse\"', \"Religion / Weltanschauung\",\n",
    "                    \"Politische Einstellung\", \"Geschlecht\", \"Anderes Merkmal\"}\n",
    "    new_anno = []\n",
    "    for entry in corp:\n",
    "        if gruppe & set(entry[\"label\"]) != set():\n",
    "            entry[\"label\"] = gruppe & set(entry[\"label\"])\n",
    "        else:\n",
    "            entry[\"label\"] = \"KeineGruppe\"\n",
    "        new_anno.append(entry)\n",
    "    return new_anno\n",
    "\n",
    "#with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Train_HATE_GruppeDetail.txt\", mode=\"w\", encoding=\"utf-8\") as out_grd_train:\n",
    "#    for i in transform_anno_gruppe_det(hate_train_anno):\n",
    "#        out_grd_train.write()\n",
    "\n",
    "#with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Test_HATE_GruppeDetail.txt\", mode=\"w\", encoding=\"utf-8\") as out_grd_test:\n",
    "#    for i in transform_anno_gruppe_det(hate_test_anno):\n",
    "#        out_grd_test.write()\n",
    "\n",
    "\n",
    "# 3.1   Handlung VVH-Allg Ja/Nein\n",
    "def transform_anno_handlung(corp):\n",
    "    handlung = {\"Aufstachelung zu Hass\", \"Aufforderung zu Gewalt- oder Willk√ºrma√ünahmen\", \"Angriff der Menschenw√ºrde\"}\n",
    "    new_anno = []\n",
    "    for entry in corp:\n",
    "        if handlung & set(entry[\"label\"]) != set():\n",
    "            entry[\"label\"] = \"HandlungVVH\"\n",
    "        else:\n",
    "            entry[\"label\"] = \"KeineHandlung\"\n",
    "        new_anno.append(entry)\n",
    "    return new_anno\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Train_HATE_Handlung.txt\", mode=\"w\", encoding=\"utf-8\") as out_handl_train:\n",
    "    for i in transform_anno_handlung(hate_train_anno):\n",
    "        if i[\"id\"] not in schwarzeListe:\n",
    "            out_handl_train.write(\"\\t\".join((i[\"id\"], i[\"data\"], i[\"label\"]))+\"\\n\")\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Test_HATE_Handlung.txt\", mode=\"w\", encoding=\"utf-8\") as out_handl_test:\n",
    "    for i in transform_anno_handlung(hate_test_anno):\n",
    "        if i[\"id\"] not in schwarzeListe:\n",
    "            out_handl_test.write(\"\\t\".join((i[\"id\"], i[\"data\"], i[\"label\"]))+\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# 3.2   Welche Handlung VVH-Allg / Keine Handlung \n",
    "def transform_anno_handlung_det(corp):\n",
    "    handlung = {\"Aufstachelung zu Hass\", \"Aufforderung zu Gewalt- oder Willk√ºrma√ünahmen\", \"Angriff der Menschenw√ºrde\"}\n",
    "    new_anno = []\n",
    "    for entry in corp:\n",
    "        if handlung & entry[\"label\"] != set():\n",
    "            entry[\"label\"] = handlung & set(entry[\"label\"])\n",
    "        else:\n",
    "            entry[\"label\"] = \"KeineHandlung\"\n",
    "        new_anno.append(entry)\n",
    "    return new_anno\n",
    "\n",
    "#with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Train_HATE_HandlungDetail.txt\", mode=\"w\", encoding=\"utf-8\") as out_handld_train:\n",
    "#    for i in transform_anno_handlung_det(hate_train_anno):\n",
    "#        out_handld_train.write()\n",
    "\n",
    "#with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Test_HATE_HandlungDetail.txt\", mode=\"w\", encoding=\"utf-8\") as out_handld_test:\n",
    "#    for i in transform_anno_handlung_det(hate_test_anno):\n",
    "#        out_handld_test.write()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtPfade = [\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Train_HATE_VVH.txt\",\n",
    "            \"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Test_HATE_VVH.txt\",\n",
    "            \"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Train_HATE_Gruppe.txt\",\n",
    "            \"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Test_HATE_Gruppe.txt\",\n",
    "            \"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Train_HATE_Handlung.txt\",\n",
    "            \"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_Test_HATE_Handlung.txt\",\n",
    "            ]\n",
    "\n",
    "import json\n",
    "\n",
    "for datei in txtPfade:\n",
    "    with open(datei, mode=\"r\", encoding=\"utf8\") as in_txt:\n",
    "        inhalt = in_txt.readlines()\n",
    "        sep_inhalt = [eintrag.strip().split(\"\\t\") for eintrag in inhalt]\n",
    "        json_inhalt = json.dumps(sep_inhalt)\n",
    "        with open(datei.strip(\"txt\")+\"json\", mode=\"w\", encoding=\"utf-8\") as out_json:\n",
    "            out_json.write(json_inhalt)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bfe21998ac49805528ba5c524b8c99ad95f34df1263c0d2d07976ea5fcbab9a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
