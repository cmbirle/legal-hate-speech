{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation der Volksverhetzung im Referenzdatensatz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Als \"HATE\" annotierte Einträge aus dem Referenzdatensatz herausfiltern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anmerkung:\n",
    "\n",
    "Die Notwendigkeit, Duplikate zu entfernen, ist erst im Laufe der Annotation klar geworden.  \n",
    "Daher enthielten die hier eingelesenen Daten und die aus der Annotation resultierenden Json-Dateien noch Duplikate,  \n",
    "die dann nach der Annotation entfernt wurden.\n",
    "\n",
    "Außerdem war anfangs die Überlegung den Split der Train- und Testdaten so zu belassen wie durch die Datensätze vorgegeben  \n",
    "und die Daten wurden dementsprechend nacheinander annotiert;  \n",
    "um später einen stratifizierten Split zu ermöglichen, wird aber ab dem Einlesen der neuen Annotation mit dem gesamten Datensatz HATE   gearbeitet;  \n",
    " der Train/Test-Split erfolgt dann in 'src/classify.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bereits ausgeführt, nicht noch einmal durchführen\n",
    "import json\n",
    "\n",
    "# Einlesen und filtern\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train.txt\", mode=\"r\", encoding=\"utf-8\") as hatetrainin:\n",
    "    all_cont = hatetrainin.readlines()\n",
    "    all_cont = all_cont[1:]\n",
    "    sep_cont = [entry.strip().split(\"\\t\") for entry in all_cont]\n",
    "    hate_train = [tweet for tweet in sep_cont if tweet[3] == \"HATE\"]\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test.txt\", mode=\"r\", encoding=\"utf-8\") as hatetestin:\n",
    "    all_test = hatetestin.readlines()\n",
    "    all_test = all_test[1:]\n",
    "    sep_test = [entry.strip().split(\"\\t\") for entry in all_test]\n",
    "    hate_test = [tweet for tweet in sep_test if tweet[3] == \"HATE\"]\n",
    "\n",
    "# Als json-Dateien speichern\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train_HATE.json\", mode=\"w\", encoding=\"utf-8\") as jsonout:\n",
    "    cont_dicts = [{\"id\":elem[0], \"text\":elem[1], \"tag1\":elem[2], \"tag2\":elem[3]} for elem in hate_train]\n",
    "    prep_cont = json.dumps(cont_dicts)\n",
    "    jsonout.write(prep_cont)\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test_HATE.json\", mode=\"w\", encoding=\"utf-8\") as jsonout_test:\n",
    "    cont_dicts_test = [{\"id\":elem[0], \"text\":elem[1], \"tag1\":elem[2], \"tag2\":elem[3]} for elem in hate_test]\n",
    "    prep_cont_test = json.dumps(cont_dicts_test)\n",
    "    jsonout_test.write(prep_cont_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Annotation in doccano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prüfung der Annotationshierarchie auf Korrektheit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_anno(datensatz):\n",
    "    \"\"\"Annotationslogik prüfen und Problemfälle sammeln\"\"\"\n",
    "    probleme = []\n",
    "    for entry in datensatz:\n",
    "        if not anno_logik_check(entry[\"label\"]): probleme.append(entry)\n",
    "    return probleme\n",
    "\n",
    "\n",
    "def anno_logik_check(labelset):\n",
    "    \"\"\" Die Annotationslogik überprüfen:\n",
    "    Input: Liste aller Labels für einen Tweet\n",
    "    Output: True (falls alles korrekt) / False (falls irgendein Problem vorliegt)    \n",
    "    \"\"\"\n",
    "    labelset = set(labelset)\n",
    "    korrekt = True\n",
    "    if len(labelset) == 0:\n",
    "        korrekt = False\n",
    "    elif (len(labelset) == 1) and (\"KEINE\" not in labelset):\n",
    "        korrekt = False\n",
    "    else:\n",
    "        # 1. VVH-ALLG interne Logik\n",
    "        if \"VVH-ALLG\" in labelset:\n",
    "            # Mind. eine Art der Tathandlung ggb.\n",
    "            if {\"Aufstachelung zu Hass\", \"Aufforderung zu Gewalt- oder Willkürmaßnahmen\", \"Angriff der Menschenwürde\"} & labelset == set():\n",
    "                korrekt = False\n",
    "            # Mind. eine Gruppe genannt\n",
    "            if {\"Nationalität\", 'ethnische Herkunft / \"Rasse\"', \"Religion / Weltanschauung\",\n",
    "                    \"Politische Einstellung\", \"Geschlecht\", \"Anderes Merkmal\"} & labelset == set():\n",
    "                korrekt = False\n",
    "            # keine VVH-NS Labels\n",
    "            if {\"VVH-NS\", \"Billigen\", \"Verherrlichen\", \"Verharmlosen\", \"Leugnen\", \"Rechtfertigen\"} & labelset != set():\n",
    "                korrekt = False\n",
    "            if \"KEINE\" in labelset: korrekt = False\n",
    "\n",
    "        # 2. VVH-NS interne Logik\n",
    "        if \"VVH-NS\" in labelset:\n",
    "            # Mind. eine Art der Tathandlung ggb.\n",
    "            if {\"Billigen\", \"Verherrlichen\", \"Verharmlosen\", \"Leugnen\", \"Rechtfertigen\"} & labelset == set(): \n",
    "                korrekt = False\n",
    "            # keine VVH-ALLG Labels\n",
    "            if {\"VVH-ALLG\", \"Aufstachelung zu Hass\", \"Aufforderung zu Gewalt- oder Willkürmaßnahmen\", \"Angriff der Menschenwürde\"} & labelset != set():\n",
    "                korrekt = False \n",
    "            if \"KEINE\" in labelset: korrekt = False\n",
    "        \n",
    "        # 3. Gruppenmerkmale - Logik (in beide Richtungen)\n",
    "\n",
    "        nation = {\"Türkischstämmige Deutsche\", \"Marokkaner:innen\", \"In Deutschland lebende Ausländer:innen\",\n",
    "                    \"Asiat:innen\", \"Pol:innen\", \"Afrikaner:innen\", \"Syrer:innen\", \"Palästinenser:innen\"}\n",
    "        herkunft = {\"POC\", \"Araber:in\"}\n",
    "        religion = {\"Muslim:e/innen\", \"Juden/Jüdinnen\", \"Christ:innen\"}\n",
    "        polit = {\"Die Grünen\", \"die SPD\", \"die Linke\", \"CDU/CSU\", \"AfD\", \"Nazis\", \"Islamist:innen\", \"Kommunist:innen\",\n",
    "                    \"Zionist:innen\", \"NPD\", \"FDP\", \"FridaysForFuture\", \"Pegida\", \"Anarchist:in\"}\n",
    "        geschlecht = {\"Trans/NB-Personen\", \"Frauen\", \"Männer\"}\n",
    "        andere = {\"Flüchtlinge\", \"Asylbewerber:innen\", \"Sich illegal in Deutschland aufhaltende Personen\", \"Migrant:innen\", \"Vorbestrafte\",\n",
    "                    \"Veganer\", \"Senior:innen\", \"Lesben, Schwule, Bi\", \"Kinder\", \"Jugendliche\", \"Polizist:innen\", \"Obdachlose\",\n",
    "                    \"Richter:innen\", \"Analphabet:innen\", \"Soldat:innen\", \"Behinderte\"}\n",
    "\n",
    "        # Richtung 1: falls Gruppe vorhanden, korrektes Merkmal auch vorhanden\n",
    "        for i in nation & labelset:\n",
    "            if \"Nationalität\" not in labelset:\n",
    "                korrekt = False\n",
    "        for j in herkunft & labelset:\n",
    "            if 'ethnische Herkunft / \"Rasse\"' not in labelset:\n",
    "                korrekt = False\n",
    "        for k in religion & labelset:\n",
    "            if \"Religion / Weltanschauung\" not in labelset:\n",
    "                korrekt = False\n",
    "        for l in polit & labelset:\n",
    "            if \"Politische Einstellung\" not in labelset:\n",
    "                korrekt = False\n",
    "        for m in geschlecht & labelset:\n",
    "            if \"Geschlecht\" not in labelset:\n",
    "                korrekt = False\n",
    "        for n in andere & labelset:\n",
    "            if \"Anderes Merkmal\" not in labelset:\n",
    "                korrekt = False\n",
    "\n",
    "        # Richtung 2: falls Merkmal vorhanden, auch eine passende Gruppe vorhanden\n",
    "        if \"Nationalität\" in labelset:\n",
    "            if nation & labelset == set():\n",
    "                korrekt = False\n",
    "        if 'ethnische Herkunft / \"Rasse\"' in labelset:\n",
    "            if herkunft & labelset == set():\n",
    "                korrekt = False\n",
    "        if \"Religion / Weltanschauung\" in labelset:\n",
    "            if religion & labelset == set():\n",
    "                korrekt = False\n",
    "        if \"Politische Einstellung\" in labelset:\n",
    "            if polit & labelset == set():\n",
    "                korrekt = False\n",
    "        if \"Geschlecht\" in labelset:\n",
    "            if geschlecht & labelset == set():\n",
    "                korrekt = False\n",
    "        if \"Anderes Merkmal\" in labelset:\n",
    "            if andere & labelset == set():\n",
    "                korrekt = False\n",
    "\n",
    "    return korrekt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probleme in der Annotationslogik von Hand durchgehen; jeweils für die Trainingsdaten und die Testdaten\n",
    "\n",
    "# Beispieleintrag der Annotations-Json-Datei:\n",
    "# { \"id\": \"01112520\",\n",
    "#   \"data\": \"@SteiblBarbara @Thomas_S_Wagner @RitaKratzert Weitaus schlimmer. ...\",\n",
    "#   \"label\": [\"KEINE\"],\n",
    "#   \"tag1\": \"NEG\",\n",
    "#   \"tag2\": \"HATE\"}\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train_HATE_annotiert.json\", mode=\"r\", encoding=\"utf-8\") as trainf:\n",
    "    train_anno = trainf.read()\n",
    "    train_annotations = json.loads(train_anno)\n",
    "\n",
    "probleme = check_anno(train_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jeder fehlerhafte Eintrag wurde einzeln von Hand korrigiert\n",
    "# Hier: Liste der Korrekturen\n",
    "\n",
    "new01110262 = {'id': '01110262', 'label': ['KEINE', 'Flüchtlinge', 'Afrikaner:innen', 'Nationalität', 'Anderes Merkmal']}\n",
    "new03113327 = {'id': '03113327', 'label': ['Angriff der Menschenwürde', 'VVH-ALLG', 'POC', 'ethnische Herkunft / \"Rasse\"', 'Kinder', 'Die Grünen', 'Politische Einstellung', 'Anderes Merkmal'] }\n",
    "new01114326 = {'id': '01114326', 'label': ['Muslim:e/innen', 'Religion / Weltanschauung', 'Islamist:innen', 'Aufstachelung zu Hass', 'VVH-ALLG', 'Politische Einstellung']}\n",
    "new01111028 = {'id': '01111028', 'label': ['KEINE', 'Araber:in', 'ethnische Herkunft / \"Rasse\"']}\n",
    "new03113648 = {'id': '03113648', 'label': ['KEINE', 'Politische Einstellung', 'Nazis']}\n",
    "new01112254 = {'id': '01112254', 'label': ['KEINE', 'Pol:innen', 'In Deutschland lebende Ausländer:innen', 'Nationalität']}\n",
    "new02110710 = {'id': '02110710', 'label': ['KEINE', 'Araber:in', 'In Deutschland lebende Ausländer:innen', 'ethnische Herkunft / \"Rasse\"', 'POC', 'Afrikaner:innen', 'Nationalität']}\n",
    "new04111621 = {'id': '04111621', 'label': ['KEINE']}\n",
    "new03112728 = {'id': '03112728', 'label': ['KEINE']}\n",
    "new02113786 = {'id': '02113786', 'label': ['KEINE', 'Palästinenser:innen', 'Nationalität']}\n",
    "new01113048 = {'id': '01113048', 'label': ['Palästinenser:innen', 'Nationalität', 'VVH-ALLG', 'Aufstachelung zu Hass'], 'tag1': 'NEG', 'tag2': 'HATE'}\n",
    "new01110156 = {'id': '01110156', 'label': ['Palästinenser:innen', 'Nationalität', 'VVH-ALLG', 'Aufstachelung zu Hass'], 'tag1': 'NEG', 'tag2': 'HATE'}\n",
    "new01222281 = {'id': '01222281', 'label': ['Die Grünen', 'Türkischstämmige Deutsche','Nationalität', 'Politische Einstellung', 'KEINE']}\n",
    "new02220290 = {'id': '02220290', 'label': ['KEINE', 'die SPD', 'Politische Einstellung']}\n",
    "new01221146 = {'id': '01221146', 'label': ['KEINE', 'die SPD', 'Politische Einstellung']}\n",
    "new01220517 = {'id': '01220517', 'label': ['KEINE', 'In Deutschland lebende Ausländer:innen', 'Pol:innen', 'Migrant:innen', 'Nationalität', 'Anderes Merkmal']}\n",
    "new02221882 = {'id': '02221882', 'label': ['KEINE', 'Zionist:innen', 'Palästinenser:innen', 'Politische Einstellung', 'Nationalität']}\n",
    "new01223120 = {'id': '01223120', 'label': ['KEINE']}\n",
    "new01221025 = {'id': '01221025', 'label': ['Verharmlosen', 'VVH-NS']}\n",
    "new02222036 = {'id': '02222036', 'label': ['KEINE', 'Palästinenser:innen', 'Nationalität']}\n",
    "new01220250 = {'id': '01220250', 'label': ['KEINE', 'Palästinenser:innen', 'Nationalität']}\n",
    "new02221762 = {'id': '02221762', 'label': ['KEINE', 'AfD', 'Politische Einstellung', 'Islamist:innen', 'Araber:in', 'In Deutschland lebende Ausländer:innen', 'Nationalität', 'ethnische Herkunft / \"Rasse\"']}\n",
    "new01223162 = {'id': '01223162', 'label': ['KEINE', 'Islamist:innen', 'Politische Einstellung']}\n",
    "new02221146 = {'id': '02221146', 'label': ['Nazis', 'Aufforderung zu Gewalt- oder Willkürmaßnahmen', 'VVH-ALLG', 'Politische Einstellung']}\n",
    "new01220249 = {'id': '01220249', 'label': ['KEINE', 'In Deutschland lebende Ausländer:innen', 'Nationalität']}\n",
    "new02221991 = {'id': '02221991', 'label': ['KEINE', 'Palästinenser:innen', 'Zionist:innen', 'Politische Einstellung', 'Nationalität']}\n",
    "new02220270 = {'id': '02220270', 'label': ['KEINE', 'Araber:in', 'Afrikaner:innen', 'In Deutschland lebende Ausländer:innen', 'Nationalität', 'Religion / Weltanschauung', 'Muslim:e/innen', 'ethnische Herkunft / \"Rasse\"']}\n",
    "new02220311 = {'id': '02220311', 'label': ['KEINE', 'Araber:in', 'In Deutschland lebende Ausländer:innen', 'Afrikaner:innen', 'Muslim:e/innen', 'Religion / Weltanschauung', 'Nationalität', 'ethnische Herkunft / \"Rasse\"']}\n",
    "new01222167 = {'id': '01222167', 'label': ['KEINE', 'In Deutschland lebende Ausländer:innen', 'Islamist:innen', 'Die Grünen', 'Politische Einstellung', 'Asylbewerber:innen', 'Anderes Merkmal', 'Nationalität']}\n",
    "new01222700 = {'id': '01222700', 'label': ['KEINE', 'Anderes Merkmal', 'Sich illegal in Deutschland aufhaltende Personen']}\n",
    "new01222612 = {'id': '01222612', 'label': ['KEINE', 'Palästinenser:innen', 'Nationalität']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erkannte Fehler: Tweets korrigiert in neue Datei schreiben\n",
    "# Dabei inkl. der Korrektur eines Off-by-One-Fehlers in den IDs, der auch erst im Laufe der Annotation aufgefallen war (je +1)\n",
    "# Bereits ausgeführt, nicht noch einmal ausführen\n",
    "\n",
    "# Korrekturen in den Trainingsdaten (noch wie in den Ursprungsdatensätzen getrennt, s. Anmerkung oben)\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train_HATE_annotiert.json\", mode=\"r\", encoding=\"utf-8\") as trainf:\n",
    "    train_anno = trainf.read()\n",
    "    train_annotations = json.loads(train_anno)\n",
    "\n",
    "korrekturen_train = [new01110262, new03113327, new01114326, new01111028, new03113648, new01112254, new02110710, new04111621, new03112728, new02113786, new01113048, new01110156]\n",
    "for i in train_annotations:\n",
    "    for j in korrekturen_train:\n",
    "        if i[\"id\"] == j[\"id\"]:\n",
    "            i[\"label\"] = j[\"label\"]\n",
    "    i[\"id\"] = i[\"id\"][:4] + f'{int(i[\"id\"][4:])+1:04d}'\n",
    "train_anno_k = json.dumps(train_annotations)\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train_HATE_anno_korrigiert.json\", mode=\"w\", encoding=\"utf-8\") as trainf_k:\n",
    "    trainf_k.write(train_anno_k)\n",
    "\n",
    "# Korrekturen in den Testdaten\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test_HATE_annotiert.json\", mode=\"r\", encoding=\"utf-8\") as testf:\n",
    "    test_anno = testf.read()\n",
    "    test_annotations = json.loads(test_anno)\n",
    "\n",
    "korrekturen_test = [new01222281, new02220290, new01221146, new01220517, new02221882, new01223120, new01221025, new02222036, new01220250,\n",
    "                    new02221762, new01223162, new02221146, new01220249, new02221991, new02220270, new02220311, new01222167, new01222700, new01222612]\n",
    "for i in test_annotations:\n",
    "    for j in korrekturen_test:\n",
    "        if i[\"id\"] == j[\"id\"]:\n",
    "            i[\"label\"] = j[\"label\"]\n",
    "    i[\"id\"] = i[\"id\"][:4] + f'{int(i[\"id\"][4:])+1:04d}'\n",
    "test_anno_k = json.dumps(test_annotations)\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test_HATE_anno_korrigiert.json\", mode=\"w\", encoding=\"utf-8\") as testf_k:\n",
    "    testf_k.write(test_anno_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Annotationen nach Merkmal getrennt speichern\n",
    "\n",
    "- in unterschiedlicher Detailgenauigkeit:\n",
    "    - VVH Ja / Nein\n",
    "    - Gruppe Ja / Nein, Welches Gruppenmerkmal / Keine Gruppe \n",
    "    - Handlung VVH-Allg Ja / Nein, Welche Handlung genau / Keine Handlung\n",
    "\n",
    "- gleichzeitig @user-Erwähnungen anonymisieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import copy\n",
    "\n",
    "\n",
    "# Liste der zu entfernenden Duplikate laden (schwarze Liste)\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\schwarze_Liste.txt\", mode=\"r\", encoding=\"utf-8\") as schw:\n",
    "    schwarzeListe = schw.readlines()\n",
    "    schwarzeListe = [line.strip() for line in schwarzeListe]\n",
    "\n",
    "def anonym_atuser(tweet):\n",
    "    tweet_anonym = re.sub('@[^@ ]+?@', '@user@', tweet)\n",
    "    tweet_anonym = re.sub('@[^@ ]+? ', '@user ', tweet_anonym)\n",
    "    tweet_anonym = re.sub('@[^@ ]+?$', '@user', tweet_anonym)\n",
    "    return tweet_anonym\n",
    "\n",
    "# Korrigierte Annotationen laden\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Train_HATE_anno_korrigiert.json\", mode=\"r\", encoding=\"utf-8\") as in_hate_train:\n",
    "    hate_train = in_hate_train.read()\n",
    "    hate_train_anno = json.loads(hate_train)\n",
    "    for entry in hate_train_anno:\n",
    "        entry[\"data\"] = anonym_atuser(entry[\"data\"])\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\RefKorpHateSpeechDe_Test_HATE_anno_korrigiert.json\", mode=\"r\", encoding=\"utf-8\") as in_hate_test:\n",
    "    hate_test = in_hate_test.read()\n",
    "    hate_test_anno = json.loads(hate_test)\n",
    "    for entry in hate_test_anno:\n",
    "        entry[\"data\"] = anonym_atuser(entry[\"data\"])\n",
    "\n",
    "hate_anno = hate_train_anno + hate_test_anno\n",
    "\n",
    "# Einheitliche Formatierung\n",
    "for entry in hate_anno:\n",
    "    entry.pop(\"tag1\")\n",
    "    entry.pop(\"tag2\")\n",
    "    entry[\"corpus_id\"] = entry.pop(\"id\")\n",
    "    entry[\"tweet\"] = entry.pop(\"data\")\n",
    "\n",
    "# aussortierte IDs löschen\n",
    "hate_anno = [entry for entry in hate_anno if entry[\"corpus_id\"] not in schwarzeListe]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Annotation Volksverhetzung Ja / Nein (\"VVH\", \"KEINE\")\n",
    "\n",
    "def transform_anno_vvh(corp):\n",
    "    new_anno = []\n",
    "    for entry in corp:\n",
    "        new_entry = copy.deepcopy(entry)\n",
    "        if \"KEINE\" in entry[\"label\"]:\n",
    "            new_entry[\"label\"] = \"KEINE\"\n",
    "        else:\n",
    "            new_entry[\"label\"] = \"VVH\"\n",
    "        new_anno.append(new_entry)\n",
    "    return new_anno\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_HATE_VVH.json\", mode=\"w\", encoding=\"utf-8\") as out_vvh:\n",
    "    vvh_keep = transform_anno_vvh(hate_anno)\n",
    "    vvh = json.dumps(vvh_keep)\n",
    "    out_vvh.write(vvh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1   Gruppe Ja / Nein (\"Gruppe\", \"KeineGruppe\")\n",
    "\n",
    "def transform_anno_gruppe(corp):\n",
    "    gruppe = {\"Nationalität\", 'ethnische Herkunft / \"Rasse\"', \"Religion / Weltanschauung\",\n",
    "                    \"Politische Einstellung\", \"Geschlecht\", \"Anderes Merkmal\"}\n",
    "    new_anno = []\n",
    "    for entry in corp:\n",
    "        new_entry = copy.deepcopy(entry)\n",
    "        if gruppe & set(entry[\"label\"]) != set():\n",
    "            new_entry[\"label\"] = \"Gruppe\"\n",
    "        else:\n",
    "            new_entry[\"label\"] = \"KeineGruppe\"\n",
    "        new_anno.append(new_entry)\n",
    "    return new_anno\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_HATE_Gruppe.json\", mode=\"w\", encoding=\"utf-8\") as out_gr:\n",
    "    gruppe_keep = transform_anno_gruppe(hate_anno)\n",
    "    gruppe = json.dumps(gruppe_keep)\n",
    "    out_gr.write(gruppe)\n",
    "\n",
    "\n",
    "# 2.2   Das die Gruppe definierende Merkmal / Keine Gruppe\n",
    "\n",
    "def transform_anno_gruppe_det(corp):\n",
    "    gruppe = {\"Nationalität\", 'ethnische Herkunft / \"Rasse\"', \"Religion / Weltanschauung\",\n",
    "                    \"Politische Einstellung\", \"Geschlecht\", \"Anderes Merkmal\"}\n",
    "    new_anno = []\n",
    "    for entry in corp:\n",
    "        new_entry = copy.deepcopy(entry)\n",
    "        if gruppe & set(entry[\"label\"]) != set():\n",
    "            new_entry[\"label\"] = list(gruppe & set(entry[\"label\"]))\n",
    "        else:\n",
    "            new_entry[\"label\"] = [\"KeineGruppe\"]\n",
    "        new_anno.append(new_entry)\n",
    "    return new_anno\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_HATE_GruppeDetail.json\", mode=\"w\", encoding=\"utf-8\") as out_grd:\n",
    "    gruppedetail = transform_anno_gruppe_det(hate_anno)\n",
    "    grdt = json.dumps(gruppedetail)\n",
    "    out_grd.write(grdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1   Handlung VVH-Allg Ja / Nein\n",
    "\n",
    "def transform_anno_handlung(corp):\n",
    "    handlung = {\"Aufstachelung zu Hass\", \"Aufforderung zu Gewalt- oder Willkürmaßnahmen\", \"Angriff der Menschenwürde\"}\n",
    "    new_anno = []\n",
    "    for entry in corp:\n",
    "        new_entry = copy.deepcopy(entry)\n",
    "        if handlung & set(entry[\"label\"]) != set():\n",
    "            new_entry[\"label\"] = \"Handlung\"\n",
    "        else:\n",
    "            new_entry[\"label\"] = \"KeineHandlung\"\n",
    "        new_anno.append(new_entry)\n",
    "    return new_anno\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_HATE_Handlung.json\", mode=\"w\", encoding=\"utf-8\") as out_handl:\n",
    "    handlung_keep = transform_anno_handlung(hate_anno)\n",
    "    hndl = json.dumps(handlung_keep)\n",
    "    out_handl.write(hndl)\n",
    "\n",
    "\n",
    "# 3.2   Welche Handlung VVH-Allg / Keine Handlung \n",
    "\n",
    "def transform_anno_handlung_det(corp):\n",
    "    handlung = {\"Aufstachelung zu Hass\", \"Aufforderung zu Gewalt- oder Willkürmaßnahmen\", \"Angriff der Menschenwürde\"}\n",
    "    new_anno = []\n",
    "    for entry in corp:\n",
    "        new_entry = copy.deepcopy(entry)\n",
    "        if handlung & set(entry[\"label\"]) != set():\n",
    "            new_entry[\"label\"] = list(handlung & set(entry[\"label\"]))\n",
    "        else:\n",
    "            new_entry[\"label\"] = [\"KeineHandlung\"]\n",
    "        new_anno.append(new_entry)\n",
    "    return new_anno\n",
    "\n",
    "with open(\"..\\Korpora\\Referenzdatensatz_HateSpeech_Deutsch\\HateSpeechDe_HATE_HandlungDetail.json\", mode=\"w\", encoding=\"utf-8\") as out_handld:\n",
    "    handdetail = transform_anno_handlung_det(hate_anno)\n",
    "    hddt = json.dumps(handdetail)\n",
    "    out_handld.write(hddt)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bfe21998ac49805528ba5c524b8c99ad95f34df1263c0d2d07976ea5fcbab9a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
